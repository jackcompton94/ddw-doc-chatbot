[
  {
    "title": "Product documentation",
    "content": "",
    "url": "https://docs.data.world/en/98527-product-documentation.html"
  },
  {
    "title": "Quickstarts and tutorials",
    "content": "",
    "url": "https://docs.data.world/en/98376-quickstarts-and-tutorials.html"
  },
  {
    "title": "Educator quickstart",
    "content": "As part of data.world's Public Benefit Mission, we offer additional features exclusively for educators at no extra cost.\nBy using the data.world platform, educators and students gain access to hundreds of thousands of open datasets, integrations with the tools you already know and love, in-browser environments to create data visualizations and practice SQL queries, and tools to aid collaboration between classes and peers. You can find an Example Classroom with demo assignments on data.world to give you an idea of what you can create with your class.\nThis guide exists to help get you up and running with data.world in your classroom. Before getting started, you'll need to set up your classroom on data.world.",
    "url": "https://docs.data.world/en/98396-educator-quickstart.html"
  },
  {
    "title": "Setting up your classroom",
    "content": "Visit our Classroom Setup process. If you don't already have a data.world account, this process will also help you set one up.\nInvite your students to join you in the classroom. You can either copy and send the invite link to your students all at once, or have your students sign up for data.world independently and then request to join your classroom. Either way, your students will need to each create free data.world accounts to join you on the platform.\nClick \"Finish\" when you're done.\nCheck out your new classroom by clicking on your classroom under the \"Activity\" pane on the left-hand side and then selecting \"View organization\".\nTake a few minutes to add some information about your class to the \"Summary\" section of the page by clicking + Add summary. This could be a useful place to provide links to relevant resources, your syllabus, or any other information that you think your students would find useful. You can view the introduction to our Simple Data Documentation Editor documentation for more information.\nOn data.world, we refer to the classroom you've just created as an organization. This is a space where you and your students can add content, collaborate, and create new things together.",
    "url": "https://docs.data.world/en/98397-setting-up-your-classroom.html"
  },
  {
    "title": "Finding and Creating Resources",
    "content": "data.world is designed to easily share resources with your students. In many classroom situations, educators will either want to find data or upload their own. We'll walk through both scenarios shortly, but first, it's helpful to understand that data.world houses two primary types of resources: datasets and projects.\nDatasets are collections of data files, documentation, scripts, metadata, and any other supporting resources to help other people understand the data. Datasets can include one file or several related files. While our platform can host a wide variety of file types, spreadsheet files (e.g., .csv, .tab, .xlsx) are likely to be the most useful for you.\nProjects are spaces to help you collect multiple datasets, query data, create data visualizations, collaborate on analyses, and share findings.\nIn a classroom setting, it may be most useful to think of projects as a place to provide details of an assignment. Datasets would then be connected to individual projects as you need them. Refer to our datasets vs. projects documentation for more information on this topic.",
    "url": "https://docs.data.world/en/98398-finding-and-creating-resources.html"
  },
  {
    "title": "Finding data",
    "content": "The data.world platform is host to hundreds of thousands of open datasets. The way to access them is to type a search term into the \"Search data.world\" bar at the top of your page.\nIf you have any resources inside of your organization, typing a term in the search bar will display your classroom's resources by default. To search the entire data.world open platform, select \"Include all community results\" under the \"Results\" header.\nOnce you've made your search, you can explore the results or utilize some of our advanced search features  by clicking \"Advanced\" to help you narrow them down. For educators, you may want to try selecting dataset from beneath Resource Type, limiting your search to only include dataset resources. You can then filter further by selecting specific Owners or Tags that match what you're looking for.",
    "url": "https://docs.data.world/en/98399-finding-data.html"
  },
  {
    "title": "Evaluating data you found",
    "content": "Now that you've found data on the data.world platform, you'll want to make sure that the data you've found meets your needs. Here are a few things that you may want to check:\nRecency: when was the data uploaded? When was it last updated?\nDocumentation: does the dataset have a full description? Is the Data Dictionary completed?\nLicense: does this dataset license allow you to use the data for your purposes?Summary of common license types:\nFile size: if your students will not be working with the data on the data.world platform, you'll want to check the file size before asking them to download since some programs struggle to open large files. If you're interested in file size limits on data.world, refer to our file size documentation.\nWhile these are not meant to solve every use case you may have when evaluating a dataset you found for use in your classroom, hopefully this has provided you a good place to start.dataset license",
    "url": "https://docs.data.world/en/98400-evaluating-data-you-found.html"
  },
  {
    "title": "Saving a dataset",
    "content": "If you find a dataset that you like, you can\u00a0bookmark it\u00a0to save it for later. Bookmarks are attached to your individual profile, not your classroom organization, so your students won't be able to see the things you bookmark.\nIf you find a dataset that you want to incorporate into a project or assignment, you can link that dataset directly to a project. We'll get into more detail on how to do that in the section on Creating an Assignment.\nYou also may find following particular organizations helpful. That way, you'll be notified whenever that organization adds new data or updates any of their existing datasets. Here are a few examples of some organizations you may be interested in following:\nData Journalism Organizations\nThe Associated Press\nFiveThirtyEight\nThe Pudding\nOur World in Data\nCollections of Data Curated by the data.world Team\nAgriculture\nAnimals\nCOVID Resources\ndata.world Research\nEducation\nFinance\nGovernment\nHealth\nPop Culture and Entertainment\nPublic Safety\nTechnology\nTo follow an organization, you can visit their profile then click the \"Follow\" button. When organizations you are following make any updates, you'll receive an email notification about those updates.",
    "url": "https://docs.data.world/en/98401-saving-a-dataset.html"
  },
  {
    "title": "Uploading Data",
    "content": "If you have found open data elsewhere that meets your needs but isn't on the data.world platform, you can upload it to the platform. You can either upload the data resource directly to your personal account or to your classroom. The process is the same.\nEither way, you'll want to click on the +New button located on the header bar right beside your profile picture.\nSelect \"Create new dataset\"\nGive your dataset a name. This should be \"human readable\", meaning that it doesn't have to match the file name or omit spaces. Name your dataset something descriptive so that other users know what it's about, particularly if you plan to share this openly on the platform. Keep in mind that datasets can be connected to several projects, so instead of naming a dataset \"Assignment 1\", it's better practice to name your dataset something based on the contents of the data.\nDecide whether you want your classroom organization to be the \"owner\" of this dataset, or your personal account. If you plan to use the dataset with multiple classes, it may be better to make your personal account the \"owner.\"\nDecide who to share the resource with:\nIf you don't want to share this resource with anyone, select \"No One\"\nIf you want to only share this resource with the students in your data.world classroom, make sure the classroom is the owner of the resource and select \"All of _____\" (where the ____ is the name of your classroom)\nIf you want to share your dataset with the larger data.world community, click \"Make public to data.world community\"\nThen click \"Create dataset\". Refer to our Creating Datasets documentation for more information.\nNext, add a brief description of this dataset. What data is included? What is the goal? Why would someone want to explore your dataset? This description and the title of your dataset will be visible when people on the open platform find your dataset in their searches.\nThen, upload your data by clicking the \"Add data\" button. You will be given several ways to add a data resource, including uploading from your computer, syncing from a URL, or integrating with other tools like Google Drive. When you're done, click \"Continue\".\nNow, add your documentation. Use the \"Summary\" area to provide any additional information or materials about where the data came from, how it was collected, any caveats involved etc. The dataset Summary area can be edited using Markdown syntax or our Simple Text Editor. Refer to our Document your data documentation for more information on data dictionaries, tagging, setting license types and more.Documenting your data\nFor each file that you've uploaded to a single dataset, you can select \"Add description\" to add a brief description to each file. You can also add labels to indicate whether the data you've uploaded is \"raw\" or \"clean\".\nIf your file is tabular (e.g., a spreadsheet), you can provide information about how to interpret each column of data in the data dictionary. You can access the data dictionary in the \"Add a description\" menu, under the \"Column details\" tab.\nIf your dataset is open to the public, you'll want to make sure that you add both Tags and a License. You can do that by clicking \"Edit\" next to \"About this dataset\". Tags make your dataset easier to find when users are searching for it, and licenses let other users know what they are allowed to do with your dataset. Refer to the \"Setting a license type\" and \"Tagging\" sections of our Document your data documentation for more information.Documenting your data",
    "url": "https://docs.data.world/en/98402-uploading-data.html"
  },
  {
    "title": "Creating an Assignment",
    "content": "On the data.world platform, projects can be used as spaces to share or collaborate on a particular assignment. For more details on the differences between projects and datasets, see the documentation on resources.\nTo create a new assignment within your classroom:\nClick on the +New button located on the header bar right beside your profile picture.\nSelect \"Create a new Project\"\nGive your project a name. This should be \"human readable\", meaning that it doesn't have to match the URL or omit spaces. Name your project something descriptive so that other users know what it's about, particularly if you plan to share this openly on the platform.\nSet the Owner of the project to your classroom organization.\nDecide who you would like to share this assignment with:\nIf you don't want to share this resource with anyone, select \"No One\"\nIf you only want to share this resource with the students in your data.world classroom, make sure the classroom is the owner of the resource and select \"All of ____\" (where the ___ is the name of your classroom)\nIf you want to share your project with the larger data.world community, click \"Make public to data.world community\"\nClick \"Create Project\"\nAdd a brief description of your project. What is the overall goal? Maybe add the due date here for easy reference.\nNext, you can connect data to this project. You can also connect data later, if you don't know which resources you want to connect yet. There are a few ways you can do this:\nConnect to a dataset that's already on data.world. Click \"add data\" and then select \"Link a data.world dataset\". Find the dataset you want to link either by searching, looking at your resources, or looking through your personal bookmarks. Once you find the dataset you want to add, click \"Connect\".\nUpload your own dataset. See the Uploading Data documentation for more information.\nWhen you connect to an existing dataset, rather than upload files directly into a project, the dataset can be reused for other projects. If the dataset and associated projects are public, you can also find queries that have been run and other projects that have used a dataset on that dataset's profile page.\nWhen you're finished, click \"Done\"\nClick \"Project summary\" to add additional information about this project. For assignments, include any information your students would find helpful or relevant to completing the assignment.\nIf you have finished creating your assignment and want to connect more datasets, you can:\nClick on \"Home\" in the project directory and then click \"Upload files or connect to data source\". Refer to Steps 8-10 in the above section to continue.\nIf you've found a dataset and want to add it to a project:\nIn the top right of the dataset overview page, you can click the arrow next to \"Explore this dataset\".\nIf the project for your assignment has already been created, click \"Connect to existing projects\" and then search for a project that you have access to. Select the assignment you are looking for and click \"Save\".\nIf the project for your assignment hasn't been created yet, select \"Create a new project\". This will walk you through the process of creating a new project with the dataset you've selected already added. See the Creating an Assignment section for more information.",
    "url": "https://docs.data.world/en/98403-creating-an-assignment.html"
  },
  {
    "title": "Analyzing Data",
    "content": "The data.world platform provides several ways for educators and students to explore and analyze data on the platform as well as connect to some of your favorite tools.",
    "url": "https://docs.data.world/en/98404-analyzing-data.html"
  },
  {
    "title": "Exploring Data",
    "content": "It is often helpful to begin data exploration by looking at a \"high-level\" view of the data. The easiest way to do that on the data.world platform is to:\nNavigate to a particular dataset that contains tabular files (e.g., .csv, .tab, .xlsx etc.)\nClick the button labelled \"Switch to column overview\" on the bottom right for each file you'd like to explore.\nThe column overview gives high-level information about each column in the dataset, including how many rows are empty, the number of distinct values in the column, a distribution of values for numeric columns, and either samples or most common values for text-based data columns.\nUsers can also click on the information icon next to any column name for more information about that column, including a text-based description of the column from the data dictionary.",
    "url": "https://docs.data.world/en/98405-exploring-data.html"
  },
  {
    "title": "Querying data with SQL or SPARQL",
    "content": "The data.world platform provides sandbox environments for your students to practice their data querying skills without leaving the platform or having to deal with complicated installations and setups.\nIf students are practicing their querying skills unrelated to an assignment or project, they can navigate directly to a dataset that they're interested in and click \"Explore this dataset\". This will generate a new, untitled project for them with the dataset already connected.\nIf students want to practice their querying skills in an assignment you've already created, they can:\nNavigate to the project\nClick \"Launch Workspace\" to open up an editable and collaborative workspace\nNext to \"Project Directory\" click \"Add\"\nThen select either \"New SQL query\" or \"New SPARQL query\" depending on your querying language of choice\nThis will create a new, untitled query. Students can then type directly into the workspace and click \"Run query\" whenever they want the query to run. They will see the results of their query directly in their browser window. Information about all datasets connected to the project is available in the panel on the right-hand side of the workspace to remain accessible.\nIf students wish to save a query so that their classmates can also run it or so that they can access the same query again at another time, click the \"save\" button. Adding a detailed title and description detailing the purpose and output of each query will make them more reusable.\nFor more information about querying data on the data.world platform, refer to our SQL and SPARQL documentation.",
    "url": "https://docs.data.world/en/98406-querying-data-with-sql-or-sparql.html"
  },
  {
    "title": "Visualizing Data",
    "content": "Just as data.world users can query data directly on the platform, they can also visualize data without leaving the platform using our Chart Builder tool.\nIf you'd like to use a different tool for data visualization (e.g., Tableau, Excel, R, Python etc.) jump ahead to our documentation on  Integrations .\nUsers can access the Chart Builder tool from a few locations:\nFrom a dataset\nClick \"View\" in the upper-right hand corner of a file. This will open a project workspace.\nFrom a project workspace\nSelect a file from a connected dataset, if one isn't already selected\nClick the arrow next to \"Open in app\" and select \"Open with Chart Builder\". This will open a new window with the data automatically uploaded into the chart builder tool\nChart Builder comes with two options for creating and modifying charts: a Visual Builder and a Vega-Lite Editor. The easiest way to use it is to create your initial chart on the Visual Builder tab and then switch over to the Vega-Lite Editor to make any changes outside the scope of the Visual Builder. See the Vega-Lite website for more information.\nYou can find more information about using our Chart Builder tool in our Data Visualization with Chart Builder article.Visualizing data",
    "url": "https://docs.data.world/en/98407-visualizing-data.html"
  },
  {
    "title": "Basic tutorial series",
    "content": "Welcome to the basic data.world tutorials! There are many great reasons why to use data.world, and in these tutorials we're going to walk through how to use it. The basic tutorials are written in logical, manageable chunks of information explaining the basic concepts of the data.world platform. The exercises at the end of each tutorial are completed on data.world. You will need a data.world login, (available for free here if you do not have one) to do the tutorial exercises. Each tutorial follows the format of:\nIntroduction - Contains a brief introduction to the topic of the tutorial including terminology introduced in it.\nRequirements - A list of tangible work from previous tutorials needed to complete this tutorial, and links to sample files you can use if you did not complete the previous tutorials.\nObjectives - The learning goals of the tutorial linked to sections in the Main body.\nMain body - The main body of the tutorial with all documentation and screenshots necessary to complete the exercise.\nExercise(s) - One or more practice exercises that walk through all the material presented in the tutorial. The output of an exercise is often the basis for a future tutorial's exercise.\nBest practices (optional) - If there are many ways to do something and we have suggestions for the best way to do it, they will be here.\nConclusion - A quick recap of the tutorial.\nReference list - A list of all the hyperlinked data in the tutorial and other important references.\nThe tutorials are ordered and presented in a natural progression for learning data.world from the ground up. Those that cover the main aspects of working with data.world are in the Basics section, and are numbered (e.g., 1.) Secondary or expert skills are found in the Advanced tutorials section. If you have already been using the platform and know the basics, you can jump around the tutorials to fill in any gaps or expand your knowledge. The entire tutorial series is based on a project using the Bee Colony Statistics dataset. Each person who uses the tutorials will need to create their own project using that dataset.\nThough the exercises in each tutorial build on the work done in previous tutorials, all of the tutorials are also stand-alone--we provide the files, queries, datasets, etc. needed to do the tutorial if you did not create them in a previous tutorial. The only action that everyone must complete on their own is to create the project. The project is the container for all the exercises, and each person working through them must have their own project.\nAfter working through the tutorials you should be able to:\nUnderstand the terms used to describe data and data analysis\nFind data relevant to your needs on data.world\nAdd, document, and share your own datasets\nCreate a project to work with your data\nProduce visualizations of your data\nFollowing are some terms it's helpful to understand before beginning the tutorials:\nData, whether you think of it as singular or plural, is information. It can be stored in a variety of formats including text files, spreadsheets, and relational and graph databases.\nA database is information stored in a structured way.\nThe terms you use to describe data are its metadata Metadata is information about data, like what format it's in, who owns it, where it came from.\nA dataset is a snapshot of a database at a specific moment in time. Datasets on data.world are worked with--combined, analyzed, and discussed--in projects.\nA project is the place you do your work with the data, and it can contain both datasets and files specific to (and only available in) the project. Datasets, in contrast, can be used in many different projects. (More detailed information about when to use a dataset or a project is in the article on datasets and projects.)\nThe data.world data catalog is a repository of datasets containing both data and metadata resources.\nThe tutorial is a living document, and as such new sections are being continually added as new functionality becomes available in the platform. If there is information you would like to see added, or if you have questions about any of part of the tutorial, please drop us a note!",
    "url": "https://docs.data.world/en/98408-basic-tutorial-series.html"
  },
  {
    "title": "Find data on data.world",
    "content": "This tutorial is part of the basic tutorial series for the data.world platform. See the article overview of basic tutorials for more information.\nIn this tutorial we'll cover the basics of searching on data.world and how to evaluate and save your search results.\nAfter working through the tutorial you should be able to:\nUse the search bar to find data\nBookmark data for later use\nEvaluate the data for your needs\nA data.world login (available for free here if you don't have one).\nThere is a search bar in the data.world header on most pages. When you enter the string bee colony in the search bar, notice that, even before hitting Enter to run the search, there is a drop down of the most popular results:\nIf a dropdown is presented as you type, you can either select one of the options on the dropdown or run the search. When you run the search, if you're in an organization you'll see all the items that are owned by you, the organization, or another member of the organization listed. You can also expand the scope of your search to include all public resources by selecting Include community results under the search bar. If you are not in an organization you'll see the community results by default:\nOn the left side of the screen there are other criteria you can use to restrict your results. These items are called facets. Facet choices are cumulative, and whenever you select one it automatically updates the availability of the others. You can read more about facets and other methods of filtering your results in our article on filtering search results.\nBy default when you do a search on data.world your results will include everything that matches your search: datasets, projects, tables, users, organizations, pdfs, charts, etc. Notice that each resource type has its own icon, making it easy to see if it's what you are looking for or not:\nA list of the different icons and their associated resource types is found in our guide to icons.A guide to icons\nIf you are looking specifically for datasets you can can use the dropdown next to All resource types to restrict the search to data only:\nIn addition to straight text searches you can also use a variety of search operators to narrow down results. See our article on advanced search for details.\nWhen you find data you would like to be able to find and reference easily later you can bookmark them by selecting the bookmark flag on the top right of the screen. The flag also shows you how many other people have bookmarked the same dataset:\nThe overview page of the dataset gives you a lot of information. On the right side you can see who it is shared with, who owns it, and when it was created. You can also see how many files and columns it contains, and how many virtual tables are linked to it. All of this information can be valuable in determining if the data meets your needs:\nSee our article on evaluating data for more information.\nLogin to your data.world account and use the search bar to search for bee colony\nFind the dataset Bee Colony Statistics owned by @siyeh\nBookmark the dataset to make it easy to find for the next exercise\nThere is a lot of information to be found on data.world, and we want to make that information as intuitive to access as possible. With all the data available, sometimes entering a simple string in the search bar--even with powerful filtering ability backing it--isn't enough to find what you need. For the times you need even more pin-point accuracy and complexity in your search terms we have a robust set of logical and data.world-specific operators to help you find just what you are looking for. In fact, with all the data that is available, you're not likely to have a problem finding what you need. It's more likely that the problem will be narrowing down the results from a search so you can focus on the resources that best meet your needs. Once you have a reasonable list of results from your search, you need to evaluate them to see which are appropriate for your project.\nIn this article we covered the basics of how to find and evaluate data, but there is a lot more you can do. The references linked in this article will provide more depth and detail on each of those topics.",
    "url": "https://docs.data.world/en/98423-find-data-on-data-world.html"
  },
  {
    "title": "Add data to data.world",
    "content": "This tutorial is part of the basic tutorial series for the data.world platform. See the article overview of basic tutorials for more information.\nThere are many ways to find data on the web, and the data you find comes in many different formats from text to tables to images. In this tutorial we will find data in a public database on the web, create a dataset from it, and link it to our project. You will walk through finding a public database on the web, downloading data from it, and uploading that data to a data.world dataset.\nAfter working through the tutorial you should be able to:\nFind data on the web\nPrepare a file for upload to data.world\nCreate a dataset\nAdd a new dataset to a project\nTo complete this tutorial you need to have:\nA data.world login (available for free here if you don't have one).\nYour own tutorial project (you must create this yourself--it cannot be downloaded)\nThe Bee Colony Statistics dataset linked to your project\nIf you need help creating the project or linking the dataset to it, detailed instructions are in the tutorial Create a project to work with data.\nWhen the original Bee Colony Statistics dataset was created, the 2017 data from the United States Department of Agriculture (USDA) wasn't available. Now that it is, we can download it, create a new dataset from it, and add it to our project. Because the original dataset is well-documented, it's easy to look up the source of the original data (the Quick Stats database of the National Agricultural Statistics Service) so we can get the latest statistics.\nThe original dataset has more than one table in it, but for this tutorial we'll be looking at just the bee colony census data by state. The link to the Quick Stats database is in the Summary of the Bee Colony Statistics dataset:\nand the parameters used in it are shown in the file Search criteria for bee colony census by state.png:\nHowever to make getting the data a little easier, here is a link to the Quick Stats database with the parameters already filled in. All we have to do now is to select the Get Data button at the bottom of the screen. The results should contain 50 rows. The number of rows is shown in the upper right corner of the window. If there aren't 50 rows, use the Back link on the bottom of the screen to go back to the previous page to verify your parameters.\nOnce you have the results, you can download them onto your desktop to re-upload them. They will be in CSV format so if you have Excel, Google Sheets, or another spreadsheet program you could open the file after you've downloaded it, but that isn't necessary for this activity. Select Spreadsheet (shown in the image above)to download the file.\nThe filename from the USDA will be a series of letters and numbers--nothing with any informational content. When files are uploaded to data.world, the names they have on ingest are the names they will have on data.world--they cannot be changed after uploading except by downloading them, changing the name, re-uploading the renamed version, and deleting the original file. To make your data more useful rename the file Bee Colony Census 2017 by State.csv:\nWhen you upload a a spreadsheet with multiple tabs each tab is preserved as a separate table in data.world. Before uploading your file it is a good idea to review the names on all the tabs as they will each show up as a table name.\nWhile you can upload any type of file to data.world you might get an error if the file is too large or if it's corrupt, or if there is another issue with the file. For a complete list of the errors you might encounter when uploading a file see the article on file upload status messages.\nCreating a dataset is very similar to creating a project. From your homepage (or any page with a + New link in the header) Select + New from the header and choose Create new dataset:\nIn the Create a new dataset dialog you can name of the dataset, choose the owner, and set the access permissions. If you are in an organization, the organization's name will show as the owner by default. If you are not in an organization, you will be the default owner. From the dropdown on the Owner field you can change the owner--including proposing ownership to an organization that accepts proposals for ownership:\nAfter you have put in a title and set the ownership, you need to set the permissions. By default, permissions are set to share with no one. If you set the ownership of the dataset to an organization, the other options are to share with everyone in the organization or to make it public to the data.world community. If you set yourself as the owner your only options are to share with no one or make public to the entire data.world community. Once you've set the permissions, select Create dataset and you can either add a description and/or upload your data file, or you can continue on to the dataset overview:\nTo add this dataset to a project select the arrow next to Explore this dataset and choose Add to existing project:\nNote: If you did not create a project in the prior exercise you can do it now by selecting Create a new project. If you need help creating the project, see the tutorial article Create a project to work with data.\nAt this point you'll be presented with a dialog box showing the dataset on the left and a list of the projects owned by you or an organization you are in to which you have write permissions:\nMake your selection, and after you click Save you can either go back to your dataset or to the project:\nThis tutorial uses real-world, feral data--not a made-up, sanitized file. It begins with accessing a live, publicly-accessible, US government database on the web, running a query against it, and saving the results from the query to a file. The next step is to create a dataset and upload a file to it. If you prefer to skip right to creating the dataset and uploading the file, download the Bee Colony Census 2017 by State.csv file from the dataset Bee colony statistics and proceed directly to step 5 below.\nGo to the Quick Stats database for the National Agriculture Statistics Service on the United States Department of Agriculture website (the parameters will be pre-populated for you).\nSelect Get Data\nDownload the data as a spreadsheet\nRename the downloaded file Bee Colony Census 2017 by State.csv\nLogin to your data.world account and create a dataset named Bee Colony Census 2017\nUpload and add the file to the dataset\nAdd the new dataset to your project\nIt is very easy to create a dataset and add new data to data.world, and there are some things you can do to make it easy to use too. See our article Dataset best practices for more information.\nCreating a dataset and creating a project are very similar activities, and both are intentionally structured to work together easily. You can create a dataset and add it to an existing project, or you can create a dataset and a project to work with it all at the same time. Uploading a file is only one way to add data to data.world. See the article on getting your data into data.world for information on other methods.\nAfter you put your data onto data.world there are many things you can do to make it easy for others to find and use. We have additional articles on how you license, document, verify, set file labels, and tag your data, and how different file types are handled in our help center. We encourage you to make use of them to get the most out of your experience putting data on data.world.\nOverview of basic tutorials - An overview of the basic tutorials\nCreating a dataset - All the ways to get data into data.world from uploading files to using a virtual connection\nSummary of the Bee Colony Statistics dataset - Shows the source of the original data\nSearch criteria for bee colony census by state.png - A list of the criteria used to search the source database for the original data\nFormatted query for the Quick Stats database - A pre-formatted query to run against the source database to get the latest data\nFile upload status messages - A list of error messages you might encounter when uploading a file to data.world\nCreate a project to work with data - The previous exercise in this tutorial\nTips for uploading data - Best practices and other information on good data practices\nGetting your data into data.world - All the different ways you can get data into data.world\nSetting a license type - Understanding how licensing works for data you own as well as data you own when it is uploaded to data.world\nDocumenting your data - How to use the description, summary, data dictionary, and other documentation resources in your datasets\nVerifying your data with data inspectors - How to validate the state of your data in a data.world dataset\nOrganizing a dataset with file labels - Using file labels to further identify files in a dataset\nTagging - How to use tags to organize and group a dataset or project by topic, category, source, department, or team\nSupported file types - How data.world handles different file formats",
    "url": "https://docs.data.world/en/98431-add-data-to-data-world.html"
  },
  {
    "title": "Create a project to work with data",
    "content": "This tutorial is part of the basic tutorial series for the data.world platform. See the article overview of basic tutorials for more information.\nIn the previous tutorial we covered how to find and evaluate data on data.world. Now we're going to create a project where we can work with the data we found. If you need more information on when to use a dataset and when to use a project, see our article on When to use a dataset and when to use a project.\nAfter working through the tutorial you should be able to:\nCreate a project\nSet ownership and permissions to the project\nLink a dataset to your project\nA data.world login (available for free here if you don't have one).\nThere are many ways to create a new project. The easiest is from a dataset that you want to use. You can also use the + New link to the right of the search bar.\nFrom the Bee Colony Statistics dataset (bookmarked in the last exercise) you can quickly create a new project from it by clicking the arrow next to Explore this dataset on the top right of your screen and selecting + Create a new project:\nIf you are on your homepage you can navigate to the dataset from your bookmarks:\nAlternatively, click + New to the right of the search bar and select Project. Whichever way you take, you'll get to the Create a new project dialog. If you started from the dataset, it will automatically be associated with the project. If you started from + New you'll need to link to the dataset later.\nYou can set the owner of the project to be yourself or any organization in which you're a member from the dropdown menu on the Owner field. Additionally from the dropdown you have the option of proposing ownership of the project to any organization that accepts proposals:\nSee the article on crowdsourced datasets for more information. After setting the ownership of a project you will also need to set permissions to it. By default, permissions are set to share with no one. If you set the ownership of the project to an organization, the other options are to share with everyone in the organization or to make public to the data.world community. If you set yourself as the owner your only options are to share with no one or make public to the entire data.world community:\nFor details on how to set yourself as the owner and share with a specific organization see the article on sharing within an organization.\nAfter you have named your project, and set the ownership and access to it, you can select Create project. On the next screen you can enter a brief description of your project, add data to it, or just continue on to the project:\nYou can link a dataset to the project by selecting the Add data button and choosing Link a data.world dataset:\nOr from the project workspace you can Click the + Add link on the top left of the screen and choose Dataset:\nEither one will take you to the dataset connection dialog box. If the dataset you wish to add to the project is listed in the dialog you can select Connect below it to link it. If the dataset isn't listed, type in some of the name and hit Enter to get a match and the choose Connect and Done:\nLogin to your data.world account and from your homepage (or any screen with a +New link on the right of the header), click + New.\nSelect Create new project.\nGive your project a name.\nSet yourself as the project owner.\nKeep the project private.\nConnect the Bee Colony Statistics dataset to your project\nIf you are in an organization and the project is for meant for members of the organization to work on, it's often a good idea to set the ownership of the project to the organization. You are still listed as the Creator, but by making the owner of the project the organization you have set it up for long-term continuity and access.\nIn this exercise we laid the foundation for the rest of the work we will do in the tutorial by creating a project to work with the data. Creating a project is a very simple operation, but there are many options available for how to create it, who owns it, who can see it, and how to link data to it. There are also many more ways to get data into your project and those will be covered in later exercises.",
    "url": "https://docs.data.world/en/98443-create-a-project-to-work-with-data.html"
  },
  {
    "title": "Query your data",
    "content": "This tutorial is part of the basic tutorial series for the data.world platform. See the article overview of basic tutorials for more information.\nIn this tutorial we introduce writing SQL queries in the query panel of the workspace. A query in the context of data is a request for information written in a query language like SQL or SPARQL. Queries use a specified set of terms in a specific order with prescribed syntax. In addition to this tutorial we have many resources to help you learn about about querying on data.world including articles on query basics, working with existing queries, and using query templates. We also have complete documentation and a tutorial for SQL and SPARQL--the query languages used on data.world.\nAfter working through the tutorial you should be able to:\nTell the difference between datasets and projects\nCreate a SQL query\nUse auto-complete of terms and objects in your query\nUse click-to-copy for columns and tables\nUnderstand the pop-up help text in queries\nAuto-format queries\nTroubleshoot queries\nSave queries\nTo complete this tutorial you need to have:\nA data.world login (available for free here if you don't have one).\nYour own tutorial project (you must create this yourself--it cannot be downloaded)\nThe Bee Colony Statistics dataset linked to your project\nIf you need help creating the project or linking the dataset to it, detailed instructions are in the tutorial Create a project to work with data.\nIf you prefer to go straight to the exercises, click here.\nAs was discussed in the Intro to the data.world tutorial, a dataset is where data is stored for use in projects. It contains only the basic resources that pertain to the majority of the projects that use it. Queries are generally specific to a project as they pull only the data from the dataset which is needed for analysis for that project. As such, queries are stored, with rare exceptions, in a project. See our overview of the project workspace for more information.\nTo write a SQL query against data in a project go to the upper left corner of the project workspace and select SQL Query from the + Add menu:\nWhen the query editor comes up in the center panel of the screen, the Project Schema also comes up in the right side panel. If the project schema isn't showing, select the arrow to the right of the Run query button to expand the sidebar:\nClick next in the query editor window to begin entering your query text. As you type your query, the editor suggests SQL terms--operators, aggregations, and functions--for you. Arrow down or up to choose another option:\nHitting 'enter' will auto-complete the highlighted suggestion in the list.\nYou have the option of typing in the table and column names for the data you are querying, but you can also use the click-to-copy feature on the project schema to copy the names for you so you don't have to worry about typos.\nHelp text is provided for all SQL functions and aggregations when you hover over the function name:\nWhile you can type your SQL query all on one line--it is perfectly legal and will run--it's good practice to split it up based on clauses and punctuation. The line breaks make it easier to see where you may have forgotten a comma or some other element of the query, and it also allows us to give you better error messages if there are problems with your query.\nHere's an example of a query before and after running it through our auto-formatter:\nTo auto-format your query, use the keyboard shortcut Cmd + Option + L . If you forget the command there is a dropdown menu to the right of the Run query button which has a link to a list of all the keyboard shortcuts.\nFrom the dropdown menu you can also manage your query (rename, copy, etc.) as well as get to our SQL tutorial for additional help.\nThe article on query editor shortcuts also contains a full list of the shortcuts for the query editor.\nIf you try to auto-format your query and it doesn't work, that's a sign that you have an error in the query. If you run it you'll get an error message telling you what the problem is and where to find it:\nIn the above example the query was written to search for the number of colonies recorded by year just for the state of Montana. However the way the query is written it looks like it is searching for records where the value in the bee_colony_census_data_by_state.state column matches the value in another column named Montana. For Montana to be recognized as a value, it has to be surrounded by double-quotes. The error message returned from running the query tells you that the problem is in the 3rd line and the 47th character position (column). The 47th character should have been the opening \":\nWhen your query is complete, save it for later use by selecting Save to the left of the Run query button. You'll be prompted to name the query, add a description, determine where to save it, and indicate who can see it:\nIt's good practice to give the query a short descriptive name for both your own later use and for use by others working on your project. Adding a description also makes your query more usable as the description shows up on the resource card returned from using search on data.world.\nOpen your tutorial project.\nSelect SQL query from the +Add button in the upper left.\nWrite the following query in the query editor pane and run it:\nSave the query to your project\nThere is much more to learn about queries and query languages than will fit into one tutorial, and this one just scratches the surface of how to write and save queries. If you're conversant with SQL or SPARQL, your best next step is use your project to practice writing queries on data.world. If you need more information, both our SQL and SPARQL documentation are great resources. If you don't know either SQL or SPARQL and want to query data.world datasets we recommend you work through our SQL tutorial. More information about queries and how to use them is covered in the tutorial Advanced work with queries.\nOverview of basic tutorials - An overview of the basic tutorials\nUsing Query templates - Creating and using query templates to make using variables in queries easier\nSQL documentation - How to use SQL on data.world\nSQL tutorial - Exercises built on our SQL documentation for learning SQL from the ground up\nSPARQL docs and tutorial - An introduction to the SPARQL query language with built-in exercises\nBee Colony Statistics - The main dataset used in these tutorials\nBee Colony Census 2017 by State - A dataset created in the tutorial Add data to data.world\nCreate a project to work with data - The previous tutorial which details how to create the project you will need to have for this and the and the rest of the tutorialss\nIntroduction to the project workspace - A complete overview of the project workspace\nQuery editor shortcuts - A list of the shortcuts that are available in the query editor\nAdvanced work with queries - A deeper-level tutorial on using queries",
    "url": "https://docs.data.world/en/98452-query-your-data.html"
  },
  {
    "title": "Produce visualizations of your findings",
    "content": "This tutorial is part of the basic tutorial series for the data.world platform. See the article overview of basic tutorials for more information.\nOne of the most powerful ways to share results of your data analysis is through a visualization. On data.world you can use integrations for various third-party visualization tools including Chart Builder--a visual editor for Vega-Lite built specifically for data.world--and Tableau. In this tutorial we will use Chart Builder to create a couple of simple visualizations that we can use in our analysis of the data. Chart Builder is a good option for those looking to create a simple visualization that is lightweight and easy to embed.\nAfter working through the tutorial you should be able to:\nBuild a chart from a file\nBuild a chart from a query\nFormat your chart\nShare and save your chart\nTroubleshoot problems\nTo complete this tutorial you need to have:\nA data.world login (available for free here if you don't have one).\nYour own tutorial project (you must create this yourself--it cannot be downloaded)\nThe Bee Colony Statistics dataset linked to your project\nThe dataset Bee Colony Data 2017 linked to your project.\nIf you need help creating the project or linking the dataset to it, detailed instructions are in the tutorial Create a project to work with data.\nIf you prefer to go straight to the exercise, click here.\nYou can create visualizations of tabular data files in a project without ever writing a query. For this example we'll use the Bee Colony Census 2017 by State file from the dataset Bee Colony Data 2017 in your project. To create a visualization of all the data in the file, expand the view of the Bee colony Data 2017 by clicking the arrow to the left of the dataset in your project directory pane, and select the file Bee Colony Census 2017 by State:\nUnless you have already set-up the Chart Builder integration you'll need to do that next. Click the dropdown arrow next to Open in app in the upper right of the screen and choose + Add more integrations:\nThis action will take you to the BI and Visualization tools section of our Integrations page, and the Chart Builder integration is right at the top:\nSelect Chart Builder and Enable Integration:\nYou'll be prompted to authorize Chart Builder\nAfter authorizing, when you are on your workspace you'll see the Open in app link has been replaced by Open with Chart Builder. Select Open with Chart Builder to continue.\nChart Builder comes with two options for creating and modifying charts: a Visual Builder and a Vega-Lite Editor. The easiest way to use it is to create your initial chart on the Visual Builder tab and then switch over to the Vega-Lite Editor to make any changes outside the scope of the Visual Builder. See our article on using Vega-Lite or the Vega-Lite website for more information.\nTo create a quick bar chart of the number of colonies by state in 2017 select the field state from the dropdown list for the x-axis, and value from the list on the y-axis. Like magic, our chart appears on the right side of the screen:\nChart Builder can also be used on queries. Using a query as the basis for your chart enables you to:\nClean up data (e.g., change column names for your visualization)\nFilter out data (e.g., specify a time period, a specific value, an aggregation, etc.)\nChange your data structure so that it can be charted\nFor this example we'll write a query against the file Bee Colony Census Data by State in the Bee Colony Statistics dataset. The only columns we care about for our chart are the year, state and value columns so that's all we'll select. We also would like to show something more meaningful as the name for the value column so we'll change that in our query to # of hives:\nAfter saving the query and opening it with Chart Builder we can create our graph. As before we select state for the x-axis, # of hives for the y-axis, and this time, because we have data from multiple years included, we use Color for year:\nOne thing you might notice about this chart is that the data on the number of hives is pretty compressed. We can adjust the size in the chart size options on the left or by clicking on the backwards L symbol in the bottom right corner of the screen and dragging the window to be the size you would like:\nAnother handy thing you can do in the options section is order the results in your graph. You may have ordered them already in your query results, but that order does not carry over to the graph. To sort by the state with the most hives select Options on the x-axis, choose Descending for the sort and y - # of hives on the sort field:\nChart-builder visualizations on data.world can be downloaded in a number of json, image, and html formats shown under the Download button:\nThere are also a variety of options for sharing/saving your visualization on data.world:\nSelecting Share > Insight lets you add your visualization to any project on data.world for which you have permission. To share the insight you are prompted to chose a project where you will share it (the default is your current project), to give it a title, and optionally to add comments:\nShare > File lets you add the visualization to any dataset or project for which you have permission. The easiest way to choose your dataset or project is to click Select to the right of the Dataset/Project field to get a dropdown list of all the datasets and projects to which you have access:\nShare > Markdown Embed (Comment) you can embed your chart in any place which accepts Markdown (e.g., insights, comments, summaries). Text fields in data.world now default to the Simple Text Editor instead of to Markdown so to add the copied markdown-embed from Chart Builder to a text field you'll first need to switch the editor to Markdown from the option in the upper right corner of the text field:\nPaste the embedded code and when you select Done, the chart will display in the text field. More information on how to use Markdown on data.world can be found in our article on Markdown.\nFinally, if you want to share a link to the Chart Builder screen for the visualization so someone else can edit and run it, you can do so with the Share > URL option:\nYou can see the above example on the discussions page for the project. Note the use of Markdown stying to make the comment more readable. Detailed instructions for using Markdown on data.world can be found here.\nAn expired token can cause one to receive a \"Error loading data.\" message when opening Chart Builder. To remedy this:\nClick on your account avatar on the top right corner of data.world and go to 'Your integrations'\nSelect the Chart Builder tile\nOn the Chart Builder page, select the Manage tab\nClick the Revoke button and disconnect the Chart Builder integration\nClick the Enable integration button and authorize access\nRe-launching Chart Builder will now allow it to fetch the data successfully.\nFrom the workspace of your tutorial project, select the file Bee Colony Census 2017 by State in the Bee Colony Data 2017 dataset .\nAdd the Chart Builder integration to your account (select Open in app at the top right of your screen). (Skip this step if you already have Chart Builder installed.)\nOpen the file with Chart Builder and set the x-axis to state and the y-axis to value.\nName the chart 2017 census by state and share it to your project as a file.\nHere is a link to the results of this exercise in our sample project.\nFrom the workspace of your tutorial project, write a query against the file Bee Colony Census Data by State in the Bee Colony Statistics dataset which returns the year, state and value columns. Rename the value column to # of hives.\nOpen the results with Chart Builder and create a bar chart with state for the x-axis, # of hives for the y-axis, and year for Color.\nSort the graph results from fewest to most hives.\nResize the chart window to 1028 wide X 760 high.\nName your chart Hive census by state and year.\nShare the chart to your project as the file Hive_census_by-state_and_year.vl.json.\nHere is a link to the results of this exercise in our sample project.\nVisualizations are a great way to bring people into your project as they are accessible to people from all different levels of data analysis. Chart Builder gives you the ability to create quick visualizations of your data that you can share and save for use in your analysis. For a deeper, more powerful visualization experience, try our integration with Tableau.\nOverview of basic tutorials - An overview of the basic tutorials\nData visualization with Chart Builder - An article that covers the content of this tutorial in depth and with a bit more detail\nUsing multiple columns of data in Chart Builder visualizations - How to use UNPIVOT to create Chart Builder visualizations using more than one column of data\nBee Colony Statistics - dataset used in the data.world tutorials\nBee Colony Data 2017 -dataset used in the data.world tutorials\nCreate a project to work with data - Another tutorial\nBI and Visualization tools - BI and visualization integrations for data.world\nVega-Lite website - More information about Vega-Lite\nQuery your data - A tutorial that introduces querying on data.world\nMarkdown documentation - How to use Markdown on data.world\nTableau integration - Link to connect Tableau to your data.world account",
    "url": "https://docs.data.world/en/98467-produce-visualizations-of-your-findings.html"
  },
  {
    "title": "Advanced work with queries",
    "content": "In the tutorial Query your data we introduced the query languages used on data.world and the way queries are written and formatted in our query editor. In this tutorial we go to the next level as we discuss the different ways queries can be used. If you need help learning a query language in order to write queries, see our SQL documentation and tutorial, or our SPARQL documentation and tutorial.\nAfter working through the tutorial you should be able to:\nUse other people's queries\nSave a query to a dataset\nMake a template from a query\nDownload query results\nSave query results to a dataset or project\nFind information on how to work with third-party applications\nRequirements\nTo complete this tutorial you need to have:\nA data.world login (available for free here if you don't have one).\nYour own tutorial project (you must create this yourself--it cannot be downloaded).\nThe Bee Colony Statistics dataset linked to your project\nIf you need help creating the project or linking the dataset to it, detailed instructions are in the tutorial Create a project to work with data.\nIf you prefer to go straight to the exercises, click here\nAs you work in projects with other people, you might find a query that you want to use and edit, but it's not your query so you don't have permissions to save changes to it. The easiest thing to do is to make a copy of the query by selecting the three dots to the right of the query name in the project workspace and selecting Save a copy:\nYou'll be prompted to name the query and write a description. For tracking lineage it's a good idea to include the name of the original query in the description. Other save options include where--the current project or the original dataset--and who can see the new query (hidden by the location selection in the image below:\nNote that if the original query is a project query (listed in the queries at the bottom of the Project directory in the left sidebar), you can only save the query to the original project--if you want to use it in a different project, you will have to copy the query text and paste it into a new query window in the other project. Here, too, it's a good idea to copy the original source of the query in case you want to go back to it later.\nMost of the time you'll want your queries to be associated with a project. However if the query would be useful to others who create projects from a dataset, it might make more sense to save it directly to the dataset. Saving it to the dataset and sharing it with everyone enables anyone with access to the dataset to use it in their own projects. It's also the easiest way for others to save the query to their own projects using the dataset. One note: queries saved to datasets show up in the Connected datasets section of the Project directory in the left sidebar--not under Queries:\nTo save a query to a dataset, click the menu icon to the right of the field Where will the query be saved? and select the dataset where you want to save it:\nAnother way to use a query is to make a template from it so you can change the input strings to various parts of it. For example, if we had a query that we could turn the query we just saved as Bee colony census by state and year into a query template by selecting the New template link at the top right of the query panel:\nA dialog window will pop up giving you basic instructions to how query templates work and linking you to the documentation for query templates:\nClick the Add a sample statement button which will add a sample of the DECLARE statement used in query templates, a sample comment, and a link to the documentation to the top of your query window:\nWith just a couple of modifications to your SQL you can have a query that other users can use to input values for a field without ever having to know SQL themselves. For example, to change the query above into a query template that returns the results for any specified census year, all you need to do is:\nInsert a new line above the DECLARE statement and enter a comment holding instructions for what to enter in the variable field\nChange the DECLARE clause so that the variable ?year is defined as an integer\nModify the WHERE clause to match where year = the year variable\nThe contents of the query editor will look like this:\nYou can preview the query to see what other users will see and to test the query yourself by selecting the Preview query template link in the upper right corner of the query panel. Then fill in a value and hit Run query:\nAnother way to use a query is to download the results from it in either a csv or an xlsx file. This option is useful if you want to work with your data file in a spreadsheet program. After running your query, click the Download link below the query window and select the option you want:\nAnother way to work with your query results is to save them as a new table in a dataset or project. For those familiar with creating a View in a SQL database, this is quite similar. When the results are saved into a table they can be queried just like any other file--eliminating the need for extremely complex queries. You can save the table in your current project, in an existing dataset, or in a new dataset. Select Save to dataset or project from the dropdown menu on Download, name your file, and either choose a project or dataset from the dropdown menu, or begin typing the name of another one:\nThere are other reasons you might want to save the results of your query in a new tabular file. See the article on working with query results for more information.\nTwo other ways you can use the results of a query are to make them available for download from a URL, or to embed them on a Markdown page. Both options are found in the Download menu under Copy URL or embed code. The URL is a download link for a csv file of the results, and the embed code option renders the results table in a Markdown page.\nThere are many applications you can use to analyze and display the results of your queries, and we make it easy to work with your favorites. You can find out about the applications we integrate with on our integrations page. If you already have an integration configured that you want to use or want to integrate a new application, the name of the first application in your list of integrations will show up as a menu to the right of the Download button:\nOpen the workspace of your tutorial exercises project.\nExpand the view of the contents for the Bee Colony Statistics dataset and make a copy of the query Bee colony census by year for Montana (click on the three dots to the right of the query name in the left sidebar and select Save a copy).\nSave the query to your project with the name Bee colony census by year for individual states.\nWith the query Bee colony census by year for individual states open, select the New template link from the top right of the query editor\nselect Add a sample statement.\nUse cut and paste to move the first line of the modified query (the DECLARE statement) down below the two comment lines.\nChange the text in the first comment line to State names must be entered in ALL CAPS (leave the # at the beginning of the line to denote that it's a comment, not a command), and delete the entire second comment line (# and all).\nReplace ?column in the DECLARE statement with ?state.\nReplace \"Montana\" in the WHERE clause of the query with ?state.\nSelect the Preview query template link in the upper right of the query editor. You should see a screen like this:\nTest your query by entering a state name in the blank field and running the query (note the comment above the selection field).\nSelect edit query if you need to make changes.\nIf your query returns the correct results, save it.\nClick here to see what the query should look like.\nUsing the Preview query screen on the query Bee colony census by year for individual states (open at the end of the last exercise), enter a state name and run the query.\nSelect Save to dataset or project from the Download link at the bottom of the query editor.\nChange the file name to Bee colony census by year for <your state name> (e.g., Bee colony census by year for Hawaii), and save the file to your project.\nThe variety of ways to access data and the ease of querying it are some of the most powerful features of data.world. We introduced the basics of querying in the tutorial Query your data, and in this tutorial we dove into the ways you can use queries to get the most out of your data. The best way to become comfortable with all the options available when querying data is to practice. If you haven't looked through our SQL or SPARQL documentation yet, now would be a good time to do it to get a feel for just how much you can do. If you're new to SQL, try our SQL tutorial to get the most out of querying with it on data.world.\nOverview of basic tutorials - An overview of the basic tutorials\nQuery your data - The basic tutorial that introduces querying on data.world\nSQL documentation - Complete coverage of how to use SQL on data.world\nSQL tutorial - An introduction to and instructions on how to use the exercises in the data.world SQL documentation as a tutorial\nSPARQL documentation and tutorial - Documentation on using SPARQL on data.world with a built-in tutorial\nBee Colony Statistics - The base dataset used in the tutorial\nBee Colony Census 2017 by State - A secondary dataset created in a previous exercise, also used in the tutorial\nCreate a project to work with data - Another tutorial\nDocumentation for query templates - More information on how to use query templates\nSaving query data to a new file - Why saving a query to a new table is important and how to use itSave the results of a query as a new table\nIntegrations - A full list of the current integrations available on data.world",
    "url": "https://docs.data.world/en/98479-advanced-work-with-queries.html"
  },
  {
    "title": "Configuring your network",
    "content": "When connecting your data systems to data.world, often your system or firewall may have a network policy that only allows access from specific IP addresses--your allowlist. In those cases, you need to add data.world to your network policy IP allowlist.\nAllow the following IP addresses:\n52.3.83.134\n52.205.195.10\n52.205.207.86\nOccasionally, the firewalls block some of the functionality on data.world, including, but not limited to, uploading files to the platform. If you suspect this is the case, we recommend contacting your network administrator to allow the following domains:\ndataworld-uploads-p-us-east-1.s3-accelerate.amazonaws.com\ndataworld-uploads-p-us-east-1.s3-accelerate.amazonaws.com\nuploads.data.world\nIf you are not sure the issues you are experiencing\u00a0are due to firewall restrictions, try performing the same tasks on a different network, such as your home internet connection.",
    "url": "https://docs.data.world/en/98612-configure-your-network.html"
  },
  {
    "title": "Get Started",
    "content": "",
    "url": "https://docs.data.world/en/98581-get-started.html"
  },
  {
    "title": "Overview of data.world",
    "content": "This overview of data.world is good both for people who are new to the platform and also for those who want an in-depth explanation of all the components of each screen. It is organized from the highest level view down to the granularity of the tabs for each screen.",
    "url": "https://docs.data.world/en/98582-overview-of-data-world.html"
  },
  {
    "title": "Home page",
    "content": "The home page (Eureka Action Center\u2122) of the application shows the most useful information in the form of widgets. Typically, you will see the following widgets on your home page.\nBrowse card: A browse card is a quick navigation card added to the application home page to help navigate users to the important organizations, collections, datasets, and other resources in the organizations. It is an optional feature that can be set for private and single-tenant installations. For details, about how it is set up, see this documentation.\nMy organizations: List of all organizations you have access to. Hover over the icon for the organization to see its name and your access level. Click the name of the organization to navigate to the Organization Profile page.\nPending alerts: Act on the alerts that need your attention or click on See all alerts and notifications to go the the Notifications page to manage all notifications and alerts.\nRecent activity: Activity feed of users and organizations you follow. To go to the list of recent activities for a particular organization, toggle to the organization from the All activity dropdown.\nMy recently viewed: Quick links to resources you have recently interacted with.\nMy recent bookmarks: List of your bookmarked resources.\nHelpful resources: Links to documentation.\nCommunity users may see Announcements, Trusted data providers widgets.",
    "url": "https://docs.data.world/en/98583-home-page.html"
  },
  {
    "title": "Main header",
    "content": "The main header on most data.world pages consists of five elements:\ndata.world logo: Click it to come back to the product home page.\nSearch bar: Global search for all resources available in data.world.\n+New button: A quick link for creating datasets and projects.\nNotifications icon:  Gives access to notifications and alerts. This icon changes to a number when you have any notifications.\nUser avatar: Click it to access your profile, your settings, and to log out of the application.",
    "url": "https://docs.data.world/en/98584-main-header.html"
  },
  {
    "title": "Left sidebar",
    "content": "The left sidebar is the Global Navigation menu. Use the Global Navigation to access the various resources available to you. The icons displayed vary depending on whether you are in an organization or not.\nIf you are a member of at least one organization, you will see the Organizations option in the Global Navigation menu. Click this menu item to access the list of your organizations. If you have access to only one organization, you are directly taken to the home page of the organization.\nCreate new organizations from the Settings  page available from the User menu in the main header.\nClick the tile of an organization to view the profile page, which lists the resources, glossary, and collections in the organization. Depending on the components that the organization uses, these resources include datasets, projects, reports, tables, members, data connections, organization settings, and curated collections of the organization's resources.\nClick the Discover icon to see all the resources you have access to. Discover directs you to the Resources tab with a list of all the resources you have access to already loaded and ready for filtering and searching. Switch to the Organizations and people, Comments, or Columns tabs to discover more.\nOn the Resources and Organizations and people tabs the resources that are owned by you are shown on top followed by other resources you have access to. Use Filters to narrow down your lists - the best place to start is with the Resource Type  and Owner filters to narrow down the list of available resources.\nThe All tab is a special tab where you can quickly navigate to the last 25 resources you viewed recently. Users in an environment with a Browse Card will also see their quick links on the All tab, for a familiar entry point to the enterprise resources.\nYou can always use the Search bar on top of the Discover page to find what you are looking for. For details about using search and advanced filters, see How to find data resources\nNote that once you run a search, the All tab adapts to show a special view of the search results. Once you clear the search term from the search box, the tab switches back to the Discover mode.\nClick the Bookmarks icon to get to the resources you have bookmarked for easy access. If you have a long list of bookmarked resources, use the filter option to narrow down the list.\nUsers can bookmark -\u00a0datasets,\u00a0projects,\u00a0insights,\u00a0tables, custom resources, collections, business terms. Note that Columns cannot be bookmarked.\nFrom the Bookmarks page, you can:\nOpen the datasets,\u00a0projects,\u00a0insights,\u00a0tables, custom resources, collections, business terms by clicking on it.\nSee who created the resource and when it was last updated.\nUse the Info icon to see the Information Card for the resource. It gives you preview of what all is included in the resource and you can use this information to then decide if you need to navigate to the resource to explore it further.\nUse the Three dot menu to access the following additional options for the resources.\nUnbookmark: To remove the bookmark. This option is vailable for all resources.\nOpen in workspace: To open a project in workspace, or to open a new project workspace for a dataset.\nAdd to project: To add a dataset to an existing project.\nPreview: To preview an insight and then navigate to it.\nClick the Notifications icon to get the list of alerts and notifications that you have received.\nFrom the Notifications page, you get access to:\nAlerts: Alerts\u00a0are actions you can take\u00a0right now\u00a0and will not dismiss until someone does something about it. These include invitations for you to join an organization, requests to access datasets and project. You can approve or reject these requests and invitations from this page.\nNotifications: Notifications\u00a0are updates about things happening on the platform, that you\u00a0may\u00a0want to take follow up actions on. These include informational messages and a list of changes suggested by other members to make improvements to existing datasets. Click through to these suggestions to review and approve or reject the changes.",
    "url": "https://docs.data.world/en/98585-left-sidebar.html"
  },
  {
    "title": "Organization profile page",
    "content": "Each organization has a Profile Page, which is accessible by clicking the Organization's name from anywhere in the application.\nThe Profile Page has three sections:\nHeader\nAbout information\nTabs for viewing Overview, Resources, Glossary, Activity, Collections, Members, Followers, and Settings of the organization.\nFor complete details about what is included in organizations and how to manage them, see Managing organizations",
    "url": "https://docs.data.world/en/98602-organization-profile-page.html"
  },
  {
    "title": "Datasets",
    "content": "The Datasets tile on the Organization Profile Page is your portal to all the datasets owned by your organization and the metadata about them. If your organization uses our metadata catalog, all the metadata captured for your organization's data sources can be found here. This is also where all of the datasets containing your data--both stored on data.world and from virtual connections--are located.\nClicking the Datasets tile opens the Resources tab that shows a filtered list of all your organization's datasets. There you can see:\nName of the dataset\nWho created it\nWhen it was last updated\nOther details, such as a brief description of the dataset, the number of files and tables in the dataset, and the number of projects that use the dataset.\nYou can use this information to then decide if you need to navigate to the dataset to explore it further.\nSome of the options available to you on this page are:\nPowerful filter options that let you narrow down the list of datasets available to you.\nA search box to search for specific datasets within the organization.\nA New Resources button to create new datasets, projects, or Analysis.\nA Three dot menu to access the option to Edit multiple datasets.\nWhen you click the dataset and open it, you see the Dataset Details page. The header at the top of the page has several valuable pieces of information and actions that you can take in regards to the dataset.\nOn the left of the header are:\nAn icon for the dataset that includes on indicator for the status of the dataset. For example, Approved, Deprecated. Needs Review.\nThe name of the person or organization that owns the dataset. Click the owner link to get to their profile page.\nThe name of the dataset.\nOn the right of the header are options to:\nShare the dataset\nBookmark it (and see how many others have bookmarked it)\nRequest access button, if you don't have access to the dataset.\nThree dot menu to access the option to subscribe or unsubscribe from email notifications for the dataset (available if you don't have access to the dataset.)\nAccess the Explore this dataset menu (available if you have access to the dataset.). It includes options to:\nSee all the projects that use the dataset\nConnect the dataset to an existing project\nOpen the dataset with a third-party application (integrated with data.world)\nCreate a new project using the dataset",
    "url": "https://docs.data.world/en/98591-datasets.html"
  },
  {
    "title": "Overview tab",
    "content": "When you open a dataset, you land on the Overview tab. The Overview section includes the Status of the dataset, if it is set, a short Description of the dataset, and the longer Summary information.\nMuch of the information on the Overview tab is configurable by your organization. In the following example, the More Information section contains an example of a completely configured metadata.\nThe next section includes a list of related resources(tables, business term, and Analysis) for the dataset. You can add other related resources by using the Add a related resource button.\nThe Files section lists the number of files in a dataset and includes a preview of each file with options to manage the files.\nThe Related datasets section includes datasets that might be of interest to you.",
    "url": "https://docs.data.world/en/98592-overview-tab.html"
  },
  {
    "title": "Right sidebar",
    "content": "An About this dataset section appears on the right side of the Overview and Activity tabs that contains both default and custom metadata about the dataset. In the following example, this metadata includes:\nThe sharing settings.\nWho created the dataset and when.\nThe various people who manage the dataset (Steward and Tech Owner).\nWho verified the dataset and when.\nThe tags used on the dataset.\nThe size of the dataset.\nThe data dictionary information about the dataset and a link to the dictionary.\nA list of the queries that are part of the dataset.\nThere are Related projects and Recent updates sections which list projects that use the dataset and recent updates to the dataset, respectively.",
    "url": "https://docs.data.world/en/98593-right-sidebar.html"
  },
  {
    "title": "Discussion tab",
    "content": "The Discussion tab captures all the communication between team members about the dataset. You get an option to Flag or Like comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nThe contents on the Discussion tab are grouped into topics. Use the default topic or start a new one. Define the discussion topic, the category (General or To dos), and the first comment for the topic.\nComments in topics are written in Markdown and can contain images and links. You can mention users (using @user_name) to notify them about an ongoing discussion to get their attention and participation. Some important things to note:\nWhen you do this for private resource only available in an organization, typing @ suggests only the members of that organization. It also shows a prompt to contact an administrator of the organization to ask them to allow others to have access to the organization. If you are an administrator, you get the option to invite other people to the organization.\nFor community resources, you get a list of suggested users from the community.\nIn both cases, only the users who have at least read access to the resource are notified. So make sure that when you are setting access for the users, you also give them access to the resource where they need to participate in the discussion.",
    "url": "https://docs.data.world/en/98594-discussion-tab.html"
  },
  {
    "title": "Activity tab",
    "content": "The Activity tab contains information about all events that occur with the dataset, and it also provides access to previous versions of the dataset. There are two sub-tabs, All Activity and Versions.\nThe All Activity sub-tab is the default, and list all the activity for the dataset.\nThe Version sub-tab contains a list of previous versions. Click the Three dot menu to download a specific version.",
    "url": "https://docs.data.world/en/98595-activity-tab.html"
  },
  {
    "title": "Settings tab",
    "content": "The Settings tab is for administrators to manage the dataset. There are three sub-tabs, General, Access and ownership, and Webhooks.\nThe General sub-tab contains administrative settings, which include:\nAutomatic syncing options.\nAdditional notification recipients.\nArchive options.\nA Delete dataset button.\nThe Access and ownership sub-tab allows you to manage access to the dataset and make datasets visible to community members.\nThe Webhooks sub-tab is for configuring and managing webhooks to the dataset.",
    "url": "https://docs.data.world/en/98596-settings-tab.html"
  },
  {
    "title": "Projects",
    "content": "The Projects tile on the Organization Profile Page is your portal to all the projects owned by your organization. From here you can access your organization's data analysis projects and the metadata around them.\nClicking the Projects tile opens the Resources tab that shows a filtered list of all your organization's projects. There you can see:\nNames of the projects\nWho created them\nWhen they were last updated\nOther details, such as, a brief description of the project, list of linked datasets, number of files and tables in the project.\nYou can use this information to then decide if you need to navigate to the project to explore it further.\nSome of the options available to you on this page are:\nPowerful filter options that let you narrow down the list of projects available to you.\nA search box to search for specific projects within the organization.\nA New Resource button to create new datasets, projects, and analysis.\nA Three dot menu to access the option to Edit multiple projects.\nWhen you click on the project and open it, you will see the Project Details page.",
    "url": "https://docs.data.world/en/98597-projects.html"
  },
  {
    "title": "Business terms",
    "content": "This feature is available only for organizations that have the\u00a0Enterprise\u00a0license.\nThe business glossary of the metadata catalog on data.world brings your business terms into the heart of working with your data. Since different business units can have their own glossary terms, each organization can also have its own glossary.\nAny business glossary for any organization is accessible from the Glossary section available on the Organization Profile Page.\nWhen you open the glossary, you see a list of terms and options to create new business terms. You may also see a variety of other metadata information custom-configured for your catalog. In the following example, you can filter business terms using Collections and Tags. Keep in mind that the glossary for each organization contains different, configurable, metadata elements.\nWhen you click on the term and open it, you will see the Glossary Details page.",
    "url": "https://docs.data.world/en/98598-business-terms.html"
  },
  {
    "title": "Collections",
    "content": "Collections are organization-defined groupings of resources. They are very useful for organizing the resources in useful categories. Note that the same resource can belong to different collections. Certain resources, like Business Terms, require a collection.\nOn the Organization Profile Page you can view tiles for all of your collections. To manage all the collections in an organization, go to the Collections tab. Here you can use filters to narrow down the list of available collections and create new collections using the New button. For complete details about collections and how to manage them, go here.",
    "url": "https://docs.data.world/en/98608-collections.html"
  },
  {
    "title": "Updating your profile settings",
    "content": "Your data.world Settings page allows you to configure a number of options in the following categories:\nProfile\nAccount\nOrganizations\nBilling\nNotifications\nAdvanced\nUpdate these settings by clicking on your profile image in the top right corner of the top toolbar and select Settings:\nThe entries in this section appear on your profile page and will be visible to other data.world users. Settings in this section include:\nFull name (required)\nCompany or organization\nWebsite\nBio\nPhoto: Click the\u00a0Add a photo\u00a0button to upload a new image for your profile. Once you upload the image, use the\u00a0Edit file\u00a0button to resize and adjust the properties of the image.\nUpload images that have an aspect ratio of 1:1 (square). The largest display allowed is 260 by 260 pixels so an image uploaded at that size is recommended. Images that are a lot smaller may not display correctly and can come out pixelated.\nEmail address: Only one account is allowed for each email address registered on data.world. When changing your email address, we will send you a verification e-mail to the new address. Follow the instructions in that email to verify the new address.\nPassword: Set up a new password for your account.\ndata.world currently doesn't support changing your username. If you need it updated or run into any login issues, please submit a support ticket.\nCreate a new organization\nLeave any organizations that you are a part of\nIf you have the appropriate access level, manage the membership of the organization and modify the subscription level of the organization\nModify the subscription level of your individual account\nModify the subscription level of any organizations in which you are an administrator\nUpdate credit card information for a subscription\nManage email notifications for projects, datasets, organization email digest, discussions, confirmations.\nAccess and reset account-wide API tokens\nRevoke access to any authorized integrations\nEnable experimental features",
    "url": "https://docs.data.world/en/99211-updating-your-profile-settings.html"
  },
  {
    "title": "Staying updated about your data catalog",
    "content": "To help you stay on top of what's happening with your data and in your organization, data.world provides a variety of notifications in different formats to various users. To make the notification process more transparent, we have the following tables which lay out the relationships between user and organization permissions, activity in the platform, and notification formats.\nThe application provides an easy way to unsubscribe from email notifications that you get from data.world.\nAction in application\ndata.world permission level\nIn-app notifications\nEmail notifications\nHomepage activity feed\nIncluded in daily organization digest\nWebhooks\nDataset created\nManage, Edit, View, Discover\nNo\nYes\nYes\nYes\nN/A\nDataset updated\nManage, Edit, View, Discover\nNo\nNo\nYes\nYes\nN/A\nProject created\nManage, Edit, View, Discover\nNo\nYes\nYes\nYes\nN/A\nProject updated\nManage, Edit, View, Discover\nNo\nNo\nYes\nYes\nN/A\nQuery published to a dataset\nManage, Edit, View\nNo\nNo\nNo\nYes\nN/A\nQuery published to a project\nManage, Edit, View\nNo\nNo\nNo\nYes\nN/A\nInvitation to a dataset\nManage, Edit, View, Discover\nYes\nYes\nNo\nNo\nN/A\nInvitation to a project\nManage, Edit, View, Discover\nYes\nYes\nNo\nNo\nN/A\nComment made on a dataset from the Discussion tab\nManage, Edit, View\nNo\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for the specific dataset)\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for the specific dataset)\nNo\nN/A\nComment made on a project from the Discussion tab\nManage, Edit, View\nNo\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for the specific project)\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for the specific dataset)\nNo\nN/A\nRequest accessed for a dataset or project\nManage (dataset or projects)\nYes\nYes\nNo\nNo\nN/A\nDataset sync failure\nN/A\nN/A\nN/A\nN/A\nN/A\nYes\nDataset recovery success\nN/A\nN/A\nN/A\nN/A\nN/A\nYes\n\nAction in application\ndata.world permission level\nIn-app notifications\nEmail notifications\nHomepage activity feed page\nIncluded in daily email digest\nUpdate made to a resource\nManage, Edit, View\nNo\nNo\nNo\nNo\nResource created\nManage, Edit, View\nNo\nNo\nNo\nNo\nComment added on a resource from the Discussion tab\nManage, Edit, View\nNo\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for catalog resources.)\nYes (if you are mentioned in the comment, or are participating in the comments, or have manage permission for catalog resources.)\nNo\nInvitation to a resource\nCurrently it is not possible to be invited to access specific resources or be granted permissions to specific resources.\nN/A\nN/A\nN/A\nN/A\nAction in application\ndata.world permission level\nIn-app notifications\nEmail notifications\nHomepage activity feed page\nIncluded in daily email digest\nInvitation to an organization\nAll\nWhen an organization is invited to join another organization, only the organization administrators get an email and in-app notification to accept or reject the invitation on behalf of the organization.\nYes\nYes\nNo\nNo\nRequest to join an organization\nOrganization administrators\nYes\nYes\nNo\nNo",
    "url": "https://docs.data.world/en/99145-staying-updated-about-your-data-catalog.html"
  },
  {
    "title": "Managing email notification subscriptions",
    "content": "The application provides an easy way to unsubscribe from email notifications that you get from data.world.\nThis section lists the different categories of email notification you get from data.world.\nThere are a few emails that data.world sends to users that are essential for using the application and you cannot unsubscribe from these emails.\nNotification email\nDetails\nWelcome email\nSent after sign up to new user as initial welcome.\nEmail address verification\nsent after sign up to new user to confirm their email address\nEmail change request\nSent to user to confirm changing their email to a new email address.\nPassword change request\nsent to end users when they request to change their password\nPassword reset request\nsent to end users when they have forgotten their password and need it reset\nInvite to join data.world or an organization in data.world\nemail invite to join data.world + join an organization (recipients are not yet users on data.world)\nInvite to contribute to datasets and projects\nEmail invite to join data.world and have a dataset/project shared with you (recipients are not yet users on data.world)\nBilling receipt\nSent to billing contact when autopay succeeds.\nBilling attempt failed\nSent to billing contact when autopay fails.\nSPC activation email\nsent to users of the Snowflake partner connect to activate their trial.\nNotification email\nDetails\nUser likes a comment\nSent to the comment author when someone likes a comment.\nUser likes a dataset or project\nSent to dataset or project owner when someone bookmarks a dataset or project.\nUser likes an insight\nSent to the insight creator when someone bookmarks an insight.\nNew dataset or project created\nSent to followers of a user when a new dataset or project is created by the user.\nSent to followers of an organization when a new dataset or project is created by in the organization.\nNew follower for a user\nSent to individual user when someone follows them. Does not go to organization administrators when someone follows an organization.\nNotification email\nDetails\nInvitation to join organization\nSent to end users when they are invited to join an organization.\nRequest to join organization\nSent to administrators when an end user requests to join an organization.\nOrganization invited to another organization\nSent to organization administrators when that organization is invite to join another organization.\nChange suggested to resources\nSent to users with editors and above access to metadata resources when a viewer suggests a change.\nContributor invited by administrator\nSent to invitee when an administrator invites someone to view or contribute directly to a dataset or project.\nAccess requested to dataset, project, or collections\nSent to dataset, project, or collections administrators when someone requests access to view or edit the dataset, project, or collections\nDataset transfer requested\nSent to data admins or new owner destination to transfer a dataset or project to a new owner.\nInvite organization as contributor\nsent to org admins when an org is invited to contribute to a dataset/project/collection\nNew task available\nsent to parties authorized to claim a task for workflows\nTransfer requested\nSent to organization administrators when someone proposes a crowd-sourced resource.\nNotification email\nDetails\nUser approved to join an organization\nConfirmation when a user is approved to join an organization\nUser accepts invitation to organization\nSent to organization administrators when a user accepts an invitation to join the organization\nContribute request approved by administrator\nSent to requester when an administrator approves an access request for a dataset or project\nContribution approved\nSent to requester when a suggested change or an ownership transfer is approved\nNew user group created\nSent to organization administrators when a new custom user group is created.\nNew user given access to dataset or project\nSent to dataset or project owner or administrator when a new user gains access to the dataset or project.\nDataset, project, or collection shared with a user group\nSent to group members when the group is added directly to a dataset, project, or collection\nDataset, project, or collection shared with an organization\nSent to all members of an organization when the organization is added directly to a dataset, project, or collection\nWorkflow updates\nsent to the initiator of a workflow when a status has changed\nNotification email\nDetails\nNew comment added\nSent to users with viewer and above access to a dataset, project, insight, metadata resource when someone comments on the resource.\nNew topic added\nSent to users with viewer and above access to a dataset, project, insight, metadata resources when a new topic is added.\nNotification email\nDetails\nA user mention in comments\nSent to individual user mentioned in a comment.\nNotification email\nDetails\nOrganization digest email\nSent to all members of an organization with a summary of activity they have access to.\nNotification email\nDetails\nDataset or project updated\nsent to viewers, contribs, and users that have bookmarked a dataset/project when the details for that dataset have been edited\nFile uploaded to a dataset or project\nsent to viewers, contribs, and users that have bookmarked a dataset/project when files have been uploaded to the dataset/project\nInsight created\nsent to viewers, contribs, and users that have bookmarked a project when a new insight is added\nSync status change\nsent to specific email identified as notifications contact when there is a sync status change for a dataset\nTo manage email notifications:\nClick your\u00a0Profile\u00a0icon on the right side of the top navigation.\nGo to the\u00a0Settings\u00a0page, then\u00a0Notifications.\nOn the Notifications page, toggle the Email notifications enabled button to turn on or off all email notifications.\nThis will not unsubscribe you from essential notification emails from data.world.\nAlternatively, turn off email notifications for specific types of notifications.\nFor Dataset & project activity email notifications you can either turn off/on all notifications or you can click the Customize button and turn off/on individual notifications for each datasets and project .\nLikewise, for Organization activity digests, click Customize button if you want to have granular control over notifications for specific organization digest emails.",
    "url": "https://docs.data.world/en/99115-managing-email-notification-subscriptions.html"
  },
  {
    "title": "Manage organizations, user groups, and members",
    "content": "",
    "url": "https://docs.data.world/en/96552-manage-organizations,-user-groups,-and-members.html"
  },
  {
    "title": "Managing organizations",
    "content": "",
    "url": "https://docs.data.world/en/96553-managing-organizations.html"
  },
  {
    "title": "About organizations",
    "content": "An organization enables you to form a mirror of the departments of your company in data.world. An organization administrator has the power to control and manage the resources available in each organization:\nMembers of the organization\nCollections of resources\nOrganization level connections to external sources\nDatasets\nProjects\nBusiness Glossary\nYou can set one Organization for your company or create one organization for the main company and create additional organizations for units of business in the organization.\n",
    "url": "https://docs.data.world/en/96554-about-organizations.html"
  },
  {
    "title": "Organization profile page details",
    "content": "Each organization has a Profile Page, which is accessible by clicking the Organization's name from anywhere in the application.\nThe Profile Page has three sections:\nHeader\nAbout information\nTabs for viewing Overview, Resources, Glossary, Activity, Collections, Members, Followers, and Settings of the organization.\nThe header contains:\nThe avatar for the organization, which also serves as a link to the organization's profile page.\nBoth the descriptive name and the username of the organization.\nA button to Follow the organization.\nThe About section displays key information about the organization, such as a list of administrators, the number or resources, members, and followers, and a list of recent activities for the organization.\nAll of this information is configurable for your organization (including the About information on the right).\nThe Overview tab of the Organization Profile Page has two sections: Overview and Resources. Organizations with an Enterprise license get access to the Glossary and Collections  section as well.\nThe Overview section includes the Description and Details of the organization. Details can be written either with Markdown or by using the powerful Simple Editor that is part of the application.\nIf configured, next you see the Browse card for your organization. A browse card is a quick navigation card that helps navigate users to the important collections, datasets, projects, and other resources in the organization. For details about setting it up, see this documentation.\nNext is the Resources section that includes:\nA list of all of the data resources owned by the organization. These include datasets, projects, Tableau dashboards etc. Click All resources to go the Resources tab to see a detailed view of the resources.\nA search bar to locate resources easily.\nA button to create a new resource.\nThe Glossary section provides access to the business terms and other glossary terms of the organization. Click the All glossary terms tile to go the Glossary tab to manage the glossary.\nThe Glossary section appears for Organizations with an Enterprise license.\nCollections are groupings of resources created by the members of the organization. The purpose of collections is to facilitate access to the large amounts of data that enterprises have. You get an option to create new collections from this screen. The collections shown in the following sample image are just examples of the kind of categorizations you can do. Click the All Collections tile to go the Collections tab to manage the collections.\nThe Collections section appears for Organizations with an Enterprise license.\nOn the Resources tab, view a list of all the resources owned by the organization sorted in alphabetical order (Name(A-Z). Use the Filters  and the Search bar available on the top to narrow down the list of resources. Use the New button to add new Datasets, Projects, Analysis, and Custom resources. From here, you also get an option to bulk-edit resources.\nThe Glossary tab is available only for organizations with an Enterprise license\nOn the Glossary tab, view a list of all the glossary terms for the organization sorted in alphabetical order. Use the Filters and the search bar available on the top to narrow down the list of terms. Use the New button to add new glossary term. From here, you also get an option to bulk-edit glossary terms.\nThe Collections tab is available only for organizations with an Enterprise license.\nOn the Collections tab, view a list of all the collections owned by the organization sorted alphabetically. Use the Filters  and the search bar available on the top to narrow down the list of collections. Use the New Collection button to add new collections.\nAs you would expect, the Activity tab on the organization profile shows a list of all the activities across the entire organization\u2014including comments on insights, activity on datasets, uploading files, and more.\nClick the links to go to the topic, the resource, or the person who made the change.\nThe Members tab on an organization profile contains a list of members and user groups in the organization.\nIn the Members section, you can view the linked list of all the people associated with an organization. You can click through to the member profile to view their resources, activities, followers etc. and choose to Follow  members they find interesting. If you have the appropriate permissions, you will be able to edit the default access for all members and add or remove members from the organization.\nThe list of member can be sorted by the Name column. When you are dealing with a long list of members, you can narrow down the list by using the Filter feature. This can be done in two ways:\nClick the arrow in the Visibility column header to filter by Public or Private visibility.\nIf you know the display name or username of a member, locate them by using the Filter box.\nIn the Groups section, you can view the out-of-the-box and user created groups. You can click through to see the group permissions and members in each groups. If you have the appropriate permissions, you an edit the groups and create new ones.\nThe Followers tab on the organization profile page contains a list of all of the people who follow the organization. Sort the list by Username or Full Name in ascending or descending order.\nThe information includes:\nAvatars\nUsernames\nBio information\nLinks to follow the users\nYou can click through to the user profile to see the details and a list of publicly available projects and datasets created by the user.\nIf you are an administrator of an organization you will have access to the Settings tile on the organization's home page and the Settings tab on the organization's profile. There are five screens on the Settings page.\nThe first is the Profile screen.\nSet the following details of the organization from this screen:\nDisplay name\nParent company or organization\nWebsite address\nDescription\nProfile picture\nThe Connection manager screen is for managing the organization-level connections. From here you can create new connections, edit and delete existing connections, and manage tasks for connections.\nIf you need to create a personally-owned virtual data connection you will need to do that through the Integrations interface. Connection manager is only for connections that are owned by an organization.\nThe Preferences screen is for setting access permissions to the organization and setting up people outside the organization as recipients for organization notifications.\nSet the description and webhook URL on the Webhooks screen. For more information about creating and using webhooks, see the article on organization webhooks in the API documentation.\nThe Billing screen shows the organization's current plan and also has a link to update the plan.",
    "url": "https://docs.data.world/en/98989-organization-profile-page-details.html"
  },
  {
    "title": "Creating organizations",
    "content": "Once you have created an organization, it cannot be deleted.\nTo create an organization:\nClick your Profile icon on the right side of the top navigation.\nGo to the Settings page, then Organizations. Here you see the list of organizations that are available to you. Click Create an Organization to create a new organization.\nOnly users with the Instance admin permission can do this task. This permission is assigned only to select users on special request to data.world support. Please contact your system administrator if you are not authorized to do this task.\nProvide the Organization name and Organization user name. Note that the organization name can be changed later but the Organization user name cannot be edited. The username becomes a part of the URL for the organization. For example, https://data.world/new-org and is also used for mentions of the organization (for example, @new-org.)\nSelect the plan.\nNote that the Plan selection panel is not available in private instances and single-tenant installations.\nOn the next screen, click the Grant Access button to invite other members to join the organization.\nWhen you are done, click Finish. You are redirected to the Organization Profile page where you can update other details about the organization.",
    "url": "https://docs.data.world/en/96555-creating-organizations.html"
  },
  {
    "title": "Updating general details of the organization",
    "content": "After you have created the organization, you can go to the Organization profile page to configure important settings for the organization.\nUpdating general information of the organization:\nOn the Settings tab, in the Profile section, set the following generic properties of the organization:\nDisplay name: This is the name you had set while creating the organization.\nParent company or organization: The name of your company.\nWebsite: The website of your company.\nDescription: A short description of the organization.\nProfile photo: Click the Add a photo button to upload a new image for your organization. Once you upload the image, use the Edit file button to resize and adjust the properties of the image.\nUpload images that have an aspect ratio of 1:1 (square). The largest display allowed is 260 by 260 pixels so an image uploaded at that size is recommended. Images that are a lot smaller may not display correctly and can come out pixelated.\nClick the Save button. This information is displayed on the Overview tab and other tabs of the Organization profile page . To further enrich the information about the organization, on the Overview tab, click the Edit button in the Details section and add more information. You can add rich content using Markdown.\nNext, from the Settings tab, Connection Manager section, you can manage the organization-level connections to external sources of data. For details, see this documentation.\nNext, from the Metadata collectors section, you can view the collector run summary and generate the YAML files and CLI commands for collectors using the Collectors wizard. For details, see Viewing metadata collectors summary page.\nNext, from the Preferences section, set the following:\nDefine how non-members of the organizations can interact with the organization. For details, see Allowing community members to join organizations.\nDefault to private membership: Select this option to default user memberships to private mode. This ensures that other users on the community cannot see what all organizations the user belongs to.\nDefault group for new members: Select the default group in which new members of the organization should be added. The default selection is Authors. You can select from: All Members, Authors, Admins. For details about these user groups, see About user groups.\nNext, from the Webhooks section, set the description and webhook URL. For more information about creating and using webhooks, see the API documentation.\nNext, from the Billing tab, you can change the plan for your organization.\nThis tab is not available in Enterprise organizations.\nNext, from the Security tab, configure SSO for the organization. For details, see Setting up SAML for data.world.\nThis tab is available only in Enterprise organizations which are enabled for SSO. If you don't see this tab, please contact the Support team.",
    "url": "https://docs.data.world/en/97992-updating-general-details-of-the-organization.html"
  },
  {
    "title": "Setting up SAML for data.world",
    "content": "data.world University!\nCheck out our Introduction to SAML course!\nSingle Sign On (SSO) is available as an authentication method for all Enterprise installations. data.world supports SSO with providers that uses SAML 2.0 protocol, such as Okta, Google, or Azure, etc.\nPrivate Instance (PI) and Single-Tenant environments are required to use SSO for user logins.\nEnterprise customers using the Public Instance have an option to set up SSO, but it is not required.\nOnce SSO is configured, all members of the organization will use SSO to login to the platform.\nThe SSO configuration is done in collaboration with the data.world support team.\nBy default all Private Instance (PI) and Single-Tenant environments are setup with Just In Time (JIT) account provisioning. This means that once SSO is enabled, users automatically get created in data.world at the time of login, as long as the SSO provider administrator has added the data.world app to the user profile in the SSO provider administation tool.\nDone by\nTask\ndata.world\nAs part of onboarding, the data.world team initiates the process and share the first set of configuration details you need to complete SSO configuration on your side.\ndata.world team also shares a list of organization where users will get automatically provisioned once SSO is all set up.\nCustomer\nUsing the configuration details shared by the data.world support team, complete the configuration in your SSO provider administration space.\nAt the end of completing your task share the XML file generated from the SSO provider with data.world.\nCustomer\nReview the list shared by data.world of organization where users will get automatically provisioned and confirm with data.world if you need anything adjusted.\ndata.world\nUsing the XML file you shared with data.world, the support team completes the configuration on their side.\nCustomer\nIn your SSO system, assign the data.world app to the users who need to login to data.world.\nCustomer\nShare the login URL with your users to use data.world. Optionally, add the data.world access link to the launch page from where your users access other application.\nCustomer\nOnce the users access the link for the first time, they are automatically provisioned in data.world.\nCustomer\nOptionally, go the Members tab of the Organization profile page to further adjust the user permissions in data.world.\nCustomer\nWant users automatically added to other organizations in your deployment? Notify data.world so that they can adjust the SSO configutation on their side.\nDone by\nTask\nCustomer\nIdentify the organizations for which you want to enable single sign-on. Contact the data.world support team and request them to begin the process.\ndata.world\nThey will enable some configurations for these organizations, as a result of which you will be able to see the SSO configurations from the Organization page. data.world team will notify you that the configuration is complete.\nCustomer\nAt this point go the organization page and note down the values you need to configure SSO.\nCustomer\ncomplete the configuration in your\u00a0SSO provider administration space.\nCustomer\nUsing the XML file you obtained from the SSO provider administration space, complete the configuration in data.world\nCustomer\nIn your SSO system, assign the data.world app to the users who need to login to data.world.\nCustomer\nShare the login URL with your users to use data.world. Optionally, add the data.world access link to the launch page from where your users access other application.\nCustomer\nOnce the users access the link for the first time, they are automatically provisioned in data.world.\nCustomer\nNow, go the\u00a0Members\u00a0tab of the Organization profile page to further adjust the user permissions in data.world.\nCustomer\nWant SSO enabled for another organization? Contact data.world to get them to enable the SSO tab for another organization. Repeat the rest of the steps to complete the SSO setup for the new organization.",
    "url": "https://docs.data.world/en/98616-saml-and-data-world.html"
  },
  {
    "title": "Prerequisites",
    "content": "Make sure the SSO provider that you are using is SAML 2.0 compliant. For example, Okta, Google, or Azure, etc.\nEnsure that you have administrator access to the Administration console or configuration interface of the SSO Provide. If you are not an administrator, reach out to the designated person within your organization.\nFor private instance and single-tenant installations, contact data.world and provide the list of organizations where you want the users to be automatically provisioned when SSO is enabled.\nFor public instance installations, contact data.world to enable SSO configuration for the organization for which you want to use SSO. You must provide a list of all the organizations in data.world that need SSO.\nAfter data.world completes this configuration, you will see the Security section on the Settings tab of the Organization profile page.\nFor private instance and single-tenant installations, the data.world team would have already shared the following details with you: Assertion Consumer Service (ACS) URL and Entity ID. If you do not have these details, please contact the support team.\nFor public instance installations, gather the following details from the platform by doing the following:\nOn the Organization profile page, from the Settings tab, go to the Security section, note down the Entity ID:\nConstruct the Assertion Consumer Service (ACS) URL using the Organization ID:\nFormat of the URL:https://data.world/sso/acs/<Organization_ID>\nExample: https://data.world/sso/acs/Banking",
    "url": "https://docs.data.world/en/98617-prerequisites.html"
  },
  {
    "title": "Setup on SAML App",
    "content": "For private instance and single-tenant installations, you need perform this task only one time. For public instance installations, repeat these step for all the organizations for which you want to enable SSO.\nLog into the Administration Console of your SSO provider. Access the SSO Provider App Management section.\nAdd a new SAML 2.0 protocolenabledApplication.\nSet the general information required by the SSO provider for the application. Fill in required details, such as name and description, to identify the application as data.world to your users.\nSet the Application SAML configuration details using the information gathered from data.world.\nType of installation\nValue format\nExample values\nAssertion Consumer Service (ACS) URL\nPrivate Instance (PI)\nhttps:// {sitename} .app.data.world/sso/site/acs/ {###}\nhttps://8bank.app.data.world/sso/site/acs/58\nSingle-tenant Instance\nhttps:// {vpcname} .data.world/sso/vpc/acs/ {VPCIdentifier}\nhttps://8Bank.data.world/sso/vpc/acs/8Bank\nPublic Instance\nhttps:// data.world/sso/acs/ {organization-id}\nhttps://data.world/sso/acs/banking\nEntity ID\nPrivate Instance (PI)\nhttps:// {sitename} .app.data.world/sso/metadata\nhttps://8Bank.app.data.world/sso/metadata\nSingle-tenant Instance\nhttps:// {vpcname} .data.world/sso/metadata\nhttps://8Bank.data.world/sso/metadata\nPublic Instance\nIf not set to use unique: https://data.world/sso/metadata\nIf set to use unique: https://data.world/sso/metadata/{organization-id}\nhttps://data.world/sso/metadata\nhttps://data.world/sso/metadata/banking\nConfigure the following three Application Attributes required by data.world.\nMake sure that the attribute name does not contain a namespace and must exactly match the following values with no extra text.\nfirstName attribute: To extract the user's first name value from the SSO Provider.\nlastName attribute: To extract the user's last name value from the SSO Provider.\nemail attribute: To extract the user's email value from the SSO Provider.\nComplete any final information for the SSO Provider and save the Application.\nExport the IdP Metadata XML for the Application to capture the values required to configure data.world to complete the connection. The values that will be required by data.world are:\nEntity ID (Identity Provider Issuer)\nSingle Sign-On URL (Redirect)\nPublic x.509 Certificate (Signing)",
    "url": "https://docs.data.world/en/98618-saml-application-configuration.html"
  },
  {
    "title": "Setup on data.world",
    "content": "Create a Support Ticket with the data.world support team if one does not already exist.\nProvide the support team the IDP Metadata XML file containing the required values for your SSO Provider Application: Entity ID (Identity Provider Issuer), Single Sign-On URL (Redirect), Public x.509 Certificate (Signing),\nOnce you are notified that the support team has completed the configuration on their side, you are ready to roll out SSO to your users.\nFollow these steps to configure data.world to use SAML:\nAn Administrator role in the organization on data.world is required to make these changes.\nGo to the Organization Profile Page and browse to the Settings tab.\nOn the Settings tab go to the Security section and configure the single sign-on options. Use the IDP Metadata XML file to get the values for setting these porperties.\nClick the Test SAML configuration button to test the configuration.\nClick Save if the test is successful. It may take a few minutes for the change to take effect for all organization members.\nWhen you enable SSO for an organization, members of that organization will need to validate through the SSO provider to access any pages on data.world, not just the organization\u2019s.\nAll users of the organization will have their personal API tokens reset when SSO is enabled.\nIf they have integrations (such as Python) set up with this token, they will need to update the token within the integration.\nIf they have saved any query results as tables within data.world, they will not be able to sync those tables. They will need to delete the tables and re-save the query results as a new table.",
    "url": "https://docs.data.world/en/162163-setup-on-data-world.html"
  },
  {
    "title": "Rolling out SSO to all users",
    "content": "Once all the configurations are complete, you are ready to roll out SSO to your users.\nShare the login URL with the users. Optionally, add the data.world access link to the launch page from where your users access other application.\nMake sure that the users who need to access data.world are assigned the data.world app in the SSO provider.\nCheck to make sure that users are getting added to the correct default group when they login.\nOnce the user logs in to data.world using SSO, a user will automatically get created for them in data.world. You can now go ahead and further adjust the permissions of the user in data.world.",
    "url": "https://docs.data.world/en/162164-rolling-out-sso-to-all-users.html"
  },
  {
    "title": "Troubleshooting",
    "content": "Cause: The SSO configuration has errors. If the Single Sign-on URL/Assertion URL is set incorrectly in the SSO provider\u2019s configuration, users will see the 404 page or a sign-on loop when trying to access data.world.\nSolution: Check the values of the property to ensure the configuration is correct.\nCause: The data.world app is not assigned to the specific users profile in the SSO provider.\nSolution: Make sure that the users who need to be able to access data.world are assigned the data.world app in the SSO provider.\nCause: The system is not configured properly to grant access to users to all the organizations they need access to.\nSolution: Contact data.world to make sure that the auto-provisioning is enabled for all the organizations users need access to.",
    "url": "https://docs.data.world/en/162165-troubleshooting.html"
  },
  {
    "title": "Managing user groups",
    "content": "",
    "url": "https://docs.data.world/en/96562-managing-user-groups.html"
  },
  {
    "title": "About user groups",
    "content": "User groups are a collection of users that share similar functions or roles in an organization. Groups make it much easier to manage members and their access requirements.\nFor example, create a group of users who only need access to datasets and projects and another group of users who have access to catalog resources. Any new user who gets added to the organization can then be added to these groups and they automatically get access to the resources that the group is authorized to access.\nNote that user groups are not discoverable outside of an organization. When users outside the organizations want to share resources with an organization, they can share the resources with the organization or with specific users in the organization, but they cannot share them with user groups in the organization.\nThree standard user groups are automatically created in each organization. The system allows changing access level of the All Members user group only and does not allow deleting the out-of-the-box groups. You can always add and remove users from these groups.\nIf the out-of the-box user groups access levels work for you, you can just add users to these groups and start using the application. Create custom groups for configuring custom access levels to resources.\nAll members: Every member of the organization is automatically added to this group. By default, this group of users does not have access to view any catalog resources or datasets and projects. They can only access datasets and projects that are shared with the organization or are set as Discoverable. The access level of this group can be adjusted to align with your business needs.\nRemoving a member from the All members group is the same as removing the member from the organization.\nCaution should be used while setting the access levels of the All members group as every new member of the organization automatically gets the access assigned to this group.\nAuthors: Users in this group are automatically authorized to add new projects and datasets and new catalog resources in the organization. The access level of this group cannot be changed.\nOrganization admins: This is a group of users who have manage access to all datasets, projects, and catalog resources in the organization and they can manage organization settings, billing, and member groups. You should add selective users in this group.\nThere must always be at least one member in this group.\nThe access level of this group cannot be changed.\nWhen the system updates to start using user groups, your existing users are automatically added to the appropriate user groups and you can seamlessly continue to use the system.\nUsers with\nGet automatically added to\nBasic member role in organizations\nAll members group\nContent creator role in organizations\nAuthors group\nAdmin role in organizations\nOrganization admins group",
    "url": "https://docs.data.world/en/96563-about-user-groups.html"
  },
  {
    "title": "Planning user groups and their access levels",
    "content": "User groups can be assigned access to datasets and projects and to all catalog resources. This section will help you plan the user groups for your organization.\nNote that ddw-catalog is a special system used dataset (available in all Enterprise organizations) and users and user groups who have Manage access to this dataset, automatically get authorized to manage the catalog resources of the organization.\nCreate user groups that have access to all datasets and projects in the organization. This way users in this group automatically get access to any new dataset or project that is added to the organization.\nAccess level\nWhat the group can do\nWhat notifications they get\nNo access\nCannot see any datasets and projects and cannot create them for the organization.\nException: Discoverable datasets and projects can be viewed by everyone.\nReceives notification when a new resource is shared with the organization.\nContribute\nCan create new datasets and projects for the organization.\nCan edit datasets and projects they created.\nReceives notifications about access requests and comments on the datasets and projects they create.\nManage\nCan manage access of other users to datasets and projects.\nCan create, edit, delete all datasets and projects owned by the organization.\nCan Add, edit, and delete connections from the Connection manager configuration settings available from the Settings tab of the Organization profile page.\nNote: User groups with this access will not be able to manage the tasks for these connections. User groups need the Manage access to catalog resources to be able to manage the tasks for the configured connections.\nReceives notifications about access requests, comments, and new datasets and projects added to the organization.\nCreate user groups and and assign them access to specific datasets and projects in the organization.\nDon't add users to any groups and give them direct access to a specific datasets and projects. This is not a recommended approach as it creates a maintenance overhead especially when you are managing large numbers of users and resources. Also, when someone leaves a company, it is much easier to remove the user from the various groups instead of going to each resource and revoking their access.\nAccess level\nWhat the individual or group of users can do\nNo access\nCannot see any datasets and projects and cannot create them for the organization.\nException: Discoverable datasets and projects can be viewed by everyone.\nDiscover\nCan view summary information and request access to the dataset or project.\nView\nCan view and suggest changes to the specific dataset/project they are given access to. Can download files, query the data and export results. Can participate in ongoing discussions or start new ones.\nEdit\nCan edit the specific dataset/project they have access to.\nManage\nManage access request to specific dataset/project and be able to delete the dataset/project.\nCreate user groups and assign them following level of access to all catalog resources in the organization. This way users in this group will automatically get access to any new catalog resources that are added to the organization.\nAccess level\nWhat the group can do\nWhat notifications they get\nNo access\nUsers cannot view all catalog resources, glossary, collections. You can assign direct access to these user groups from the collection level.\nNone\nView\nCan view, suggest changes, and comment on all metadata resources, glossary, collections owned by the organization.\nReceives notifications about changes they have suggested and comment threads they have participated in on metadata resources\nEdit\nView +\nCan edit all metadata resources, glossary terms, collections.\nCan create metadata resources and glossary terms. Cannot create new collections.\nReceives notifications about suggested changes and comments on metadata resources.\nManage\nEdit +\nCan manage access to collections.\nCan create and delete collections.\nCan delete metadata resources and glossary terms.\nCan create tasks for the connections available from the Connection manager configuration settings in the Settings tab of the Organization profile page.\nNote: As a prerequisite, user groups need Manage access to datasets and projects to able to view the Connection manager configuration settings. As a result, user groups with this combined access will be able to manage the connections and the tasks for the connection.\nReceives notifications about suggested changes, comments, and access requests on metadata resources.\nCreate user groups and assign them access to the specific collections in the organization.\nNote that when members are not given Edit or Manager access to catalog resources at the organization level and have access to specific collections only, they can only create the catalog resources from the Collections Details page and not from the Overview tab or the Resources or Glossary tab of the Organization Profile page.\nDon't add users to any groups and give them direct access to specific collections. This is not a recommended approach as it creates a maintenance overhead especially when you are managing large numbers of users and resources. Also, when someone leaves a company, it is much easier to remove the user from the various groups instead of going to each resource and revoking their access.\nFor more details about planning access control for collections, please see Planning collection & permissions for collections.\nA user group with the This group can manage organization settings, billing, and member groups access enabled will be able to manage organization settings available from the Settings tab of the Organization profile page, and members and member groups from the Members tab.\nUsers need to have Manage access to Datasets and projects to manage the Connection Manager configuration available in the Settings tab. And, users need to have Manage access to Catalog resources to manage the tasks for the configured connections.\nWhen you belong to multiple groups, we aggregate the permissions and the highest level of permissions are honored.\nConfiguration\nResults\nA user belongs to two groups:\nSuper admins group is given permission to administer users and settings of the organization.\nAll members group is not allowed to do the task.\nUsers in the Super admin group will be able to do the task as the highest level of access is honored.\nA user belongs to two groups:\nSale group is set to not have access to the catalog resources.\nAll members group is allowed to view all catalog resources.\nUsers in the Sales group will still be able to view the catalog resources as the All members group has that permission.\nUser belongs to Marketing group that is set to not have access to Dataset and project in the organization.\nYou give the Marketing group Manage access to a specific dataset directly.\nUsers in the Marketing group will be able to manage that specific dataset.",
    "url": "https://docs.data.world/en/98973-planning-user-groups-and-their-access-levels.html"
  },
  {
    "title": "Adjusting permissions of all members in the organization",
    "content": "Members is a special out-of-the-box group that automatically includes all members  in a organization and it determines the minimum level of access for all members in the organization. Therefore, when you set the permissions of this group, you are authorizing every user in the organization to have a certain access level. This level of access cannot be removed by adding the users to a group that has lower access levels.\nTo adjust the permissions for all members in the organization:\nOn the Organization profile page, browse to the Members tab > Members section.\nClick the Access summary button.\nOn the Access summary page, in the Default access for all members section, click the Edit button.\nIn then Edit group access window, set the following permissions and Click Save.\nSelect the This group can manage organization settings, billing, and member groups option to allow the users to be organization administrators.\nGive No access, Contribute, Manage access to datasets and projects\nGive No access, View, Edit, Manage access to catalog resources.\nThe Access summary page is refreshed to show the new access details for the group.\nNext, from the Access summary page, give access to the group to specific datasets, projects, or collections by click the Grant access button.\nIn the window that opens, select the resource or collection to which you want to give access and set the permissions. Click Save.",
    "url": "https://docs.data.world/en/100122-adjusting-permissions-of-all-members-in-the-organization.html"
  },
  {
    "title": "Creating user groups",
    "content": "Each organization comes with three out-of-the-box user groups. Create new user groups to set up the organization members with different levels of access to the organization resources.\nTo create new custom user groups:\nGo to the organization where you want to create user groups.\nOn the Organization profile page, browse to the Members tab > Groups section.\nClick the Add group button. The Create a new group window opens.\nIn the Create a new group window, provide the name of the group and a brief description. Click Submit. The user group is added to the list of groups.\nClick the user group from the list view to assign access permissions.\nOn the Groups details page, click the Access summary button.\nOn the Access summary page, click the Edit button to adjust the access level for the user group.\nIn the Edit group access window, set the following. For details about these access levels see Planning user groups and their access levels\nSelect the This group can manage organization settings, billing, and member groups option to allow the users to be organization administrators.\nGive No access,  Contribute, Manage access to datasets and projects\nGive No access,View, Edit, Manage access to catalog resources.\nNote that when you try to set permissions that are lower than the All members group, the system notifies you about it.\nNext, from the Access summary page, give access to the group to specific datasets, projects, or collections by clicking the Grant accessbutton. In the window that opens, select the resource or collection to which you want to give access and select the permissions. Click Save.\nAdd members  to the user group and click Save.",
    "url": "https://docs.data.world/en/96564-creating-user-groups.html"
  },
  {
    "title": "Adding and removing members from user groups",
    "content": "Once you have created a user group and decided the level of access they will have, go ahead and add the members to the group.\nTo add members to a group:\nFrom the Organization profile page, go to Members tab > Groups section.\nBrowse to a group where you want to add members and click the Add member button.\nIn the Add members to group window, select from the list of members on the organization. Note that you can only add members who are already part of the organization.\nTo remove members from a group:\nWhen a user is removed from a group, any access granted through the group is revoked. Direct access to resources and access to resources through membership in other groups continues to work as before.\nNote that when a user is removed from the All members group, the user is automatically removed from the organization.\nBrowse to the group from where you want to remove a member.\nFrom the list of Group members, click the Delete button next to the member you want to remove.",
    "url": "https://docs.data.world/en/96565-adding-and-removing-members-from-user-groups.html"
  },
  {
    "title": "Granting user groups access to specific datasets, projects, & collections",
    "content": "To grant access to specific resources from the groups:\nOn the Organization profile page, browse to the Members tab > Groups section.\nOn the Groups details page, click the Access summary button.\nOn the Access summary page, click the Edit button to adjust the access level for the user group.\nFrom the Access summary page, give access to the group to specific datasets, projects, or collections by clicking the Grant accessbutton. In the window that opens, select the resource or collection to which you want to give access and select the permissions. Click Save.\nTo grant access to specific resources from the resources page:\nGo to the specific dataset or project in your organization.\nFrom the Settings tab > Access and Ownership section, click the Grant access button.\nIn the Grant access window, search for the user group that should have access to this resource.\nAssign one of the following permissions:\nDiscover:  Allow the members in this group to discover the resource.\nView: Allow the members in this group to view this resource.\nEdit: Allow the members of this group to view and edit the resource.\nManage: Allow the members of this group to view, edit, delete the resource and to be able give others access to this resource.\nClick the Grant access button.\nFor details about doing this task, see Setting permissions for collections",
    "url": "https://docs.data.world/en/96566-granting-user-groups-access-to-specific-datasets-and-projects.html"
  },
  {
    "title": "Deleting user groups",
    "content": "Note that out-of-the-box user groups cannot be deleted.\nOnly users in the Organization admins group or user groups with explicit This group can manage organization settings, billing, and member groups permission can delete user groups.\nTo delete a user group:\nBrowse to the group that you want to delete.\nClick the Delete button in the list view.\nA message appears notifying that the group members will lose access to resources. Direct access to resources and access to resources through membership in other groups continues to work as before.",
    "url": "https://docs.data.world/en/96567-deleting-user-groups.html"
  },
  {
    "title": "Managing members of organizations",
    "content": "",
    "url": "https://docs.data.world/en/96557-managing-members-of-organizations.html"
  },
  {
    "title": "Inviting members to organizations",
    "content": "To invite members to organizations:\nFrom the Members tab, go to the Members section.\nClick the Add members button.\nIn the Invite members to organization window, do the following:\nType the name or email address of the users you want to invite. You can also invite other organizations.\nIn the Add to Group section, select from one of the out-of-the-box groups available in the organization. If you want, you can move the user to a custom user group after they have joined the organization.\nBy default, members get added to the Authors group. You can change the default selection from the Organization profile page > Settings tab > Preferences section. For details, see Updating general details of the organization.\nSelect if the user is going to be a private member or other community users can see that the user belongs to a specific organization. The default selection is Public Member. If you want all users to be added as Private Members, you can change the default selection from the Organization profile page > Settings tab > Preferences section. For details, see Updating general details of the organization.\nClick the Add a custom message link to include a personalized message in the invitation.\nClick Invite to send the invitation.\n",
    "url": "https://docs.data.world/en/96558-inviting-members-to-organizations.html"
  },
  {
    "title": "Viewing member access summary",
    "content": "Once a user or organization accepts the invitation and becomes part of the organization, you can click through to the user or organization information and see what level of access they have in the organization, what groups they are part of, and if the user or organization has direct access to any of the resources such as datasets and projects in the organization. This helps administrators audit different ways members are granted access within the organization and take quick actions to manage the access.\nTo view the member access summary:\nOn the Organization profile page, go the Members tab.\nIn the Members section, locate the user or organization you are interested in and click the row for the member.\nOn the next screen you can view the following information about the member:\nView a brief summary of the user and click the View profile link to view the full profile of the user, which lists the organizations the user is part of, their recent activities in the application, bookmarks, followers etc. If you are looking at a member organization, clicking the View profile link opens the Organization profile page.\nIf you are an organization administrator, you can also remove the user from the organization by clicking the Remove them from the organization link.\nIf you are looking at your own access page, you get an option to Leave the organization.\nThe Organization access summary section shows the calculated access of the user in the organization. It is calculated based on all group memberships and represent the highest level of access given to the member by groups.\nNext, you see all the groups the user is part of and the specific access they get from each group. You can click the group name to go to the user group details.\nIf you are an organization administrator, you can click the Remove button to remove the member from the group.\nLastly, the Direct access summary lists all the datasets & projects and collections to which the user is granted direct access (the access is not granted through a user group). You see this section only if you are an organization administrator.\nUse the Grant access button to grant access to new collections and resources. You can also change the access levels to the listed resources or remove the access completely. To know more about the resource, click the name of the resource to get to the resource details page.",
    "url": "https://docs.data.world/en/109087-viewing-member-access-summary.html"
  },
  {
    "title": "Allowing community members to join organizations",
    "content": "Setting community visibility for the organization\nUse the following settings to decide if community users can request access to the organization and suggest new resources for the organization.\nOn the Settings tab, in the Preferences section, set the following properties of the organization:\nAllow community members to request to join this organization: Select this option if you want users of the platform be able to request joining an organization.\nWhen this setting is enabled, users can go to the Members tab of an organization and request to join the organization. Once a request is submitted, the administrator gets notifications in the application and as an email.\nAlso, an Invite members via link option becomes available on the Members tab, Members section. Copy this link and share with the users so they can sign-up to join the organization. You still get an option to approve the memberships requested using the link.\nAllow community members to propose new resources: Select this option to allow community members to propose new datasets and projects to the organization.\nAdditional notification recipient: Specify an email address (like admin@yourcompany.com) to receive organization notifications. Organization admins will continue to receive emails.\nClick the Save button.",
    "url": "https://docs.data.world/en/98851-allowing-community-members-to-join-organizations.html"
  },
  {
    "title": "Approving requests to join organizations",
    "content": "When users outside the organization request to be added to an organization, the administrators are notified about the requests through email notifications and in-app notifications.\nTo approve a join request:\nYou can simply approve the join request from the Notifications list by clicking the Accept button. However, if you want to see more details about the user requesting access, click the notification to browse to the Members tab of the organization.\nIn the Members tab of the organization, click the name of the user to view their profile. Once you are satisfied with the request click the Accept button. If you do not wish to grant access, just click the Reject button. The user is automatically added in the All members group.",
    "url": "https://docs.data.world/en/96560-approving-requests-to-join-organizations.html"
  },
  {
    "title": "Removing members from organizations",
    "content": "To remove a member from an organization:\nOn the Organization profile page, from the Members tab, go to the Members section.\nLocate the member you want to remove. If it is a long list of members, use the Filter members feature to narrow down the list.\nClick the Remove button next to the user to remove them from the organization. Note that once a member is removed, they lose all privileges in the organization. When prompted, confirm your action to remove the user.",
    "url": "https://docs.data.world/en/96561-removing-members-from-organizations.html"
  },
  {
    "title": "Connect to data",
    "content": "We support connecting to many data sources, and we continue to add new sources all the time. There are different ways to connect your data source with data.world, and also different ways to use the connection including:\nTo collect metadata and lineage - Connections for metadata collection are configured and run with either the Connection manager, data.world's GUI for connections or with the data.world Collector, a data.world program created to catalog metadata.\nTo create a live connection to your data for query and and analysis - Most live, or virtual, connections to your data can be created with the Connection manager--the preferred method for creating organization-owned connections, or through the Integration gallery.\nTo extract data from your database - Data extracted using a connection is stored on data.world for query and analysis, and can be synced to update regularly through the connection. Connections used to extract data also an be created with the Connection manager or through the Integration gallery.",
    "url": "https://docs.data.world/en/98627-connect-to-data.html"
  },
  {
    "title": "Currently supported data sources",
    "content": "data.world University!\nCheck out our Introduction to Collectors course!\nThe following table shows both the ways the connection can be configured for each source, and also what can be done with the connection.\nData source\n\n\n\nWays to connect\nFeatures available\nLineage collected\n(only for metadata)\nConnection Manager\nMetadata Collector\nIntegration Gallery\nMetadata collection\nData virtualization\nData extraction\nAmazon S3\n\nyes\n\nyes\n\n\n\nApache Spark\n\nyes\n\nyes\n\n\n\nAthena\nyes\nyes\nyes\nyes\nyes\nyes\n\nAWS Glue\n\nyes\n\nyes\n\n\nyes\nAzure Data Lake Storage Gen2\n\nyes\n\nyes\n\n\n\nAzure Synapse Analytics\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nBigQuery\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nDatabricks\nyes\nyes\n\nyes\nyes\nyes\nyes\nyes\nDatakin\n\nyes\n\nyes\n\n\n\nDB2\n\nyes\n\n\nyes\n\n\n\ndbt Cloud\n\nyes\n\nyes\n\n\nyes\ndbt Core\n\nyes\nlegacy collector\n\nyes\n\n\nyes\nDenodo\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nDomo\n\nyes\n\nyes\n\n\n\nDremio\n\nyes\n\nyes\n\n\nyes\nFivetran\n\nyes\n\nyes\n\n\nyes\nGeneric JDBC\n\nyes\n\nyes\n\n\n\nGrafana\n\nyes\n\nyes\n\n\nyes\nHive\n\nyes\n\nyes\n\n\n\nHive metastore\n\nyes\n\nyes\n\n\n\nInfor Ion\nyes\nyes\nyes\nyes\nyes\nyes\n\nInformation Schema\n\nyes\n\nyes\n\n\n\nKafka - Confluent Cloud\n\nyes\n(Collector Wizard not available)\n\nyes\n\n\n\nKafka - Confluent Platform\n\nyes\n(Collector Wizard not available)\n\nyes\n\n\n\nLooker\n\nyes\n\nyes\n\n\n\nManta\n\nyes\n\nyes\n\n\nyes\nMarquez\n\nyes\n\nyes\n\n\n\nMicrosoft SQL Server\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nMonte Carlo\n\nyes\n\nyes\n\n\n\nMySQL\nyes\nyes\n\nyes\n\n\n\nNetezza\n\nyes\n\nyes\n\n\n\nOpenAPI\n\nyes\n\nyes\n\n\n\nOracle\nyes\nyes\nyes\nyes\n\n\n\nPostgreSQL\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nPowerBI\n\nyes\nyes\nyes\n\n\nyes\nPresto\n\nyes\n\nyes\n\n\n\nRedshift\nyes\nyes\nyes\nyes\nyes\nyes\nyes\nReltio\n\nyes\n\nyes\n\n\n\nSalesforce\n\nyes\n\nyes\n\n\n\nSAP HANA\n\nyes\n\nyes\n\n\n\nSnowflake\nyes\nyes\nyes\nyes\nyes\nno\nyes\nSQL Anywhere\n\nyes\n\nyes\n\n\n\nSQL Server Reporting Services (SSRS)\n\nyes\n\nyes\n\n\n\nTableau\nyes\nyes\nyes\nyes\nNot applicable\nNot applicable\nyes\nThoughtSpot\n\nyes\n\nyes\n\n\n\nVertica\n\nyes\n\nyes\n\n\n",
    "url": "https://docs.data.world/en/105177-currently-supported-data-sources.html"
  },
  {
    "title": "Connection manager",
    "content": "The Connection manager  is a one-stop shop for creating and managing organization-owned connections to your data sources. These connections can be used to:\nCatalog your metadata\nAccess your data (either through a live table or through a data extract)\nThe Connection manager is the easiest way to create a connection to your data source to query either metadata or data. It uses a in-app dialog for all configuration options and doesn't need a script, file, or command-line interface. Besides being the easiest, it also renders the most basic metadata catalog. If you need more control over the parameters you use to catalog your data, one of the other methods of running a metadata cataloger might be a better choice for you.\nUsers need to be\u00a0Administrators\u00a0in an organization\u00a0to manage data source configurations with the\u00a0Connection manager\u00a0.\nBrowse to the profile page of your organization and go to Settings > Connection manager.\nOn the Settings tab go to the Connection manager section.\nFrom here you can\nCreate new connections\nEdit existing connections\nManage tasks for the connection.\nTo create a new connection to your data sources, click the Add connection button.\nThe dialog opened lists the database sources for which you can create an organization level connection:\nSelect a source to be presented with the configuration screen for your connection.\nThe Three dot menu to the right of a connection name gives options to:\nCreate task\nEdit connection - links to the same configuration screen used to create the connection, and you need to have the same credentials (login, password, etc.) to change anything about the connection.\nDelete connection prompts you to be sure you want to proceed, but does not require any further validation.\nAny organization admin or other authorized person with the credentials for a connection can modify it, and any data steward or other authorized person with access to the Connection manager can delete connections.\nA metadata task catalogs the metadata for a data source and places the extracted information into a specified Collection. For now, the system only supports creating metadata tasks. Use the Create task option available in the Three dot menu to the right of a connection to create a new task for the configured connection.\nBefore creating a task you need to have a collection in which to put your task output. The collection must be created before the task.\nTasks work with version 2.x of the data.world Collector. If you are still using the Collector v1.x, see the article on data.world Collector v2.0 for more information.\nThe Create task button opens a dialog where you can configure your new task. To create a task you will be prompted to choose a database and schema as appropriate. You also need to add it to a collection.\nThe Three dot menu to the right of each task gives options to sync, edit, or delete the tasks.\nSync runs the task again, pulling in any new metadata.\nEdit task details opens the Edit task window where you can change any of the original configuration (database, schema, or collection),\nDelete task removes the task.",
    "url": "https://docs.data.world/en/105156-connection-manager.html"
  },
  {
    "title": "Connection Manager configurations",
    "content": "",
    "url": "https://docs.data.world/en/105183-connection-manager-configurations.html"
  },
  {
    "title": "Athena connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nS3 Output Bucket Location - The Amazon S3 bucket where query results should be stored. The location should start with s3://. For example, to store results in a folder named \"test-folder-1\" inside an S3 bucket named \"query-results-bucket\", you would set the location to s3://query-results-bucket/test-folder-1\nWorkgroup - If your Athena instance is configured with different workspaces you can assign your connection to a workspace here\nAWS ARN - A dedicated Identity Access Management (IAM) role created specifically for data.world. This role must be created before you can configure a connection to Athena. See Create a dedicated IAM role for Athena connections for more information.\nAWS external id - provided in the \"Add a new Athena connection\" dialog\nBefore configuring a connection to Athena you need to have set up an IAM role in the AWS console.\nTo configure a virtual connection to Athena you will need to create a dedicated IAM role in your Amazon Web Services (AWS) console and enter the AWS Amazon Resource Name (ARN) for it in the Add a new connection dialog. To create the role, however, you will need to first get the AWS External ID from the bottom of the connection dialog. Follow the steps below to create the AWS role and the connection to Athena.\nOpen the configuration screen as described above,\nCopy the External ID and do not close the dialog.\nYou have to leave the Add a new connection dialog open while you connect to the AWS console and create the role needed for the connection because every time you open the dialog to create a new connection, a new external ID is generated.\nGo to the AWS console and select Create role.\nUse the following parameters for the role:\nSelect type of trusted entity - Another AWS account\nAccount ID - 465428570792\nRequire external ID - checked\nExternal ID - The value copied from the Add new connection dialog in data.world\nSelect Next: Permissions:\nUse the search bar to find the following two policies and add them:\nAmazonAthenaFullAccess\nAmazonS3FullAccess\nYou may choose to be more fine grained in precisely which buckets you allow data.world to access. We will only need write access on the S3 output bucket location configured earlier. Otherwise, the minimum permissions required to query data from table backing buckets is required.\nAdd any tags you would like.\nSelect Next: Review\nName the role, write a description, verify that the two policies shown above are present, and select Create role.\nFind the role you have just created:\nCopy its ARN, and paste the ARN into the dialog window you left open for adding a new Athena connection.",
    "url": "https://docs.data.world/en/105184-athena-virtual-data-connection.html"
  },
  {
    "title": "Azure Synapse connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105185-azure-synapse-virtual-data-connection.html"
  },
  {
    "title": "BigQuery connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nProject ID - The unique identifier for your BigQuery project\nService account username - A Google account that is associated with your Google project, as opposed to a specific user\nService account key file - Provides the authentication information used in the connection configuration. This file will be uploaded when you enter the other configuration information into the dialog",
    "url": "https://docs.data.world/en/105186-bigquery-virtual-data-connection.html"
  },
  {
    "title": "Denodo connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105187-denodo-virtual-data-connection.html"
  },
  {
    "title": "Infor ION connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nConnection username - Your username for your data source\nionapi authentication file - the credential file used to authenticate the user",
    "url": "https://docs.data.world/en/105188-infor-ion-data-source.html"
  },
  {
    "title": "MS SQL Server connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105189-ms-sql-server-virtual-data-connection.html"
  },
  {
    "title": "MYSQL connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105190-mysql-virtual-data-connection.html"
  },
  {
    "title": "Oracle connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nService Name (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105191-oracle-database-virtual-data-connection.html"
  },
  {
    "title": "PostgreSQL connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105192-postgresql-virtual-data-connection.html"
  },
  {
    "title": "Redshift connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105193-redshift-virtual-data-connection.html"
  },
  {
    "title": "Snowflake connection",
    "content": "The Snowflake user specified in the connection must have a default Warehouse set in Snowflake\nAll queries run against Snowflake with this connection will use this Warehouse for their compute power.\nIf the Snowflake user specified in the connection does not have a default Role set in Snowflake, the connection will use the Public role, which may limit privileges to access and query data.\nIf a default Role is set for the Snowflake user, that Role will be used by the data.world connection.\nIn order to create a virtualized connection to a table or view, the user must have USAGE privileges on the database and schema and SELECT privileges on the table or view.\nIf a default database is specified in the data.world connection modal, it must be specified using all UPPER CASE letters.\nConnection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nHost/IP - the url for your database\nPort (optional) - use if you connect through a special port\nDatabase (optional) - You can specify it here or you can choose from a dropdown list when you create a dataset\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105194-snowflake-virtual-data-connection.html"
  },
  {
    "title": "Tableau connection",
    "content": "Connection owner - When you create the connection from the Connection Manager, the owner is automatically set to be the organization from which you create the connection. For connections enabled in the Integration Gallery, you are the default owner. Ownership can also be assigned to any organization of which you are an admin.\nDisplay name - The display name is the name everyone in your organization will see for the connection.\nServer - The hostname of the database server to connect to\nAPI version - The Tableau API version\nSite Id (optional) - The ID or name of the Tableau site to catalog (if not provided, all sites accessible to the user will be cataloged)\nConnection username - Your username for your data source\nConnection password - Your password for your data source",
    "url": "https://docs.data.world/en/105195-tableau-metadata-configuration.html"
  },
  {
    "title": "How to catalog metadata",
    "content": "Metadata is cataloged using either:\nThe Connection Manager - data.worlds GUI for creating and managing data connections, is a simple interface used to create connections for both catalog ingmetadata and accessing remote data from a virtual connection. The databases that can be configured with the Connection manager are listed in this table.\nHere is an example of the Configuration Manager interface:\nThe data.world Collector - a data.world program created to catalog metadata that is either run from a Docker container or from a jar file. We recommend that you run it from inside a Docker container. If you cannot use Dockerwe can provide you with a .jar file containing the correct Collector version for your implementation. See our data.world Collector FAQ for more information about the Collector.\nDocker is a highly configurable, and very well documented application. This is a brief introduction to its options and syntax when used with the data.world Collector. Here are two examples of the Docker portion of scripts used with the Collector:\nThis is a brief explanation of the various pieces:\ndocker run The\u00a0docker run\u00a0command first\u00a0creates\u00a0a writeable container layer over the specified image, and then\u00a0starts\u00a0it using the specified command.\n.-it is a combination of --i Keep STDIN open even if not attached, and -t Allocate a pseudo-TTY- The\u00a0-it\u00a0instructs Docker to allocate a pseudo-TTY connected to the container\u2019s stdin; creating an interactive\u00a0bash\u00a0shell in the container.\n--rm  By default a container\u2019s file system persists even after the container exits. This makes debugging a lot easier (since you can inspect the final state) and you retain all your data by default. But if you are running short-term\u00a0foreground\u00a0processes, these container file systems can really pile up. If instead you\u2019d like Docker to\u00a0automatically clean up the container and remove the file system when the container exits, you can add the\u00a0--rm\u00a0flag.\n--mount allows you to mount volumes, host-directories and\u00a0tmpfs\u00a0mounts in a container. It consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. See the Docker site for more information on bind\u00a0mounts and their options, and a comparison between\u00a0--volume\u00a0and\u00a0--mount in the\u00a0service create command reference.\ntype=bind,source=/tmp,target=/dwcc-output reference here\nbind bind-mounts a directory or file from the host into the container.\nsrc or source src\u00a0is required, and specifies an absolute path to the file or directory to bind-mount (for example,\u00a0src=/path/on/host/). An error is produced if the file or directory does not exist.\ntarget Mount path inside the container, for example\u00a0/some/path/in/container/. If the path does not exist in the container's filesystem, the Engine creates a directory at the specified location before mounting the volume or bind mount.\n-env or -e Set environment variables\nAn explanation of all the Docker run commands is available here,",
    "url": "https://docs.data.world/en/98648-how-to-catalog-metadata.html"
  },
  {
    "title": "Metadata collection tiered access",
    "content": "Access to cataloging metadata for various data sources is determined by your enterprise license tier. The tier structure is as follows:\nTier one\nTier two\nTier three\nGoogle BigQuery\nAmazon S3\nDenodo\ndbt\nApache Spark\nDomo\ndbt Cloud\nAmazon Athena\nDremio\nFivetran\nAmazon Glue\nGeneric JDBC\nLooker\nAzure Data Lake Storage Gen2\nIBM DB2\nMonte Carlo\nAzure Synapse\nInfor ION\nOpen API\nDatabricks\nMS SQL Server on-prem\nPower BI cloud\nDatakin\nNetezza\nPresto\nGrafana\nOracle Db on-prem\nAmazon Redshift\nHive\nReltio\nSnowflake\nHive Metastore\nSalesforce\nTableau online\nKafka - Confluent Cloud\nSAP HANA\nThoughtSpot\n\nKafka - Confluent Platform\nSAP SQL Anywhere\nMarquez\nTableau Server\nMS SQL Server cloud\nVertica\n\nMySQL\nOracle DB cloud\nPostgreSQL\nSQL Server Reporting Services\u00a0(SSRS)\nIn addition to the tier structure for metadata collection support, there are also add-ons available for purchase for the tiers.\nPlease contact the data.world Customer Experience team to find out about the subscription charges for enabling these add-ons.\n\nAdd-on\nDescription\nAn additional Tier 3 integration\nAdditional supported advanced source system\nOne additional data.world Bridge\nOne additional data center or cloud environment - two agents (high availability)\n100 additional service accounts\n100 additional API service accounts (on top of the 50 included accounts)\nAdvanced partner lineage\nMANTA lineage bundle - features and limitations noted separately\nData virtualization PREVIEW\nQuery virtualization and federation - unlimited datasets and projects, unlimited live tables and query minutes while in Preview\nSensitive data discovery PREVIEW\nIdentify PII and PHI - special preview price\nHIPAA BAA\nHIPAA business associate agreement, liability limits up to 1.5 year\u2019s fees\nDPA\nData processing addendum, liability limits up to 1.5 year\u2019s fees",
    "url": "https://docs.data.world/en/99738-metadata-collection-tiered-access.html"
  },
  {
    "title": "Catalog collector release note",
    "content": "Published versions of collectors are available as a docker image here.\nDetails about the release\nItem\nDetails\nRelease version\n2.151\nRelease date\n15 August, 2023\nDocker image ID\narm64:\u00a06cf7d92cd8fdf9fdc8b9ff2ea3bfb369203c43f1bc57e1e3f14c12f7ef651af8\namd64:\u00a0655cf89d3bd60e6a95da428abb1d3eb3621f96a384166ac15cdc4d73ca1a354e\nNew features and changes\nA new collector is now available for SQL Server Reporting Services.\nDatabricks collector: Update the collector to retry after a pause when the Databricks API responds with too many requests.\nAll database collectors: Optimized the database collectors to reuse database connections where possible.\nDetails about the release\nItem\nDetails\nRelease version\n2.150\nRelease date\n10 August, 2023\nDocker image ID\narm64: 57a1a0425917d69c40688dfcf46b05a531a8cdca5ae3c798b0d20e518fcb60ee\namd64: 53920c9e9b80e45f3494b8f755c2e3dbf8275a0d125dd0997a0161f6d2edba99\nBug fixes\nAll collectors: Fixed an issue that prevented the user from passing command-line options containing spaces, when running the collectors using the docker container.\nDetails about the release\nItem\nDetails\nRelease version\n2.149\nRelease date\n9 August, 2023\nDocker image ID\narm64: d456d1966e6ede73cfabd4d11e77c4fddc9b3861ca82ab360493cf9a6b0f782b\namd64: \u00a08406dae1ab24932c24a3bbc82618368653e32e0f4a5c09f0caa0eb22ed4e711a\nNew features and changes\nDatabricks Collector The collector now allows users to use Personal Access Token without specifying username/password for authentication.\nPower BI Gov collector: The --include-user-workspace parameter is removed from the collector CLI options.\nDetails about the release\nItem\nDetails\nRelease version\n2.148\nRelease date\n27 July, 2023\nDocker image ID\narm64: 2c856f96af8576024b9a81fb89eb3803eaa03abbe055f601f5597f2f79a62019\namd64: \u00a0dcb90da4d519a165da4228f02ee75da4a892320901305df85279a99ea84e2cee\nNew features and changes\nDatabricks Collector has a new option --workflow-exclude to exclude harvesting of jobs/workflows.\nPower BI and Power BI Gov Collectors now support parameter values in Power BI expressions.\nBug fixes\nTableau collector properly handles duplicate data sources when multiple filtered projects are specified.\nDetails about the release\nItem\nDetails\nRelease version\n2.147\nRelease date\n24 July, 2023\nDocker image ID\narm64: 872cbd1f19c7e54d6db57fdab60dc08fab2690a59df54e74f0718d8cd8794381\namd64: \u00a0d0a77c9cb81754d217f53dc4373a3b9b85425d687dcb6fb218abea678c42c148\nNew features and changes\nPower BI and Power BI Gov collectors: The collectors have a new option --max-parseable-expression-length, which sets the maximum number of characters in a PowerBI expression that will be parsed for lineage metadata.\nBug fixes\nPower BI and ThoughtSpot collectors now refresh expired authentication tokens.\nThe SQL Server collector now properly handles missing SQL definition when harvesting stored procedures.\nDetails about the release\nItem\nDetails\nRelease version\n2.146\nRelease date\n19 July, 2023\nDocker image ID\narm64:\u00a03c70ebdae29b700f381f8dd079279ae16403083846549b747fe1be2efb03d5d2\namd64:\u00a060fa58a68e2ea530faef3a5286d0e2fb55bd0d1b48260a31658ad25ea1f97644\nNew features and changes\nTableau collector: The collector now harvests lineage relationships between embedded data sources and published data sources to reflect any such relationship that exists in Tableau.\nBug fixes\nPower BI collector: Improvements made to the collector to avoid hitting Power BI Admin API rate limits that prevented successful collection for certain large Power BI organizations.\nMarquez collector: API authentication token [--marquez-api-key] is now a required parameter for the collector.\nFivetran collector: Fivetran API key (--fivetran-apikey) and Fivetran secret (--fivetran-apisecret) options are now required parameters for the collector.\nDetails about the release\nItem\nDetails\nRelease version\n2.145\nRelease date\n14 July, 2023\nDocker image ID\narm64:\u00a0fcb792fef00634de9e02e635d522b776dad844d6918b8bd002d84eda1bc1c9a1\namd64:\u00a0318ce1c64752c55318941576a952015e9d8f11b2db1c1f94c86607504d3896ab\nNew features and changes\nThe following two new collectors are now available:\nKafka - Confluent Cloud\nKafka - Confluent Platform\nDetails about the release\nItem\nDetails\nRelease version\n2.144\nRelease date\n11 July, 2023\nDocker image ID\narm64: d98bbc978ee4beb798b3068fc554f7ce4478fd9c5af971e598ad2b349422e0f6\namd64: \u00a093bb18e5dd6a0f2ff4caaa8058f040d28f23420bf4a535209909a48cbb4f028f\nBug fixes\nDatabricks collector: The collector now properly handles missing information returned by the Databricks APIs.\ndbt Core and dbt Cloud Collectors: The collectors now use the description property for resources in a dbt manifest file to populate the description of associated catalog resources.\nPower BI collector: The collector now properly handles unexpected source formats.\nDetails about the release\nItem\nDetails\nRelease version\n2.143\nRelease date\n7 July, 2023\nDocker image ID\narm64:\u00a098423e91c32ea3a45410da41290a0b4afb6041e1d59e4488adfb12269f48d695\namd64:\u00a0506104e806a64eb170b3f27a4b4f40067ff27ed2f43de101f9c7e1f004aac463\nNew features and changes\nDB2 collector: The collector now support harvesting of column statistics and function and stored procedure information. For details about using the new parameters (--target-sample-size,\u00a0--sample-string-values,\u00a0--enable-column-statistics) for these features, see the DB2 collector documentation.\nRedshift collector: The collector now properly distinguishes between user-defined functions and stored procedures when harvesting function and stored procedure metadata in the collector.\nTableau collector: Improved error messages and handling of missing Salesforce connection information within the Tableau collector.\nBug fixes\nDatabricks collector: Fixed defects in the collector to accommodate invalid number formats and missing information returned by Databricks APIs in some cases.\nDetails about the release\nItem\nDetails\nRelease version\n2.142\nRelease date\n23 June, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: ccc19cfe72b618bce99c18f755f1c7c7489f012f626e5ecf7abf98f8f9590012\namd64: 507d600ade53fdad8973e0ef9cccdb116cb46b1e198ed38427feb2f8ebb8ac95\nNew features and changes\nPower BI collector: A new parameter --all-workspaces-and-apps is available for the Power BI collector which allows users to catalog all available data from the tenant using the admin API.\nBug fixes\nDatabricks collector: Fixed an issue where the collector was terminating abnormally when it encountered a notebook that had no language specified for it.\nMicrosoft SQL Server collector: Fixed an issue with parsing the SQL for certain Views in Microsoft SQL Server that prevented harvesting of lineage.\nDetails about the release\nItem\nDetails\nRelease version\n2.141\nRelease date\n20 June, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: d17699bcb6ac1a11f8f32be735dd04367517a4cd32c2afaecce17e1069f1e203\namd64: a0da0c8f2db9ad45ee028fe491a7e43159969033edc1bbdd906d6327c44d7812\nNew features and changes\nThoughtspot collector now harvests:\nColumn-level lineage between JDBC source tables columns and ThoughtSpot logical columns.\nColumn-level lineage between ThoughtSpot logical columns and Answers and Liveboards that connect to the data.\nDatabricks collector now harvests additional metadata for Databricks tables.\nThe Redshift, SQL Server, and PostgreSQL collectors now harvest:\nFunctions\nStored procedures\nPower BI, Looker, and Thoughtspot collectors: The resources cataloged by these collectors will now automatically include a link to the resource in the source system. This allows users to go from data.world to the associated URL for the same resource in the source system so the users do not have to manually find that resource in the source system.\nDetails about the release\nItem\nDetails\nRelease version\n2.140\nRelease date\n13 June, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 9c2d1cf893eddc89924f212d1e10c9f26deb05ac254db783d3bdc8fc83fb6d5e\namd64: dc9dc94deed7fbf7da17fb822d9a085434f0a72baee8e965c7dfe1b537d62b95\nNew features and changes\ndbt cloud collector allows the user to pass in a Snowflake role and Snowflake warehouse to override values found in the dbt cloud project configuration.\nBug fixes\ndbt core and dbt cloud collectors properly handle source meta config values that are objects rather than strings in the generated dbt manifest file.\nSQL Server Collector properly disables lineage collection when the --disable-lineage-collection parameter is set.\nDatabricks collector includes additional checks for existence of and access to Unity Catalog.\nDetails about the release\nItem\nDetails\nRelease version\n2.139\nRelease date\n7 June, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 95c668b9ceb092cc7d99a78caa4b64b1d70ee2a1de9d47fadd1f8206aecb6948\namd64: fbe3a314510718a74a0162918db20d345a48e76c879f49ed6851768547e9e855\nNew features and changes\nSQL Server collector now harvests created date and modified date for tables and schemas, and harvests table size in bytes.\nSnowflake collector now harvests table size in bytes.\nPower BI collector allows setting the --azure-tenantid option when using username and password authentication.\nAll collectors now support the ability to set the JVM stack size using the DWCC_JVM_OPTIONS parameter.\nBug fixes\nSQL Server collector properly handles harvesting of View SQL containing character length that is more than the SQL Server column default character length (6000).\ndbt Cloud collector Rather than reporting an error, the collector now skips job runs that do not have generated documentation artifacts.\nTableau collector properly catalogs all sites if no site is specified in the CLI/YAML.\nDetails about the release\nItem\nDetails\nRelease version\n2.138\nRelease date\n2 June, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: ae9e23f1063eea7c998e4028a286e93411483a6d7a46c81eb71f11ca48f3e0ca\namd64: a729ad549567254d7cd4e96bf7546457b8977f250c20ecef558bf97c1565dd0c\nNew features and changes\nMonte Carlo collector: The collector now:\nCatalogs additional metadata for incidents, monitors, and tables.\nUses a smaller default GraphQL page size.\nBug fixes\nBigQuery collector:\nProperly handles issue where table IDs are returned as null.\nProperly handles issue with table IDs that have quotes in them.\nMonte Carlo collector: Properly handles external URLs for tables that contain spaces.\ndbt Core and dbt Cloud collectors: Properly harvests all meta config containing object values.\nProfiling: Properly handles histogram values containing excessive range, overflow, or underflow values.\nDetails about the release\nItem\nDetails\nRelease version\n2.137\nRelease date\n23 May, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 4976a94145f0872a6bf99135accfa3bf2d656d8a7ff9b492c8064a1f4cf1d807\namd64: a5f3651c5816c27888f5b0434255e35e6005cd4ede3849c254f8a18ac3843350\nNew features and changes\nThe new Azure Data Lake Storage Gen2 collector is now available.\nSnowflake collector: The collector now harvests:\nSnowflake Stored Procedures and Functions\nLineage between functions and database objects\nDatabricks collector and Power BI collector properly identify database object for Power BI connections to Databricks database objects.\nDatabricks collector: The collector now harvests:\nJobs, Tasks, and Clusters\nFunction\nColumn-level lineage for Hive metastore and Unity Catalog\nLineage between Tasks and the Notebooks referenced in Tasks\nLineage between upstream and downstream table with intermediate Job\nPrimary and foreign keys for Tables\nColumn statistics\nBug fixes\ndbt and dbt Cloud collectors now properly harvests meta config containing object values.\nThis release was for internal improvements and has no customer impacting changes.\nItem\nDetails\nRelease version\n2.136\nRelease date\n22 May, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 41889d3723ac38294a45771188f294729eff046deca34f986a4e6a580d0b77ba\namd64: ae06ee0bb4445f1c532318c2e6bf0cfdf89d750805eff0d23ad46646d5f6468b\nDetails about the release\nItem\nDetails\nRelease version\n2.134\nRelease date\n9 May, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: a97fa7126a89d4167f631932f9961e0de9b7a50522763372475a0e9f1cae7597\namd64: 1542530a6d7fab3487182e52178eeccb9c52f838feef4ca2bf966552e0d6f3f0\nBug fixes\nSnowflake collector: Fixed an issue in the collector when calculating sample size for harvesting column-statistics.\nDetails about the release\nItem\nDetails\nRelease version\n2.133\nRelease date\n26 April, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: bec64902a5d224a3167690e30d28e92e977d5f80b2409496f9de81f8f1f71e13\namd64: 9c7b987f273be3d1267c9e22205e3b23505446fcd80a50c59f488d0338f2437a\nNew features and changes\nMonte Carlo collector\nReduced the number of API calls to improve the performance of the collector runs for large Monte Carlo instances.\nA new parameter --montecarlo-incident-lookback-days is now available to harvest incidents from a specific number of days from collector run.\nA new parameter \u2013montecarlo-domain is now available to harvest resources from specified domain names.\nFor details about these new parameters, please see the Monte Carlo collector documentation.\nBug fixes\nFixed a NullPointerException issue which occurred while harvesting column-statistics for columns with large string.\nDetails about the release\nItem\nDetails\nRelease version\n2.132\nRelease date\n18 April, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: f3d1cb23e0b3077741f2d05321cb4f456d8eb8703074d82de3225dd8c81fb2a0\namd64: d5d328a7460617d45dc9db2c868e6faf5e84365a74bc9e858e99879079293ce0\nNew features and changes\nThe new ThoughtSpot collector is now available.\nGrafana collector: The collector now produces catalog outputs containing hashed namespace. This allows the resources with spaces to be properly harvested.\nMonte Carlo collector: The collector now has improved logging messages.\nDetails about the release\nItem\nDetails\nRelease version\n2.131\nRelease date\n17 April, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: c8f4d7e5374ce7fc2d835fac1ff008ef0e0e0ddfc421997abd87d3453295c702\namd64: 2aca9a07affac355c6e2ed688b4aa297337c2245aa015db1a932c6cfcab9f0cf\nNew features and changes\nPower BI collector: Performance improvements made to the Power BI collector. The collector now utilizes less memory when parsing expressions to harvest Lineage relationships.\nSnowflake collector: Removed JDBC URL parsing warning messages from the collector log file. These warnings were caused due to Snowflake JDBC driver.\nCollector logs: The logs now include the Operating System information on which the collector (jar) is running.\nDetails about the release\nItem\nDetails\nRelease version\n2.130\nRelease date\n7 April, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 2403a222d94cdcc1a1fbc19c921de986aa74aaf7e9ea7b729ee146d207489012\namd64: 67b534504174f86ce0a611b85e6aebd665e4a19b02008b117d37401c59ab9f4b\nBug fixes\nPower BI collector: The collector now properly handles escape characters in the directory paths of SharePoint files.\nBigQuery collector: The release includes an update to how catalog resource IRIs are generated to ensure proper lineage relationships to other systems such as Tableau.\nProfiling:\nProfiling properly generates column histograms for string data types.\nProfiling properly supports decimal values that are stored in scientific notation format\nDetails about the release\nItem\nDetails\nRelease version\n2.129\nRelease date\n28 March, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 13b4a37f5c21531bb282525c7282001857b5959ee6276665f149b4a5ebe7f7d4\namd64: 771c0739c3698f8bdfaab1acc97524ad9a6de0813245a6c38cfb5803de9c584d\nBug fixes\nMicrosoft SQL Server collector:\nFixed an issue that prevented profiling from working with the collector.\nAddressed an issue that prevented parsing of some Microsoft SQL Server views to harvest lineage.\nMonte Carlo collector: Added a page size option to the collector, which helps if a customer runs into timeouts with the current default of 5000. Set the optional --montecarlo-graphql-page-size parameter to use this option.\nTableau collector: Made an adjustment to a query in Tableau so that a warning message which previously printed Column null with id... will now show the column name rather than null.\nDetails about the release\nThis release was for internal improvements and has no customer impacting changes.\nItem\nDetails\nRelease version\n2.128\nRelease date\n22 March, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 70328cef174353aaee098316df6324d6c7777805d0e8dca25210976c4083b979\namd64: 4568d255e7c9a36ab82ac980ec915decc7beddebb1e3f124a8e5f18bd3515c27\nDetails about the release\nItem\nDetails\nRelease version\n2.127\nRelease date\n21 March, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a01a2686c8317986cea74500dd1018152614f08937cbbbd4232ff675cc4e3a2100\namd64:\u00a03f1a3f175fcfafa3d7dae390850eb7fa2aa9393c9f1196316bce4c80e64b910b\nNew features and changes\ndbt cloud collector is now available. Detailed documentation about the collector is available here.\nDetails about the release\nItem\nDetails\nRelease version\n2.126\nRelease date\n17 March, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 44d78be268b948226cad5fc41310202e34e30c5313f026380a00554e135ddb27\namd64: f9033e2060fea8f22beb599296800a4b4494592878e0048dd2de59bf7a308321\nNew features and changes\nSnowflake collector: supports profiling for columns with values stored in scientific notations.\nBug fixes\nPower BI collector: Fixed an issue with tabular files to properly handle invalid paths or http paths.\nDetails about the release\nItem\nDetails\nRelease version\n2.125\nRelease date\n9 March, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a0 37c33410f1e3162b11e0885600f860d3fe9a41790faeed62cf791b4289797703\namd64 : 64e38233b4c47fac90e2d68eafa948a4c46fbf7f3504968e28244857298b2a46\nBug fixes\nFivetran collector: Updated destination identifiers to match the case for currently supported database types. Specifically, this resolves the duplicate Snowflake resource pages issue.\nSnowflake collector: Fixed an issue that was causing\u00a0duplicate snowflake tag-value pairs.\nTableau collector: Updated project filtering to ensure collector harvests calculated fields which are referenced in a sheet but were not created in the sheet.\nDetails about the release\nItem\nDetails\nRelease version\n2.124\nRelease date\n21 February, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: b756d2f91373067746af00d951e117fabcd930d65df2dcb27706ee05689f495c\namd64: 165071142006ba509759d1e5d7fa49a57e9b09ff9d1ce665bf41a6683685d27b\nNew features and changes\nAmazon S3 Collector: The new Amazon S3 collector harvests metadata about buckets and objects, including the Region, Version State, Size, Last Modified Data, ACL Owner, Grantee and Grant Permission, amongst others. See all the details about this collector in \u00a0this documentation.\nBigQuery collector enhancements:\nYou can now harvest column-level lineage between views and tables, as well as more metadata about datasets, projects, tables, and views.\nThe collector now provide an option to do a test run to validate that the collector can authenticate to the specified source system. This is done by adding the --dry-run parameter while running the collector. If specified, the collector does not actually harvest any metadata, but just checks the connection parameters provided by the user and reports success or failure at connecting.\nBug fixes\nPostgres, Snowflake, Redshift, Microsoft SQL Server collectors: When parsing view definition SQL to harvest column-level lineage, the collectors now correctly parse SQL in which tables are fully qualified in the FROM clause but not in the SELECT clause.\nPower BI collector: The Power BI collector has changed\u00a0the URL used as the dwec:externalUrl property from Power BI's embedUrl to Power BI's webUrl, which now allows the user to open the Report, Dashboard, or Dataset in a browser. Additionally, the collector now harvests the embedUrl from Power BI as a separate property, kos:embedUrl.\nSnowflake Collector:  The collector now handles scenario when Snowflake JDBC driver\u00a0does not\u00a0provide valid default values for certain database columns.\nDetails about this release\nItem\nDetails\nRelease version\n2.123\nRelease date\n13 February, 2023\nDocker image ID\narm64:\u00a07e2738ad5f2dae819332ef2f17a1cc34adaa0e3af167bdfa1fd6fedd36520871\namd64:\u00a0ac00e4820508b612a7dbceb865112ad5d9115c258e02842991b0850dc8b4ea89\nNew features and changes\nPostgres, Snowflake, Redshift, Microsoft SQL Server collectors: Enhancements have been made to parsing of\u00a0view definition SQL\u00a0to harvest column-level lineage. We now support joins on named subqueries and correctly handle quoted identifiers.\nBug fixes\nSnowflake collector: The sampling queries used to calculate the column statistics were failing.\nDetails about this release\nItem\nDetails\nRelease version\n2.122\nRelease date\n10 February, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: b50f49f324e6df07864a39b904609b007105eb0378daee89fda1e255fd07a075\namd64: 87e2c4d932f4c584c42b87d2842b81cc3e54a7a42a18af5dd400340a4eae62e5\nNew features and changes\nManta collector:\nCollector now supports Manta version r38.1\nThe collector now also supports token-based authentication.\nJDBC collectors: The description of the --jdbc-property for JDBC collectors is updated for clarity.\nThe following additional collectors now provide an option to do a test run for the collectors to validate that the collector can authenticate to the specified source system. This is done by adding the --dry-run parameter while running the collector. If specified, the collector does not actually harvest any metadata, but just checks the connection parameters provided by the user and reports success or failure at connecting.\nTableau collector\nYAML configuration files used to configure collectors can now interpolate system environment variables and Java system properties. For details about using this feature, see this documentation.\nDetails about this release\nItem\nDetails\nRelease version\n2.121\nRelease date\n2 February, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64:\u00a0a730cf46ed312e871e5d54c6fc89ab1cd050a8c3ed7f0a0155bcae7c1aab7ac3\narm64:\u00a0894f34263dd50b1290d5cbbd63849830150815c10a4969ab57c58ed625466ab1\nNew features and changes\nTableau collector: Improved detection of underlying database type when a Tableau data source uses ODBC.\nBug fixes\ndbt collector: Fixed an issue in the dbt collector that caused coining of IRIs that were inconsistent with IRIs coined by the Snowflake collector, which prevented the linking of database objects between dbt and Snowflake in the catalog. The application now ensures consistency of database object IRIs created by the dbt and Snowflake collectors.\nDetails about this release\nItem\nDetails\nRelease version\n2.120\nRelease date\n27 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: 78e999b0bcd7493ab9c0c48680483689dd7272efe84d006df74e5f33a17c700d\narm64: d26d71eb535ef56bb68cb411292865967b5399a8c49bedb1f5d0e8cda9b8173c\nNew features and changes\nBigQuery collector: The collector now harvests catalog resources representing BigQuery datasets and their associated metadata.\ndbt collector:\nThe collector now supports key pair authentication to Snowflake allowing users to use private-public key pair for authenticating to Snowflake.\nThe collector now has improved detection of target database type information when that information is missing in the profiles.yml file.\nUsers can now use the new --snowflake-account CLI parameter to override snowflake account information from the command line.\nThe help text for --snowflake-role, --snowflake-warehouse, and snowflake-application parameters now include examples and case-sensitivity information.\nSnowflake collector:\nThe collector now supports key pair authentication allowing users to use a private-public key pair for authenticating to Snowflake.\nEnhancements made to parsing of Snowflake SQL dialect when harvesting column-level lineage allows for parsing of statements with copy grants.\nTableau collector The help text now includes examples for the --tableau-project and --tableau-exclude parameters.\nPower BI collector The help text now includes examples for the --include-workspace and --exclude-workspace parameters.\nDetails about this release\nItem\nDetails\nRelease version\n2.119\nRelease date\n20 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\nAMD64: 5a8f9e24ebe05dc027caf74075cac4ce51667271da30935640fc3c9471578445\nARM64: af0b4528e0ee097d29d286c29c803db185f616babdb2a867b6228e77efaf1cd5\nNew features and changes\nA new Further Help section is added to the help available for collectors that is accessed using the -H or --help parameters in the command. It now guides users to the collectors help available on the data.world documentation site.\nThe collectors now emit a globally unique IRI to track collector runs.\nBug fixes\nSnowflake collector: Column statistics now supports Number data type.\nDetails about this release\nItem\nDetails\nRelease version\n2.118\nRelease date\n18 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a0aca6f9202192bac23a5579f88eb155576e46425ad6b901c3febfbab32ff4158a\namd64: 56cbb748ada10f006d41a841034a0a6ba7211085c9682aa32331682ea92d20b0\n\nBug fixes\nSnowflake collector: Column statistics now supports columns with spaces in names.\nTableau collector:\u00a0 The Tableau collector released from version 2.113-2.117 had an issue because of which it was not able to parse GraphQL queries. If you are using collectors between version 2.113-2.117, you must upgrade to 2.118 to be able to use the Tableau collector successfully.\nDetails about this release\nThis release was for internal improvements and has no customer impacting changes.\nItem\nDetails\nRelease version\n2.117\nRelease date\n12 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a0429a55a11d4bcd15647d1316d9debd9ead4b4ab5c0b9146894d07c39aa814290\namd64:\u00a0481dd2da6de71525248eba186feeeafcc73cc956ade0a196a4e8b0c2424e74b9\nDetails about this release\nItem\nDetails\nRelease version\n2.116\nRelease date\n10 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a0011ebeaf6000b1fdc47f1d3f8cb8a7655cbbe3528b844abe2a2cd9bd9fddc0fe\namd64:\u00a0192a2b94b6e58016c8e5f7ae871480e6e38fb74214597640f8b862a245d5c629\nNew features and changes\nPower BI Collector: The following alternate options are added for some of the command line parameters:\nFor --include-user-workspace alternate parameter --user-workspace-include\nFor --include-workspace alternate parameter --workspace-include\nFor --exclude-workspace alternate parameter --workspace-exclude\nBigQuery collector:\nThe collector now harvests additional metadata from projects, datasets, views, and tables available in BigQuery.\nColumn-level lineage added between Views and Tables.\nBug fixes\nSnowflake collector: Fixed issue for parsing a SQL statement that contained copy grants in Views. This helps improve the column-level lineage harvested by the collector.\nDetails about this release\nItem\nDetails\nRelease version\n2.115\nRelease date\n10 January, 2023\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:\u00a0693e1661d3ae178c0d5a2bca8e40f406928e91d34b4c1c749f1cce31bf720592\namd64:\u00a0060e0268ecdfb3d9c7382cce30192c334c1240831edf21681887bc8ee29a33c4\nNew features and changes\nThe documentation of the jdbc-property property for database collectors is improved to explain how users can specify multiple properties. This change applies to 19 collectors that include this parameter.\nA new resource dwec:Source is added to the catalog emitted from database collectors. It is a mechanism that allows users to render specified resource properties as read-only in the data.world catalog UI.\nPower BI collector: The collector now has enhanced parsing of power BI transformation expressions. As a result of this change more column-level lineage\u00a0information\u00a0is harvested from Power BI.\nSnowflake collector: The collector now harvests table usage counts information.\ndbt collector: User-defined database attributes are now enabled for the dbt collector to fully mitigate missing or incomplete profiles YAML file when cataloging database objects referenced by dbt.\nDetails about this release\nItem\nDetails\nRelease version\n2.114\nRelease date\n22 December, 2022\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64:7202f9ae528a73e8ff7e6a29c36a22c8202680e733000886e760b0a3864b692a\namd64: 321d046a526f04b47fee389c3e48222b0b6b6c0d940ff66b938be92d85f59b0\nNew collectors\nThe Grafana collector is now available as a private beta release for select customers. Please contact data.world if you are interested in using this collector.\nNew features and changes\nThe following additional collectors now provide an option to do a test run for the collectors to validate that the collector can authenticate to the specified source system. This is done by adding the --dry-run parameter while running the collector. If specified, the collector does not actually harvest any metadata, but just checks the connection parameters provided by the user and reports success or failure at connecting.\nPower BI\nCatalog graphs (.ttl files) that are automatically uploaded to the data.world platform with -u / --upload are now compressed, enabling larger graphs to be uploaded.\nPower BI collector: The collector now provides a new option --disable-expression-lineage to skip parsing lineage from the source expressions.\nSnowflake collector: The collector has a new ability to harvest table usage and query count. This functionality is enabled by passing --table-usage-collection. It calculates, for each table in the database being harvested, the percentage of tables in the database that have been queried no fewer times than the subject table. The time period over which this analysis is performed is controlled with option --table-usage-lookback-days (that is, the number of days prior to the time when the collector is being run during which queries of each table are tallied), which defaults to a value of 7.\nBug fixes\nSnowflake collector: Fixed an issue with SQL parsing in Snowflake for windowed aggregate functions.\nPower BI collector: Fixed an issue with the Power BI expression parsing related to joins in source expressions.\nLooker collector: Fixed an issue in the Looker collector that caused an abnormal termination of the collector run with certain Looker views.\nDetails about this release\nItem\nDetails\nRelease version\n2.113\nRelease date\n12 December, 2022\nDocker image ID\n(use this to verify the integrity of the Docker image.)\narm64: 0517b905198728ba73bc59304ce06dd60ac99c3f7a25ad84569b94bef41eb1c2\namd64: b52b36ccf20c00fc0bb16b6abcb01496d55c1f64a8425a23a24f9473de54c9e3\nNew Features and changes\nThe following collectors now provide an option to do a test run for the collectors to validate that the collector can authenticate to the specified source system. This is done by adding the --dry-run parameter while running the collector. If specified, the collector does not actually harvest any metadata, but just checks the connection parameters provided by the user and reports success or failure at connecting.\nDatabricks\nDb2\nDenodo\nDremio\nGeneric JDBC\nHive\nInfor Ion\nMysql\nOracle\nPresto\nSalesforce\nSQl Anywhere\nVertica\nPower BI collector: Updated the Power BI collector to harvest metadata for Dataflows.\nDatabricks collector: Updated the Databricks collector driver version to 2.6.32. Drivers available here\ndbt collector: Updated the dbt collector to harvest metas (as key value pairs) for dbt resources.\nDetails about this release\nItem\nDetails\nRelease version\n6 December, 2022\nRelease date\n2.112\nDocker image ID (use this to verify the integrity of the Docker image.)\narm64: 0921bdcf30a1e28f7a1d5094ff806537bfa023af93d2904bce6c9624e8cde3cf\namd64: 80e973a297d89d73d1a3c62d319d11baa23f45bc804e01050868827c60c2ad64\nNew features and changes\nAdded the following options for Snowflake, Redshift, PostgreSql and MS SQL:\n--dry-run: If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\n--enable-column-statistics: to enable harvesting of column statistics (i.e., data profiling)\n--sample-string-values: to enable harvesting of sample values and histograms for columns containing string data\n--target-sample-size: to control the number of rows sampled for computation of column statistics and string-value histograms\nDetails about this release\nItem\nDetails\nRelease date\n29 November, 2022\nRelease number\n2.111\nDocker image ID\n(use this to verify the integrity of the Docker image.)\n\narm64:\u00a04d4cd1fde0816ae5209b72f92f87c798da83dba5b2f155e3614bc89c68f39b71\namd64:\u00a0bdd7daa56b2a62864c59b8c8958100e12080e61dc8f18559555840bef58c8079\nNew features and changes\ndata.world now produce images for the arm64 architecture (in addition to amd64). The addition of arm64 means that dwcc images run seamlessly on M1 mac. As a result of this change, from this release onward two hashes are available per release.\nDetails about this release\nThis release was for internal improvements and has no customer impacting changes.\nItem\nDetails\nRelease date\n12 November, 2022\nRelease version\n2.110\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: 98583ecda023782df1e08a0f2347a536e239186dcca3936d16c67ae1f6aad0f6\nDetails about this release\nItem\nDetails\nRelease date\n10 November, 2022\nRelease version\n2.109\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: 6602f313506e5eb3ea74c296994f7e4d7bd56845c6f2b35e6d1d4cde5f402832\nNew features and changes\nSnowflake collector: Snowflake policy fully-qualified names are being written to the title instead of to the description property.\nDetails about this release\nItem\nDetails\nRelease date\n8 November, 2022\nRelease version\n2.108\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: b81a221abff982a356a21c4430f80da1e4459f0c3ead2d4f4f51a8f1d45c5604\nNew features and changes\nThe --post-process-sparql cli option is now available for all other collectors (This feature was previously made available for some collectors in release version 2.107). This option allows the user to pass in a SPARQL query to post-process the catalog graph created by the collector prior to it being written to the filesystem and/or uploaded to the data.world API.\nBigQuery collector: The option to use the credential file for BigQuery no longer allows use of -c . It must be specified with --credentialFile.\nDetails about this release\nItem\nDetails\nRelease date\n3 November, 2022\nRelease version\n2.107\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: 4a33db022e92488914d1f088d4041a31d76883706ae13a88bf1a0e8aa67eaa51\n\nNew features and changes\nAdded --post-process-sparql cli option to some collectors. This option allows the user to pass in a SPARQL query to post-process the catalog graph created by the collector prior to it being written to the filesystem or uploaded to the data.world API.\nBug fixes\nFixed an issue where MS SQL Server database objects referenced from Power BI did not always link to those objects harvested by the SQL Server collector due to mismatched IRIs.\nDetails about this release\nItem\nDetails\nRelease date\n30 October 2022\nRelease version\n2.106\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64 0ac22da04737fbcaaac0da9d076eaf92e3cdd870c85544dd16a556f54a8900a8\nNew features and changes\nGoogle BigQuery collector: The collector is updated to coin IRIs for database objects that align with IRIs coined by other collectors.\ndbt collector The collector now writes catalog records for each Snowflake tag and policy.\nBug fixes\nFixed a defect in which boolean properties that appeared in the global_options section of a dwcc configuration file were not properly recognized.\nDetails about this release\nItem\nDetails\nRelease date\n28 October, 2022\nRelease version\n2.105\nDocker image ID\n(use this to verify the integrity of the Docker image.)\namd64: 427443cadbc21a3e26f095a4c054f6193bf3c2b96d257cdb22b57abd061bad68\nNew features and changes\nPower BI collector: The collector now supports the ability to include specific workspaces for cataloging via the parameter --include-workspace. The collector continues to allow exclusion of specific workspaces with --exclude-workspace. Use of --include-workspace takes precedence.\ndbt collector:\nThe collector now supports harvesting of dbt projects/artifacts that specify Snowflake as the target database.\nThe collector now correctly coins database object (e.g., database, schema, table, column) IRIs that align with IRIs coined by the JDBC collectors. Previously, if the case used for identifiers in dbt artifacts did not match the target database\u2019s default collation, the IRIs would not align (they do now).\n10-25-22\nCollector v2.104\nhash: 0b01f8c379e52f3167577a6fd1e5ad2f8d2f3d73871797ae3859b79f83bf5c29\nUpdated the Monte Carlo collector to add a --bigquery-credentials-file option, in order to standardize the option since the dbt collector has a --bigquery-credentials-file option (note the --big-query-credentialFile still exists in Monte Carlo, this is a new alias for the same option).\nThe Snowflake collector now harvests Tags, Masking Policies, and Row Access Policies, and associate these resources with the database objects to which they apply. There are new CLI options in order to include these: --tag-collection and --policy-collection.\n10-14-22 Collector v2.103\nhash: 0f4f021c4c8fc17c7f47618ef9942e255327eef0d4c749e453a06e1d0e96760b\nUpdated SQL Server collector to harvest intra-database lineage from views.\nUpdated the log messages for missing files not required for DBT collector to run.\nAdded table name to warning messages in Tableau collector in addition to the table ID.\nAdded parent-child relationship between projects in Tableau collector when the parent project is not included in the filtered projects.\nAdded pagination for certain queries in Tableau to prevent the result hitting the max node limit.\nUpdated automatic catalog upload functionality to accommodate large catalog graphs.\n9-30-22 Collector v2.102\nhash: 4dd8a1bdc776f0e8eb352954298842867c7873224d658f2abd8faefe31c40a76\nUpdated the Tableau collector to accommodate changes in the Tableau metadata api that were preventing detection of lineage relationships between Tableau fields and underlying database objects.\nUpdated the AWS Glue collector to handle an error with jobs that have a space or other invalid characters in their paths.\nUpdated the Databricks collector to include the UserAgentEntry property in the jdbc connection.\nThe collectors will now emit the Collector version to the logs.\nAdded a fix to SQL parsing for window aggregate functions (e.g. SUM(X) OVER (PARTITION by Y ORDER BY Z..) )\n9-19-22 Collector v2.101\nhash: 2be46a6268e34acceedd5b80412787d10f732ad1a2f1ceb83c6d5ce2fe819457\nAdded a filtering feature to filter Tableau fields by project in Tableau collector.\nFixed an intermittent authentication issue associated with harvesting metadata from a single site with Tableau collector.\nAdded a log message for missing job script in AWS Glue collector.\nEnhanced harvesting of column-level lineage from database views, including handling SQL SELECT statements missing a FROM clause, and updated list of Snowflake keywords passed to functions.\n9-9-22 Collector v2.100\nhash: 946a0c51c091e74d6043dea1450a1ac818546b040e702e91526da185297a2858\nFixed the Fivetran collector so that it doesn't produce \"blank\" nodes (no id or name)\nAdded a change to use log_level rather than log-level.\n9-2-22 Collector v2.99\nhash: 68e4c4d6a6b40cb91a8e574a1f106c9c20ba2f1156a93f5f871b0284e975a766\nFixed an issue in Power BI with the new metadata API calls.\n9-1-22 Collector v2.98\nhash: 228090b0af31681952b7ccd5abef9beaf070450692501fef527bff8ca32280cb\nAdded harvesting of column-level lineage in the dbt collector, for dbt projects that target one of the collectors for which intra-database lineage is supported (i.e., Snowflake, Redshift, and PostgreSQL).\n8-29-22 Collector v2.97\nhash: 0a2134ef29a057b0c003ab353c14f034e65702b298a450f01fafcea5b6e8c1ea\nPostgreSQL can now be cataloged using either catalog-postgres or catalog-postgresql for the command.\nMicrosoft SQL Server collector now harvests SQL Server extended properties for databases, schemas, tables, and columns.\nColumn-level lineage harvesting in the Snowflake, Redshift, and PostgreSQL collectors now properly harvests lineage from views whose sql statements include comments starting with \u201c--\u201c, and also statements with inline subselects.\nThe dbt collector harvests process (activity) and model (agent) metadata using PROV-O qualified derivations.\n8-17-22 Collector v2.96 (no 2.95 release)\nhash: a09e365296d57965385563569c6c58a6f706da1a4c1c6d711141aabd316d8629\nTableau collector now supports multiple --tableau-project options, allowing the user to include multiple projects in the same collector run.\nTableau collector no longer associates Custom SQL Table resources as part of a database.\nThe Collector no longer includes a bundled jdbc driver for Salesforce. Please contact support@data.world for assistance in obtaining an appropriate JDBC driver.\n8-15-22 Collector v2.94\nhash: ac21b2f728b79e3dff38c2a395a81c0b0b1558979b8385747c6d00b76e1d6724\nEnhancements to the Tableau Collector for project filtering and additional logging\nPostgress additional triple for Table to Database linking\n8-2-22 Collector v2.93\nhash: afdecd160fd38e3db565cb14db3805fed05fa86b5c3a70662d0c8f0b0d10799f\nIncludes some internal dependency updates.\nEnhancements to the DBT collector to validate the input profile.yaml file\n7-29-22 Collector v2.92\nhash: 9b87934376246cd3926bfe413d36f2a7a0f2e7d848d7f2e68380d6035fe276f6\nEnhancement to the tableau collector to add a retry if a graphql query fails\nEnhancement to the collectors to add a check at the beginning of a collector run to ensure the output directory exists (if the o/-output option is used), which will log an error and stop if the output directory doesn't exist.\n7-21-22 Collector v2.91\nhash: 708b34b19b2695d14d5a74a8281d5365cb659c4eeeebb791b3ebf2aa2e4d6686\nEnhancement to the Tableau collector to reauthenticate if the Tableau API reports failed authorization during the collector run.\nReleased the new Fivetran collector (catalog-fivetran)\n7-12-22 Collector v2.90\nhash: bf94b0431b5a99dd95f485a8a48f202ea138f103b54873b4440b5080d86d529a\nAdded the parameter --include-information-schema for Snowflake and SQL Server collectors; we no longer catalog the information schema in these collectors when --all-schemas is specified, unless the user also specifies --include-information-schema.\nImproved handling of manifest json structures with some nulls in the dbt collector.\nAdded reporting on user access issues during parsing/resolution for Snowflake collector.\n7-9-22 Collector v2.89\nhash: 848e38708b832c703652dc45d148e471a2341bce7f6ec159c2471f287a8d3620\nUpdated tableau collector to print a clear log message when authentication expires during a collector run\nUpdate tableau collector to allow optimized serialization of API requests under JDK 17\n7-1-22 Collectorv2.88\nhash: 8534190cb3f0f93bd2a326abd54086e89eb38ece8180bf0487486dc66242d6c8\nSignificant updates to the Power BI collector. As of this release Power BI Collector outputs different classes than the version before it. The collector now emits information about where it is sourcing it's data.\nInternal developer and testing improvements\nThe MANTA collector is now more specific about the concepts that it emits about Informatica PC.\n6-24-22 Collectorv2.87\nhash: 7860e33213ba90783851cd7f7e6529ee99a5f261ae086d3a7038938c6f290ae6\nThe information schema collector now explicitly supports Oracle.\nWe have added enhancements to the dbt collector to harvest DBT snapshots and sources.\n6-22-22 Collectorv2.86\nhash: 6fdae2dd70896e402ca648701bcd48210a8fd5979230c958b0dc06030bd7b1ec\nFor collectors that take API endpoint URLs, the data.world Collector will add a trailing slash to the URL if needed and not specified by the user\nNew command-line option --warehouse available for the Snowflake collector that allows the user to specify which warehouse to use to connect to snowflake.\n6-18-22 Collectorv2.85\nhash: aaa6e55bf19af7ef37f1ab80ad28522af77a6ff286ef616085d92ab51f7d7899\nAdded a the data.world Collector collector for dbt - legacy collector still available.\nFixed an issue with auto-uploaded log files, in which not all log messages were being written out.\n6-15-22 Collectorv2.84\nhash: 2e128cd3c89ffc8c35fbad12f6ee4ba7e6e5cdf9bfcf991fac78e8033d5d17d0\nLooker collector now emits resources for Looker Views and relates the Looker Dimensions and Looker Measures that are configured within those Views.\nImproved handling of unexpected database types encountered when cataloging Tableau.\n6-3-22 Collectorv2.83\nhash: 9487027423a076231cec76f5679f044493a3d75032882c4ca0e5cf1c0304e6cf\nFurther improvements in handling of SQL ORDER BY, GROUP BY, WHERE, and HAVING clauses when harvesting intra-database lineage from database views.\n6-2-22 Collector2.82\nhash: 22459e3d3a2a38f448d4e56137ed4ecd05170767b5a682dd4870135cceff23c2\nCorrected coining of IRIs in catalog graphs emitted by the Tableau collector.\nImproved logging in the Tableau collector to detect unknown linked database types.\nImproved harvesting of lineage between database views and referenced columns, including support for columns in SQL ORDER BY, HAVING, GROUP BY, and WHERE clauses, and parsing of a wider range of column expressions.\nthe data.world Collectorv2.81 - INTERNAL RELEASE\n5-24-22 the data.world Collectorv2.80\nHash: be5a85c754d54328accabe332dec55ce507baddbe68d2fe9e29a211e9ea1420f\nWith this release, the data.world Collector now requires Java 17. If you run the collector from within Docker this change will not affect you. If you run the data.world Collector from a .jar file, you will need to upgrade your JRE to 17 to run DWCv2.80 and greater.\nAdd the parameter --disable\u00a0lineage-collection to enable users to turn off cataloging lineage for PostgreSQL, Redshift, MS SQL Server, and Snowflake\n5-13-22 the data.world Collectorv2.79\nHash: 5b548c82b96ad5e5dbd4770adff205c9d07cac3c5f949882d7d9381240366ddb\nThe Manta collector can now accept OAuth tokens for MANTA authentication (for harvesting metadata from manta version R35 and above)\nWe have released a new collector powerbigov that only allows tenantid for auth and not user/password and connects to the government powerbi api urls.\n5-11-22 the data.world Collectorv2.78\nHash: 71edd8ff7a4c3ed8a91eaf36d59c8e2745b7a76f8666b5750cbee8205021c9c6\nAdded some small Tableau collector enhancements.\nNew PowerBiGov collector with specific endpoints for .gov customers. This collector does not accept a username or password.\nFor PowerBI, a new way to authenticate is available. A user can now enter a tenant ID with a client id and a client secret to authenticate, in addition to using a username and password.\nFor both PowerBi and PowerBiGov, when using the tenantid, secret and client id authentication method, this collector no longer emits information about PowerBI Apps.\n4-27-22 the data.world Collectorv2.77\nhash: 4bed848791cfa9e46c9db4a78c7a593bb1c986900dc6fcfcd4255ddce1528579\nFixes an issue with the Snowflake collector that prevented the bundled jdbc driver from being found. Any users working with the data.world Collector 2.76 should update.\n4-22-22 the data.world Collectorv2.76\nhash: 30e60a4434ee64d2981b40eb2dc92506da3d367eab22bc0bca0c61bdd44a3f02\nThe Snowflake collector harvests some intra-database lineage information from database views.\nImproved the host mapping in the Manta collector.\n4-7-22 the data.world Collectorv2.75\nhash: 1a59dbb3ff8679fb6ee22eadaeb04ccdb28c5660be029e78fbc96403ae33096f\nthe Manta collector now emits resources for file sources and targets and their directory structure. It also emits sources and targets as files.\n4-1-22 the data.world Collectorv2.74\nhash: 219428f6a72be91205408d5cb3f8cc8b27e1a9a4df0208e4cacb8fbaa1352f90\nThe Tableau collector now emits \u201ccolumn-level lineage\u201d:\nImproved styling of the data.world Collector command-line errors\nUpdated command-line options for Datakin and Marquez.\n3-16-22 dbt collector v.05\nThis version adds a third command-line argument to specify an output file name.\n3-8-22 the data.world Collectorv2.73\nhash: 119daf987dcfad25db599e1c1affedf17a35ff2aa002d0618d642eb309cebaaf\nPermalinks to Looker explores included via externalUrl\nImprovements to datakin/marquez collectors\nTableau collector now emits resources for Tableau Projects, allowing us to establish full relationships between projects and the workbooks and views that they contain\nMonte Carlo data collector now emits data quality information using enhanced dwec ontology concepts\nLooker collector now emits descriptions for measures and dimensions\nMANTA collector now emits Snowflake resources found in MANTA scans\n3-1-22 the data.world Collectorv2.72\nHash: 62d156aca58ec92513e8d6490f00fd10ee52dfb7a65f71c20c6a988c938dfddd\n[BUGFIX] Invalid prefix when using --base option\nUpdate the data.world Collector transform to add catalog events to specific collectors\nAdded a Snowflake Sensitive Data Discovery collector\nSync CLI options between collector types\nValidation of CLI options for the data.world Collector\nImprovements to the the data.world Collector CLI\nUpdate the MonteCarlo Collector to use the new Data Quality Ontology\n2-17-22 the data.world Collectorv2.71\nDigest: 03fc3df90ae63896d62ea22e00688f42cacf5b76d0f47691c06c104736680b2a\nBug fix for Marquez collector\nBug fix for Manta collector\n2-9-22 the data.world Collectorv2.70\nDigest: 06bb747c4d7705c1e44664de7854158d87468316bab549ec5604b0a075380c69\nPreview images for Tableau assets are now harvested much more efficiently, and the resulting image data in the catalog graph are much smaller, reducing catalog harvest run time and enabling image objects to remain within platform constraints during ingest.\nFix for unexpected column type errors in BigQuery collector\n2-8-22 the data.world Collectorv2.69\nDigest: 5ab9b97d5f8f4568613438a9e52b0bdc12974f8d6edd0dab374a281c4982c737\nCreated new collectors for Marquez and Datakin\nAdded schema information to the Tableau collector outputs\n2-4-22 the data.world Collectorv2.68\nDigest: 23674ee02a6b725d5f9a453615dc507286da2ee606dca83c386472f3aa36d118\nThe Tableau collector now accepts Tableau \u201cPersonal Access Tokens\u201d for authentication, via new cli options --tableau-pat-name and --tableau-pat-secret.\nFixed an issue with mis-identification of views as tables in BigQuery.\n2-2-22 the data.world Collectorv2.67\nDigest: 032867c9c52c8d46dc0b90a61a128be65ecec1440bb0adccb8b0d1b249b4e351\nFixed an issue with server name identification in Manta.\n1-26-22 the data.world Collectorv2.66\nDigest: fa9ae2eb3d68375a3ff01ac7bde98fd36f372b84dce0d411444146ea9566b47b\nWith this release the Athena collector is no longer a JDBC collector--we harvest metadata by accessing the Athena API directly, rather than going through a JDBC driver. This means that it is no longer necessary to provide a JDBC driver when running the collector.\n1-10-22 the data.world Collectorv2.65\nDigest: ed08cdd21a374c30456de0989076f5180bc4187ca998358b051807e521fd44e6\nThis release adds a new option for the MANTA collector, --manta-max-parallel-scenarios. Specifying this option and passing an integer value will configure the MANTA API to export the specified number of scenarios in the MANTA graph in parallel. The default value is 4; adjusting this up or down can improve performance.\n1-5-22 DWCv2.64\nDigest: 45b72798b0602885790388331a75db1f4286b15bf57b21f30f416eda79041571\nThis release upgrades the data.world Collector's dependency on the Apache Jena RDF library to version 4.2.0, which addresses security vulnerability\u00a0https://nvd.nist.gov/vuln/detail/CVE-2021-39239.\n12-23-21 the data.world Collectorv2.63\nDigest: sha256:eb4208c914269c793a5e2143d59a9982e7b087c5da1c17dd075e02a326e64a3e\nThe Athena JDBC driver is no longer bundled with the data.world Collector as we have discovered that the Athena driver itself has a dependency on a vulnerable version of log4j. Customers that use the the data.world Collector Athena collector will now need to supply their own driver and put it in the jdbc driver directory (as is done with other collectors for which we don\u2019t distribute a driver).\n12-15-21 the data.world Collectorv2.62\nDigest: sha256:2cd579e09f4eee94e141e8cf7e4e40e9a9b8803029df1be7112d67d62ef33b9e\nThe Oracle collector now supports connecting to the database via SID (instance ID) or Service Name. Service Name is the default. If a connection via SID is desired, pass the SID as the value of the\u00a0-d/ --database\u00a0option and add the\u00a0--oracle-sid-mode\u00a0option (flag).\n12-13-21 the data.world Collectorv2.61\nDigest: sha256:bd0ba96208d714ecef4131867cf5d16372be0a33f416c1d6bd01f132c8517323\nThe information schema collector has been modified so that the files table_constraints.csv and constraint_column_usage are now optional, not required.\n12-10-21 the data.world Collectorv2.60\nDigest: sha256:7fd825bfe7d2f99c9a1298ad26bc1934c9657cc7c5868dd093844344d18fc7b7\nUpdated the BigQuery collector to support current Google Cloud API enhancements.\nAdded a new Information Schema Collector. This collector runs via the\u00a0{{catalog-information-schema}}\u00a0command and is notably cataloging four CSV files that are provided to the collector via a\u00a0{{--csv-file-directory}}\u00a0parameter rather than connecting to a database. This collector is an option for customers with tricky DB setups that do not allow them to authenticate or establish connections to their DB via our normal the data.world Collector collectors.\n12-2-21 the data.world Collectorv2.59\nDigest: sha256:051f76748be1c6cf2c7557600dde71a39e1b822c9e49120881ce938f1c8c2b80\nVerified the Manta collector works with MANTA R34.\nReleased the config file command.\nModified the Tableau collector to remove schema and database names from table names.\nUpdated the BigQuery collector to support cataloging all datasets in a project at once by default, and to be able to use cli options to select specific datasets in a project as well. With this last change, the \u00a0--dataset param is no longer required. The help text has been updated with new messaging to reflect these changes.\n11-10-21 the data.world Collectorv2.58\nDigest: sha256:82ebc1cec46f70de000aa94695359bd28d65c2782afc362c9ce14fadc04eae07\nAdded a new collector for Hive (as an alternative to\u00a0catalog-hive) that uses only the Hive metastore--it does not connect to the Hive server directly.\nThe PowerBI collector now harvests workspaces and identifies other assets as being in workspaces\nthe data.world Collector now emits \u201ccatalog events\u201d into the catalog graph. These capture details about the cataloging process itself, including selected configuration options with which the Collector was run, and summary statistics about the catalog. The ingest process will soon extract this information from catalogs at ingest time and send them to segment for downstream analysis.\n11-1-21 the data.world Collectorv2.57\nDigest: sha256:606f7cfbe60bf56b4c2ecd5fb3902d4de621e31ae76ad78e68c56c788f81e5e6\nFixed an issue in the Tableau collector in which Custom SQL Table objects without an associated database were not handled correctly.\n10-27-21 the data.world Collectorv2.56\nDigest: sha256:335f7e110a9506d95dff05971492e6509fb8537e74f9275d04dcf9e2427df0f0\nAdded new cli options to salesforce collector so that it can handle sandbox environments and custom login domains customers might have.\n10-25-21 the data.world Collectorv2.55\nDigest: sha256:c60ae69edc88b8801be833d578ef5dca73b6302646be9b30d31ccdfd7444288a\nThis release updates the BigQuery collector to handle fields in BigQuery tables for which the BigQuery API returns null type.\n10-5-21 the data.world Collectorv2.53\nDigest: sha256:59c960d525e66e77d08dd34fd58c9b5027334a4bd2271f1f059370ae006a4b0b\nEnhancements to the MANTA collector to harvest additional lineage information from MANTA scans (lineage from Informatica PowerCenter in particular)\nTableau collector enhancement to provide a better warning to the user when an obsolete version of the Tableau API is specified\n9-29-21 the data.world Collectorv2.52\nDigest: sha256:915e4e91841001f80a84a65fcd76350b9a1d53f4e31678bb0e628d32beab94a1\nFixed an issue with the handling of certain fields and database information when the Tableau collector was run with a non-admin credential.\n9-28-21 the data.world Collectorv2.51 (internal)\nDigest: sha256:261c5bf33b2ae38cbda35a346fcb37c56bbf8ebfb773f328deb9140efba1c8bf\nFixedan issue with the Tableau collector issue to handle views/workbooks that exist outside of a project.\n9-28-21 the data.world Collectorv2.50 (internal)\nDigest: sha256:b407c629247f36afac3869eb8320464fce8caeb2865dd79811882b54ef94d1b5\nFixed an issue with the Tableau collector to handle workbooks that exist outside of projects.\n9-24-21 the data.world Collectorv2.49\nDigest: sha256:397e78867f41aaa393ff69f42b0fa524fdcad662ddd027925cf27f80497b24ce\nAdded a collector for Salesforce (catalog-slesforce)\nFixed a IRI mismatch issue for Tableau Collector when running on Tableau instances with a Snowflake datasource.\n9-18-21 the data.world Collectorv2.48\nDigest: sha256:c36755489b6235408aa4e639e6e184cab027a32a34e3b8ca369c3c6b3c4bff96\nMade internal improvements to the tableau collector to enable more efficient querying of the Tableau metadata api.\nFixed an issue in the manta collector in which certain missing data in the MANTA lineage graph caused an exception\n9-10-21 the data.world Collectorv2.47\nDigest: sha256:219edfa247929e15d7c4e2be99ef890b2487c398abc1a23b2f85b3de11812be3\nFixed an issue in the Reltio collector that occured when a Reltio configuration was missing certain objects.\nAdded a collector for Databricks (catalog-databricks)\n9-8-21 the data.world Collectorv2.46\nDigest: sha256:e48cba45b457e076714d94d3a83d1164cb892864213732b3b2b334c041ff178a\nFixed an issue with creation of resource IRIs by certain collectors when the user chooses version 1 minting\nUpdated BigQuery collector to enable integration with data.world platform / connection manager\nFixed an issue with the MANTA collector in which certain large MANTA scans caused a numeric overflow during json de-serialization\nUpdated Reltio collector to include information about survivorship groups in the emitted catalog\n8-24-21 the data.world Collectorv2.45\nDigest: sha256:77f4c784b1d0166cf3bb87903696528f712fbe6aee1d4cb7e60097a0f494c7de\nThis release fixed an issue with JDBC drivers not being loaded by the Athena collector.\nAdded a collector for Reltio configurations (catalog-reltio).\nthe data.world Collectorv2.44\nDigest: sha256:47c1bb38b88c25801adf1f765e23c63637d15a60ae11fca8d63b53a8cd4755b2\nFixes an issue with URLs for sheets and dashboards that exist in Tableau Online or in Tableau Server within a site other than the default site.\nthe data.world Collectorv2.43\nDigest:\nsha256:696deaad59d2948a6adf3c275a90539cbf87057c93de9ee94d911fe105c574ce\nAdditional datetime fields added for Looker objects and typed as\u00a0xsd:dateTime.\nFixed an issue caused by an undocumented change in Tableau Online\u2019s REST API when using the Tableau collector to harvest metadata from Tableau Online.\nthe data.world Collectorv2.42\nDigest: sha256:e6bc353ea4b2ec3486b54d4e9280856d328d93f5d406e367c0c50303cde93704\nThe generic jdbc collector harvests database name when cataloging Intersystems Cache databases\nRunning the Snowflake collector with the\u00a0-A\u00a0/\u00a0--all-schemas\u00a0option harvests metadata from all available schemas, as with other collectors\nthe data.world Collectorv2.41\nDigest: sha256:bb79aa8afd19bf35b4b7e75840c21598702ec1d74b5f8640cc72a6758a3a0bc9\nFixed an issue with permalinks to objects in the MANTA collector .\nthe data.world Collectorv2.40\nDIGEST: sha256:44dd710a49a1500863f49e2f2e4ef261a45cdc6c7354702fe8e764210c27293b\nAdded support for Looker folders and additional attributes to the Looker metadata collector.\nAdded the ability to preview images to the Tableau metadata collector.\nthe data.world Collectorv2.39\nDigest: sha256:992671530f7483bfeb8a2aab52880a524b7df79caf427b373bd825115d71f4dc\nFixed an issue with the handling of certain special characters in catalog resource IRIs.\nThe\u00a0--schema\u00a0option for JDBC collectors can now be specified multiple times to enable the cataloging of multiple schemas in a single catalog.\nthe data.world Collectorv2.38\nInternal release\nthe data.world Collectorv2.37\nDigest: sha256:6a84217fa33df75d67ce51c486a90a802a8313a3432835abb55fffb5f1d3afc7\nUpdated Tableau collector to paginate additional graphql queries to avoid hitting Tableau Metadata API limits.\nUpdated the Hive2 collector to capture table-level metadata from the hive metastore\nUpdated the Tableau collector to allow the user to exclude specified Tableau objects from the catalog\nthe data.world Collectorv2.36\nDigest: sha256:8dd9793f3b0e74adcd7e7bc153f06b8c3098470217fb07af4336dde611269671\nImprovements to error messages produced when using\u00a0a config-file to run the data.world Collector\nWe disallow running\u00a0catalog-postgres\u00a0and\u00a0catalog-redshift\u00a0in the same config file as the two collectors use incompatible JDBC drivers\nImproved error handling throughout the data.world Collector\nImprovements in representation of Tableau data source names in tableau catalogs\nImprovements to the MANTA collector\nthe data.world Collector v2.35 Changes in this release:\nUpgrade of Denodo collector to Denodo 8\nHandle edge case of very large field values embedded in manta\u2019s exported artifacts\nSupport for sites\nHandle edge case of stored procedure columns in manta\nthe data.world Collector v2.34 This release includes:\nEnhancements to domo collector output\nTesting improvements\nA minor tableau collector enhancement\nFix for an issue in the tableau collector in which column fields were sometimes not properly identifying the Tableau Table from which they sourced their data\nImprovment to the presentation of domo catalogs in the platform UI.\nChanges to the dockerhub repository where we house images containing non-released versions of the data.world Collector. Previously we were calling these \u201cbeta\u201d releases; we now call them \u201crelease candidates\u201d. The new repository is datadotworld/dwcc-rc and the image tags are x.y-rc-z where x.y is the next expected Collector release, and z is an increment.\nthe data.world Collector v2.33 Adds support for harvesting intra-database lineage from manta scans, and accommodates changes in MANTA R32 (aka 1.32). We no longer support MANTA versions earlier than MANTA R32.\nthe data.world Collector v2.32 This release adds in collector support for Vertica db.\nthe data.world Collector v2.31 Issued fix to ensure alignment of identifiers for databases referenced by Tableau and Looker collectors.\nthe data.world Collector v2.30 Installed a config file-driven configuration (as a hidden feature for now). Issued a fix for handling empty powerbi objects returned by the API\nthe data.world Collector v2.29 The data.world catalog collector now supports Tableau Online! Additionally there was a bugfix for PowerBi.\nthe data.world Collector v2.28 Bugfix release\nthe data.world Collector v 2.27 Added the optional CLI option tableau-graphql-page-size to the Tableau collector which allows the user to set a number of objects to be included in each page of paginated queries.\nthe data.world Collector v2.26 Updated the PowerBi collector so that if a report is unavailable via the API it will be logged, and cataloging will continue on the rest of the repository.\nthe data.world Collector v2.25 This release includes better and more user-friendly error handling and reporting. We have also added an enhanced collection of Tableau metadata via the Tableau Metadata API (graphql endpoint). New metadata includes data sources, databases, fields, metrics, and many more inter-object relationships.\nthe data.world Collector v2.24 the data.world Collector is now distributed via Dockerhub Additionally there are changes to the Tableau and PowerBI collectors, and the ability to change the level of error messages written to the console and log file, and a new subcommand to display the the data.world Collector license text.\nFor Tableau:\nThe Tableau collector now emits RDF in which the object of dct:crea\u2192r is a dwec:A\u2265nt instead of a string literal. This means we write additional details about the Tableau account that created the dashboard, via properties of the dwec:A\u2265nt resource. These details include: account name, account \u201cfull name\u201d, and account email address (if they are populated in Tableau).\nFor PowerBI:\nThe PowerBI collector writes resources representing powerbi \u201cdata sources\u201d that are now of a PowerBI-specific class, rather than dwec:DataArtifact.\nLogging changes:\nIt is now possible for users to set the level (severity) of log messages written to the console and log file. By default, we write \u201cinfo\u201d level messages; users can choose to write only errors (level=\u201cERROR\u201d), errors+warnings (level=\u201cWARN\u201d), or all messages including debug trace (level=\u201cDEBUG\u201d). This is useful if we want to have customers run the data.world Collector with debug logging turned on, for troubleshooting problems etc.\nDisplay the data.world Collector license information:\nLicense information for the data.world Collector is now available as a subcommand of the data.world Collector. To get all licensing information, run the command docker run -it --rm datadotworld/dwcc:X.XX\u00a0display-license where X.XX is a version of the data.world Collector greater than or equal to 2.24.\nthe data.world Collector v2.23 Internal release\nthe data.world Collector v2.22 Internal release\nthe data.world Collector v2.21 fixed some timeout issues with Looker collector when fetching images from the Looker API. Fixed an issue with cataloging reports and dashboards based on user workspace permissions in PowerBi.\nthe data.world Collector v2.20 With this release our Tableau collector now supports cataloging of workbooks and non-dashboard views as well as harvesting tags on workbooks and views. FIxed an issue in the Looker collector where preview images returned from looker api were missing.\nthe data.world Collector v2.19 Includes a clean-up of the embedded help commands for several collectors and:\nFixes an issue with the Tableau Server collector when cataloging multi-site server instances.\nAdds\u00a0--tableau-site\u00a0parameter to enable user to restrict cataloging to a single site (not required, by default all sites in the instance are scanned). Value provided to\u00a0--tableau-site\u00a0can be a site ID or name.\nthe data.world Collector v2.18 The tableau collector now has a flag option\u00a0--tableau-skip-images\u00a0which skips the harvesting of preview images for views. Usage is like this:\n...\u00a0catalog-tableau\u00a0--tableau-api-base-url=http://ec2-44-192-86-11.compute-1.amazonaws.com/api/3.10/ --tableau-username=admin --tableau-password=password -a\u00a0sc-test3\u00a0-n\u00a0tableau-test\u00a0--tableau-skip-images\nthe data.world Collector v2.17 Adds a collector for Presto\nthe data.world Collector v2.16 This release:\nAdds the parameter --all-databases to the Athena collector so that it can catalog all the databases accessible from the logged-in account.\nFixes some issues with datatypes for dwec:externalUrl\u00a0predicates.\nthe data.world Collector v2.15 This release contains the following:\nThe Tableau collector formerly had a CLI parameter\u00a0--tableau-project-id\u00a0which could be used to catalog only assets in the project with the specified ID. The parameter is now\u00a0--tableau-project\u00a0and takes either a project ID or project name\nUpdate to the MANTA collector to accommodate a minor change in the MANTA API with v 1.31. Customers who have updated their MANTA instance to v 1.31+ will want to use the data.world Collector 2.15+.\nThe Looker collector now works for non-admin Looker users; however, when the data.world Collector is run by a non-admin, the emitted catalog will not contain any information about databases used by Looker analysis assets (access to database information in Looker requires admin permissions).\nAll JDBC collectors now populate two new properties for\u00a0dwec:DatabaseColumn:\u00a0dwec:columnDefaultValue\u00a0 and\u00a0dwec:columnIsNullable, which contain the default value for that column in newly inserted rows, and whether the column can be null, respectively. (Note that only some databases/drivers provide this metadata\u2026we put it in the catalog if it\u2019s there).\nthe data.world Collector v2.14 Adds a collector for Looker. Minor update to the\u00a0docker-save.sh\u00a0script that includes available versions in the error message if you don\u2019t supply a version.\nthe data.world Collector v2.13 Adds cli params with this version so it now possible to pass arbitrary driver properties through to the connection\nthe data.world Collector v2.12 Adds collector for SAP (formerly Sybase) SQL Anywhere metadata collector\nthe data.world Collector v2.11 Improves the Dremio collector\u2019s handling of data sources nested within multiple layers of folders, and fixed a minor issue with the Dremio collector\u2019s harvesting of lineage metadata from the Dremio graph API.\nthe data.world Collector v2.10 Adds a collector for Domo and JDBC database collectors can now catalog all schemas in the database at once (default remains to catalog only user's default schema).\nthe data.world Collector v2.9 Adds Tableau Server collector and extended the OpenAPI collector to include a few additional schema property metadata properties.\nthe data.world Collector v2.8 Adds Infor ION data lake collector. Optimized collection of JDBC metadata (performance improvement).\nthe data.world Collector v2.7 Adds a collector for PowerBI.\nthe data.world Collector v2.6 Adds the Manta collector.\nthe data.world Collector v2.5 Upgrads Java runtime.\nthe data.world Collector v2.4 Extends handling of OpenAPI collector parameters and responses.\nthe data.world Collector v2.3 Adds support for OpenAPI (fka Swagger) collector.\nthe data.world Collector v2.2 A refactoring release.\nthe data.world Collector v2.1 Fixes an issue with the Denodo cataloger jdbc url port.\nthe data.world Collector v2.0 We now use v2 URIs as the official locator IDs for metadata resources. This is a breaking change (for structural, intentional reasons) which is not backwards compatible with v1 URIs. For more information see the article on the data.world Collector v2.X.\nthe data.world Collector v 1.20 Addresses some memory issues and open-cursor leaks.\nthe data.world Collector v.1.19 Adds writing statements to the catalog graph indicating that the catalog was the data.world Collector by the data.world Collector (with a version). We also added the ability to write database schema objects to the catalog graph.\nthe data.world Collector v1.18, Allows you to specify alternate organization permissions and upload locations when performing an automatic upload of the metadata.\nthe data.world Collector v.1.16 and the data.world Collector v.1.17 Address issues with the SQL Server cataloger.\nthe data.world Collector v.1.15 Adds Dremio support with optional Catalog API lineage fetching.\nthe data.world Collector v1.14, Enables you to change the amount of memory that gets allocated to a the data.world Collector docker process. See our article on allocating additional memory to Docker for more information.\nthe data.world Collector v.1.13 Adds support for Microsoft SQL Server, and we enable JVM to use available memory in the container (useful for creating large catalogs). Additionnally we Improve data type recognition in AWS Glue cataloger.\nAs of the data.world Collector v1.12 we can support not only Glue ETL jobs, but also Glue Data Catalog tables and columns.\nWith the data.world Collector v.1.11 you can:\nUpload generated catalogs via the --upload / -U command-line parameters\nUpload the the data.world Collector log when uploading generated catalogs with --upload\nFetch an organization's current catalog with the\u00a0fetch-catalog\u00a0command\nIn the data.world Collector v1.10 we added support for AWS Glue and AWS Athena including cataloging ETL jobs associated with an AWS account. There is no need to mount in a jdbc drivers directory as the Glue cataloger uses the Glue API, not JDBC.\ndwc v.1.9 is a bug cleanup release.\nIt is now possible with the data.world Collector.1.8 to use jdbc drivers on classpath as well as those found in user-specified JDBC Driver Directory (drivers in directory have higher precendence than classpath drivers).\nthe data.world Collector v.1.7 is a bug-fix release\nthe data.world Collector v.1.6 adds the support for arbitrary jdbc data sources and the ability to build one-off docker images for testing, demos, etc.,\nWith the data.world Collector v.1.5 we add support for Oracle.\nIn the data.world Collector.1.4 we add support for Google BigQuery.\nthe data.world Collector v.1.3 brings much new functionality including:\nSupport for Denodo and Snowflake\nCompatibility of JDBC catalogs with tables imported through data.world integrations\nAbility to differentiate source information for databases cataloged from localhost\nCataloging of\u00a0REMARKS\u00a0fields into\u00a0dct:descriptio\nWith the data.world Collector v.1.2 we support Redshift databases.\nthe data.world Collector v.1.1 contains documentation clarification and expansion for the documents to streamline tags on customer docker hosts.\nThe initial release of the data.world Collector v.1.0 provides support for metadata catalog extraction for DB2, Hive, MySQL, Postgres.",
    "url": "https://docs.data.world/en/98670-catalog-collector-change-log.html"
  },
  {
    "title": "Viewing metadata collectors summary page",
    "content": "The metadata collector summary page shows a list of all the collector runs feeding data into an organization's Catalog. Use this page to see the sources powering your catalog as well as the status of the most recent run.\nThis page lists the collectors run using the Connection Manager and the collectors that are run on-premise.\nCollector runs appear on this page when the metadata harvested by the collector is successfully uploaded to the ddw-catalogs dataset.\nThe page displays all collector runs for collector version 2.60 and higher.\nTo view the metadata collectors summary page:\nOn the Organization profile page, go the Settings page.\nGo to the Metadata collectors section. The following information is available for each run of the collector:\nData source What the collector was run against.\nTarget: The specific target from which metadata is collected. For example, name of DB/Schema for database sources or Name of project/workbook etc. for non database sources.\nRun location:  The House icon indicates that the collector is run on-premise. The Cloud icon indicates the collection is run using the Connection Manager.\nDestination: Collection where collected metadata resources appear. Click the collection name to navigate to the collection page. If you do not have access to the collection, you may see a 404 error page.\nStatus:  Hover over the status to see exact completed time, run time, and number of resources collected.\nComplete (green color): collector run successful, without exceptions.\nComplete (yellow color): collector run successful, with exceptions. Click View messages to see the list of errors. Exceptions are not uncommon and likely just mean that the collector has an issue accessing a particular table.\nFailed: Collector run unsuccessful. Click View messages to see the list of errors.\nClick the Help me set up a collector button to launch the Collector wizard that helps you generate the YAML file or CLI command for running the collectors. This wizard is available for a subset of collectors for now. You can find the Details for each collector are available on their respective collector pages.",
    "url": "https://docs.data.world/en/130553-viewing-metadata-collectors-summary-page.html"
  },
  {
    "title": "Creating and managing service accounts",
    "content": "This section walks you through the process of creating a service account for running the metadata collectors.\nTo create a service account:\nOn the Organization profile page, go to the Settings tab.\nFrom the Service accounts section, click the Add service account button.\nIn the Create a new service account window, set the following and click Create.\nService account name: This is the name with which the account gets created.\nToken expiration: Set an expiration timef rame. Options available are 1 year, 1 month, 1 week, and Never. The generated token expires on the set time and will need to be regenerated again.\nOnce the service account is created, you are prompted to note down the Service account token.\nYou must note down this token and the expiration date before closing the window as this token cannot be retrieved after the window is closed.\nThis is the account you will use to upload the collector output to the dataset.\nTo manage the service account:\nGo to the organization for which you created the service account.\nIn the Members tab, you will see the service account. Give this account permissions to the datasets in which the collector output will be saved. At a minimum the account must have Edit access to the dataset.\nCaution should be used while refreshing the service accounts. Refreshing the token invalidates any usages of the existing token attached to this service account.\nTo refresh an access token:\nOn the Organization profile page, go to the Settings tab.\nFrom the Service accounts section, locate the account you want to refresh.\nOpen the Three dot menu and click the Refresh button.\nCaution should be used while deleting service accounts. Deleting this service account will cause collectors or scripts using it to lose access.\nTo delete a service account:\nOn the Organization profile page, go to the Settings tab.\nFrom the Service accounts section, locate the account you want to delete.\nOpen the Three dot menu and click the Delete button.",
    "url": "https://docs.data.world/en/130574-creating-and-managing-service-accounts.html"
  },
  {
    "title": "data.world Collector instructions by data source",
    "content": "Here is a list of the current sources supported by the data.world metadata cataloger collectors. The links go to the configuration pages for each source where you can find detailed instructions for using them with Docker:",
    "url": "https://docs.data.world/en/98671-data-world-collector-instructions-by-data-source.html"
  },
  {
    "title": "Amazon S3 and the data.world Collector",
    "content": "\nThe latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to directly harvest metadata on S3 buckets and objects metadata from your Amazon S3 instance. Note that if you are looking to harvest tables and columns from Amazon S3 objects that have been cataloged in AWS Glue Data Catalog, you must instead use the AWS Glue collector.\nThe S3 collector authenticates to S3 using the default credential profiles file. The collector needs a user created in the AWS portal with read access to S3.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nBuckets\nAmazon resource number (ARN)\nRegion\nName\nVersion state\nCreation date\nACL owner ID\nACL grantee ID\nACL grant permission\nObjects\nKey\nAmazon resource number (ARN)\nRegion\nSize\nLast modified date\nACL owner ID\nACL grantee ID\nACL grant permission\nMetadata\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nS3 Bucket\nRelationship to S3 Object\nS3 Object\nRelationship to S3 Bucket\n\nFor an optimized experience, the system has set limits for harvesting metadata from the buckets in S3.\nBy default, the collector has a limit of harvesting 10,000 objects per bucket. If the contents of a bucket cross this limit, the bucket is skipped, no metadata is harvested for it, and a warning message is logged for the bucket.\nIf you want to overwrite the default limit, set the --max-resources parameter in your collector command. The maximum value for this parameter can be 10,000,000 (ten million). If the total contents (total buckets and objects) cross this limit, further buckets are not cataloged, and a warning message is logged for the bucket.\nThis section will walk you through the process of setting up an account with S3 read-access policy and setting up a credentials profile file.\nSkip this step if you already have an user that you want to run the collector with and the user has ReadOnlyAccess access to Amazon S3. Detailed AWS documentation on this topic is available here.\nLogin to the AWS portal and navigate to IAM service. Under Users, click Add users to add an user. You can also select an existing user.\nOn the next screen:\nIn the Permissions option, select Add permissions (attach policies directly).\nIn, Permissions policies section, select AmazonS3ReadOnlyAccess.\nClick Next.\nOn the last screen click the Add permissions or Create user.\nSkip this step if you already have the access key for the user that you plan to use for running the collector. Detailed AWS documentation on this topic is available here.\nLogin to the AWS portal and navigate to IAM service.\n\nUnder Users, select the user that plan to use for the collector.\nOn the Security credentials tab, click Create access key.\nSelect Application running outside AWS. Click Next.\nAdd the optional Description tag. Click Create Access key.\nNote down the Access key ID and Secret access key. You will need this information for setting up the credentials file.\nSkip this step if you already have the AWS CLI installed and credentials profiles file set up.\nInstall the AWS CLI.\nFrom the command line, run aws configure. This stores the credentials to ~/.aws/credentials.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup screen, select\u00a0Amazon S3. Click\u00a0Next.\nOn the Configure a new on premises Amazon S3 Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDetails\nRequired?\nAWS Credential Source File Path\nsource=${HOME}/.aws/credentials\nProvide the location of the credentials file you generated for authentication. For example, ${HOME}/.aws/credentials\nYes\nAWS Region\n--awsRegion=<awsRegion>\nThe AWS region used to initialize the S3 client.\nYes\nMax Resources\n---max-resources=<maxResources>\nThe maximum resources the collector should harvest. More details on when this parameter should be set are available here.\nNo\nOn the\u00a0Finalize your Amazon S3 Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate Configuration file ( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you generated a YAML file, click\u00a0Next.\nThe\u00a0Amazon S3 Collector command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The account used to authenticate to Amazon S3 does not have permissions to read buckets or objects.\nSolution: Follow the instructions to set the user and permissions for the collector.\nCause: The access key for the AWS account is expired or is incorrect.\nSolution: Delete the ~/.aws/credentials file and re-run the steps to obtain the access key and setting up the credentials file.\nCause: This issue generally happens when the bucket has resources more than 10,000 or what is set in the --max-resources parameter.\nSolution: Check if the --max-resources parameter is set and if so, what value is configured for it.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/125114-amazon-s3-and-the-data-world-collector.html"
  },
  {
    "title": "Apache Spark and the data.world Collector",
    "content": "Metadata for an Apache Spark data source can be cataloged using the Hive metadata collector for data.world. To use it, create a Hive metadata collector connection (full instructions are here), and then point the collector at Spark.\nThis integration only works for Apache Spark. If you are interested in cataloging Databricks Spark or another Spark variation, please contact us.",
    "url": "https://docs.data.world/en/98672-apache-spark-and-the-data-world-collector.html"
  },
  {
    "title": "Athena and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Oracle tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nOften the tables in an Athena database will participate in Glue ETL jobs. The data.world Collector has the ability to catalog lineage information in Glue ETL jobs. For more information see the article on AWS Glue and the data.world Collector.\nThe Athena collector supports basic authentication to Athena.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nAn AWS credentials file for authentication which contains the user profile to determine which AWS account's instance to catalog. Typically the AWS_CREDENTIALS_FILE is at [user\u2019s home directory]/.aws/credentials. See the AWS documentation on configuration and credential file settings for information on setting up this file.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Athena. Click Next.\nOn the Configure a new on premises Athena Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nParameter name\nDescription\nRequired?\nAWS Credential Source File Path\nsource= ${HOME}/.aws/credentials\nWhen you set up the CLI command to run the collector, mount the path of the local directory containing the AWS credentials profiles file to the /root/.aws/credentials directory on the Docker container by specifying the source path to your existing AWS credentials.\nYes\nAWS Region\n--aws-region=<awsRegion>\nThe AWS Region where the catalog lives.\nYes\nDatabase\n-d, --database= <AthenaDatabase>\nThe name of the Athena database within the Glue Data Catalog.\nNo\nOn the Finalize your Athena Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate Configuration file ( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you generated a YAML file, click Next.\nThe Athena Collector command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98673-athena-and-the-data-world-collector.html"
  },
  {
    "title": "AWS Glue and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest tables, databases, columns, and jobs from AWS Glue.\nThe collector uses AWS SDK for Java 1.11. This is associated with AWS API version 2020-04-08. For more details, see this documentation.\nThe user running the collector must have permissions to GetDatabases, GetTables, ListJobs, and GetJobcreds on AWS. For more information see the Amazon API reference docs.\nThe collector catalogs the following information.\nObject\nInformation collected\nDatabase\nName\nTable\nName\nColumn\nName, column index, column type, column size, JDBC type\nJob\nName\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nDatabase\nTable that is in the database\nTable\nDatabase that contains the table\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nObject\nLineage available\nTable\nTable that a Job sources its data from\nTable that a Job writes its data to\nJob\nTable that is the source or destination of a job\nAn AWS credentials file for authentication which contains the user profile to determine which AWS account's instance to catalog. Typically the AWS_CREDENTIALS_FILE is at [user\u2019s home directory]/.aws/credentials. See the\u00a0AWS documentation on configuration and credential file settings\u00a0for information on setting up this file.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select AWS Glue. Click Next.\nOn the Configure a new on premises AWS Glue Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nParameter name\nDescription\nRequired?\nAWS Credential Source File Path\nsource=${HOME}/.aws/credentials\nWhen you set up the CLI command to run the collector, mount the path of the local directory containing the AWS credentials profiles file to the /root/.aws/credentials directory on the Docker container by specifying the source path to your existing AWS credentials.\nYes\nAWS Region\n--aws-region=<awsRegion>\nThe AWS Region where Catalog lives.\nYes\nOn the next screen, set the following advanced options and click Next.\nField name\nParameter name\nDescription\nRequired?\nSpecify if no Databases should be cataloged\n--no-databases\nSwitch to specify that no databases in the Glue Data Catalog are to be cataloged\nNo\nDatabase(s) to Catalog\n--database-name=<databaseNameRegex>\nName (or a regular expression matching multiple names) of the database(s) in the Glue Data Catalog to catalog\u00a0(by default, all are cataloged)\nNo\nSpecify if no Jobs should be cataloged\n--no-jobs\nSwitch to specify that no Glue jobs are to be cataloged\nNo\nName of Job(s)\n--job-name=<jobNameRegex>\nName (or a regular expression matching multiple names) of the Glue job(s) to catalog\u00a0(by default, all are\u00a0 cataloged)\nNo\nOn the\u00a0Finalize your AWS Glue Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML)\u00a0or\u00a0Command line arguments (CLI). Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generated a YAML file, click\u00a0Next.\nThe AWS Glue Collector command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98674-aws-glue-and-the-data-world-collector.html"
  },
  {
    "title": "Azure Data Lake Storage Gen2 and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to directly harvest metadata on Azure Data Lake Storage Gen2 storage accounts, containers, and files from your Azure Data Lake Storage Gen 2 instance.\nAuthenticate to Azure Data Lake Storage Gen 2 using Service principal.\nThe collector catalogs the following information from Azure Data Lake Storage Gen 2.\nObject\nInformation collected\nStorage Account\nName, Description, Last Modified, Resource Group name, Region Name, Creation Time, Subscription ID, Account Status, Account Kind, Access Control, Access Tier, Provisioning State, Tags\nContainer\nName, Description, Server, Last Modified, Metadata, Subscription ID, Entity Tag, Public Access, Access Control\nBlob\nName, Description, File URL, File Path, Blob Type, Content Length, Creation Time, Last Modified, Metadata, Subscription ID, Entity Tag, Access Control\nBy default, the data.world catalog will include catalog pages for the resource types below. Each catalog page will have a relationship to other related resource types. Note that the catalog presentation and relationships are fully configurable, so these will list the default configuration.\nResource page\nRelationship\nStorage Account\nRelationship to Containers contained within Storage Account\nContainer\nRelationship to Blobs contained within Container\nRelationship to Storage Account containing Container\nBlob\nRelationship to Container containing Blob\nBy default the collector harvests metadata from Azure Data Lake Storage Gen 2 with up to 10,000 objects in each Storage Account. If your Azure Data Lake Storage Gen 2 has more than 10,000 objects in a given Storage Account, you must set the --max-resource-limit parameter to what you want. The max value can be set to 10 million. If the contents of a Storage Account cross this maximum limit, the Storage Account is skipped and a warning message is logged for the Storage Account.\nThe Azure Data Lake Storage Gen 2 collector authenticates using Azure Service Principal.\nTo register a new application:\nGo to the Azure Portal.\nSelect Azure Active Directory.\nClick the App Registrations option in the left sidebar.\nClick New Registration and enter the following information:\nApplication Name: DataDotWorldADLSGen2Application.\nSupported account types: Accounts in this organizational directory only.\nClick Register to complete the registration.\nTo create a Client Secret:\nOn the new application page you created, select Certificates and Secrets.\nUnder the Client secrets tab, click the New client secret button.\nAdd a Description.\nSet the expiration for the client secret.\nClick Add, and copy the secret value.\nTo get the Client ID from the Azure portal:\nClick on the Overview tab in the left sidebar of the application home page.\nCopy the Application (Client) ID from the Essentials section.\nFrom the page of new application you created from step 1, copy and save the Directory (tenant) ID. You will use this for the --tenant-id parameter.\nNavigate to a storage account that you would like to harvest from. From the Overview page, copy the Subscription ID. You will use this for the --subscription-id parameter.\nEnable access to the detailed data source information (like tables and columns) provided by Power BI through the read-only admin APIs. For details about doing this task, please see this documentation.\nPerform these tasks for each Storage Account you plan to harvest.\nGo to Storage Account. Click on Access Control (IAM).\nClick Add > Add role assignment.\nIn the Role tab, select Job function role as Storage Blob Data Reader.\nClick Members tab. Click Select Members.\nFind and click the Service Principal you created earlier. Click Select.\nClick Review + assign.\nRepeat Steps 1-6 and add\u00a0the Reader role.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Azure Data Lake Storage Gen 2. Click Next\nOn the Configure a new on premises Azure Data Lake Storage Gen 2 Collector Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nAzure Active Directory Client ID\n--client-id=<clientId>\nThe Active Directory client ID used to initialize the azure client.\nYes\nAzure Active Directory Client Secret\n--client-secret=<clientSecret>\nThe Active Directory client secret used to initialize the azure client.\nYes\nAzure Subscription ID\n--subscription-id=<subscriptionId>\nThe subscription ID for the Azure account. It is required for fetching the list of storage account.\nYes\nAzure Active Directory Tenant ID\n--tenant-id=<tenantId>\nThe Active Directory tenant id is used to initialize the Azure client. To find the tenant ID, navigate to the Azure Active Directory resource. You can find the Tenant ID listed within the Overview page.\nYes\nOn the next screen, set the following advanced properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nAzure Storage Account Names\n--storage-account-name=<storageAccountName>\nThe Azure storage account name used to initialize the azure client. It can be declared multiple times.\nNo\nMaximum Resource limit\n--max-resource-limit=<maxResourceLimit>\nThe maximum resources the collector will harvest, The maximum limit can be up to 10 million. If not specified, by default the collector harvests a maximum of 10,000 resources.\nNo\nOn the\u00a0Finalize your Azure Data Lake Storage Gen 2 Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML) or\u00a0Command line arguments (CLI). Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Azure Data Lake Storage Gen 2 command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of the collector downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of the collector. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: This generally happens when the bucket has more than 10,000 resources or what is set in the --max-resource-limit parameter.\nSolution: Check if the --max-resource-limit parameter is set and if so, what value is configured for the parameter.\nCause: The collector did not have permissions to read from a storage account.\nSolution: Ensure that the Service Principal has Storage Blob Data Reader role for each of the Storage Accounts you want to harvest.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/146950-azure-data-lake-storage-gen2-and-the-data-world-collector.html"
  },
  {
    "title": "BigQuery and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for BigQuery datasets, projects, tables, and columns across the enterprise systems and make it searchable and discoverable in data.world. The collector also harvest column-level lineage relationships between tables and views.\nThe collector authenticates to BigQuery using a Service Account associated with the project.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nDatasets\nID, name, description, labels (note these are key/value pairs), created date, last modified date, default table expiry, default partition expiry, data location\nProjects\nName\nTables\nName, Description, Created date, Last modified date, Default table expiration, Data location, Labels, Type (Standard, External, Snapshot, Model), Partitioned on field, Clustered by columns for standard and snapshot tables, Partition type (range or time) requires partition filter - Range (Start, end, interval) Time (Partition type (hour, day, month, year), expiration)\nColumns\nName, Description, Data Type, Is Nullable, Column size\nView\nName, description, created date, default table expiration, last modified date, data location, default collation, labels, view SQL, clustered by columns for materialized\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nDatasets\nTables, Views\nProjects\nDataset\nTables\nColumn, Labels\nColumns\nTable, View\nViews\nColumn\nLabel Value\nTable, View, Project, Dataset\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the BigQuery collector.\nObject\nLineage available\nView Column\nThe collector identifies the associated column in an upstream view or table:\nWhere the data is sourced from\nThat sort the rows via ORDER BY\nThat filter the rows via WHERE/HAVING\nThat aggregate the rows via GROUP BY\ndata.world connects to BigQuery using a\u00a0Service Account\u00a0associated with your project.\nTo set up authentication for BigQuery:\nCreate a service account with the following roles\u00a0BigQuery Data Viewer\u00a0and\u00a0BigQuery User. For additional information on\u00a0predefined roles and permissions, see Google Cloud Platform documentation.\nAfter you create a service account, create a\u00a0key\u00a0for the account and download the associated\u00a0JSON\u00a0key file.\nPlace this key file on the machine from where you plan to run the collector. You will need this file while running the collector.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML\u00a0file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0BigQuery. Click\u00a0Next.\nOn the Configure a new on premises BigQuery Collector screen, set the following properties and click Next.\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDetails\nRequired?\nBig query credential file path\n--credentialFile=/creds/creds.json\nGCP service account credential file. Must match the target specified in the --mount command for credentials. For example,\ncredentialFile=/creds/creds.json\nYes\nContainer Directory\n--mount\u00a0type=bind,source=/local_creds_dir,target=/creds\nProvide the location of the credentials file\u00a0you generated\u00a0for authentication.\nYes\nBigQuery Project\n-p=<project>\n--project=<project>\nThe BigQuery project.\nYes\nThe BigQuery dataset(s) to catalog in the given project\n-d=<dataset>\n--dataset=<dataset>\nThe BigQuery datasets to catalog in the given project. By default all datasets in a project are cataloged.\nNo\nOn the\u00a0Finalize your BigQuery Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate Configuration file ( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you generated a YAML file, click\u00a0Next\nThe\u00a0BigQuery Collector command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The credential file was not found on the Docker container.\nSolution: Check that the path for the credential file is properly mounted. You will need to mount a local directory containing the credential file with a directory on the Docker container. The --credentialFile should be the path to the directory on the container with the credential file.\nCause: An exception occurs while reading the credentials file.\nSolution: Check that the credentials file contains the correct credential information required for running the collector.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98675-bigquery-and-the-data-world-collector.html"
  },
  {
    "title": "Databricks and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Databricks tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, , Column size, Column index\nTable\nName, Description, Schema, Primary key, Foreign key, Owner, Type, Creation, Last Modified, Location, Provider, Version, Size, File Count, Partition Columns, Properties\nViews\nName, Description, SQL definition\nSchema\nName\nDatabase\nType, Name, Server, Port, Environment, JDBC URL\nNotebook\nNotebook ID, Path, Language Type (SQL, Python, Scala, R)\nFunction\nName, Description, Function Type\nJob\nTitle, Description, Creator, Created At, Job run as, Format, Max Concurrent Runs, Notification On Start, Timeouts (sec), Notification On Success, Schedule, Git Source, Notification on Failure, Job Tags, List of tasks, List of clusters\nCluster\nName, Description, Node Type ID, Driver Node Type ID, Spark Version, Number of Workers, Autoscale Max Workers, Autoscale Min Workers, AWS Attributes, Cluster Tags\nTask\nTask Key, Type of Task (Notebook, dbt, Spark jar, Python script, Python wheel, Pipeline task, SQL), Task timeout, Retry interval, Cluster used by the task, Max retries, Depends on, Libraries, Notifications (On start, On success, On failure), Notebook File Path,\u00a0Notebook Source,\u00a0\u00a0Notebook Parameters, Spark Jar Main Class Name, Spark Jar Parameters,\u00a0Python Script File path,\u00a0Python Script Parameters, Spark Submit Parameters, Pipeline ID, Pipeline Full Refresh, Python Wheel Package Name,\u00a0Python Wheel Entry Point, Python Wheel Parameters, SQL Warehouse,\u00a0SQL Query ID, SQL Dashboard ID, SQL Alert ID, Dbt Project Directory, Dbt Profiles Directory, Dbt warehouse, Dbt catalog, Dbt schema, Dbt commands\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this\u00a0data\u00a0source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nRelationship page\nRelationship\nTable\nColumns contained in Table\nColumns\nTable containing Column\nJob\nClusters used by tasks in Job\nTasks contained within Job\nCluster\nCluster Tag referenced by Cluster\nCluster contained in job\nTask using Cluster\nTask\nJob containing Task\nCluster used by Task\nTasks depending on Task\nNotebook\nFolder containing Notebook\nTask sourcing data from Notebook\nFolder\nFolders contained in Folder\nNotebooks contained in Folder\nJob Tag\nJobs containing Job Tag\nCluster Tag\nClusters containing Cluster Tag\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the Databricks collector.\nObject\nLineage available\nColumn in view\nThe collector identifies the associated column in an upstream view or table for both Hive metastore and Unity Catalog:\nWhere the data is sourced from\nThat sort the rows via ORDER BY\nThat filter the rows via WHERE/HAVING\nThat aggregate the rows via GROUP BY\nNotebook\nTasks that reference Notebook. (Only if Databricks Unity Catalog is enabled).\nTable\nThe collector identifies the upstream and downstream tables along with the intermediate Job. (Only if Databricks Unity Catalog is enabled).\nTo generate a personal access token:\nThe user should have Can Use or Can Manage permission in Databricks workspace to generate PAT.\nIn the Databricks workspace, click on username in the top bar.\nSelect User Settings from the drop down, Go to Access tokens tab. Click on Generate New Token button.\nEnter a comment that helps you to identify the token and change the token\u2019s lifetime as required.\nTo create a token with no lifetime, leave the lifetime box empty. Click Generate.\nCopy the displayed token and click Done. Save this token for future use.\nAlternatively, you can use the token API to generate PAT.\nIn Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.\nYou will need to grant the user that runs the collector under appropriate permissions to harvest resources from Unity Catalog.\nThe user should have minimum USE CATALOG, USE SCHEMA, and SELECT permissions on the catalog to access the data. See the Dataricks documentation for detailed about these permissions.\nTo grant the permissions:\nClick on the catalog on which you want to grant permission to the user.\nSelect permissions tab and click the Grant button.\nSelect the user and the permissions.Click GRANT.\nThe user should also have Can Use permission on the existing cluster or SQL warehouse. Or, they should be able to create their own compute resources.\nTo grant Can Use permission on the compute resource:\nClick on the three dots at the extreme right end of the resource and select Permissions.\nAdd the user and select appropriate permission.\nFor the collector to harvest Jobs, you need to set up proper permissions across the Jobs that you want the collector to harvest. The user that runs the collector should have minimum Can View permissions. For details about setting the Job permissions, see the Databricks documentation.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Databricks. Click Next.\nOn the Configure a new on premises Databricks Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer\n-s =<server>\n--server=<server>\nSpecify the hostname of the database server.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nDatabase ID\n-D=<databaseId>\n---database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nUsername\n-u=<user>\n--user=<user>\nThe username to use to make the JDBC connection. (For authentication to work with a personal access token, the user flag (-u) must be set to\u00a0token.)\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database. Default value is set to ${DW_DATABRICKS_PASSWORD}.\nYes\nSchemas to collect\nSelect from one of the following options:\nCollect all schema, Specify which schemas to collect\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if --schema is not set)\nSpecify which schemas to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nIn Databricks, the schema is sometimes referred to as a Database. For more information, see the Databricks schema documentation.\nYes\n(if --all-schema is not set)\nInclude Information Schema\n--include-information-schema=<true/false>\nInclude the database's Information Schema in catalog collection.\nYes\nDatabricks API token\n--access-token=<token>\nDatabricks personal access token for API authentication (Go here for more information). Default value is set to ${DW_DATABRICKS_TOKEN}.\nYes\nDatabricks HttpPath\n--http-path=<httpPath>\nhttpPath parameter for Databricks compute resources URL (see Databricks documentation for details)\nYes\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDisable lineage collection\n--disable-lineage-collection\nSkip harvesting of intra-database lineage metadata\nNo\nEnable sample string values collection\n--sample-string-values\nEnable sampling and storage of sample values for string-valued columns\nNo\nEnable column statistics collection\n--enable-column-statistics\nTo enable harvesting of column statistics (i.e., data profiling).\nNo\nTarget sample size for column statistics\n--target-sample-size\nThe number of rows sampled for computation of column statistics and string-value histograms\nNo\nServer environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is 'localhost' use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments\nNo\nDatabase ID\n-D=<databseid>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example, property1=value1;property2=value2\nNo\nOn the Finalize your Databricks Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Databricks command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nYou can add the following parameter to the\u00a0Command\u00a0or\u00a0YAML file\u00a0to use this additional feature.\nParameter name\nDetails\nRequired\n--workflow-exclude\nSkip harvesting of Databricks workflows and their lineage metadata.\nNo\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The parameters all-schemas or schema is missing from the Command line or YAML file.\nSolution: Check your command or YAML file to make sure the all-schemas or schema parameter is setup properly.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98676-databricks-and-the-data-world-collector.html"
  },
  {
    "title": "Datakin and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nWe develop using admin credentials.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML\u00a0file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux\u00a0or\u00a0MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Datakin. Click\u00a0Next.\nOn the\u00a0Configure a new on premises Datakin Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDatakin API base URL\n--datakin-api-base-url=<baseUrl>\nBase URL of the Datakin API.\nYes\nDatakin API key\n--datakin-api-key=<apiKey>\nThe token for authentication to the Datakin API.\nNo\nOn the\u00a0Finalize your Datakin Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI). Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Datakin command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Dspaocker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98677-datakin-and-the-data-world-collector.html"
  },
  {
    "title": "DB2 and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for DB2 tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to DB2.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, foreign), column size, column index\nTable\nName, description, primary key, schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, name, identifier, server, port, environment, JDBC URL\nFunction\nName, Description, Function Type\nStored Procedure\nName, Description, Stored Procedure Type\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nThe computer must have the appropriate JDBC driver on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nMake sure you download the appropriate JDBC driver for DB2 on the machine from where you will run the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Db2. Click Next.\nOn the Db2 Collector requires an additional driver file screen, set the JDBC driver directory. Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\nsource=${HOME}/dwcc-drivers\nThe driver required to run the collector. You must download this driver yourself and place it in a directory on the machine that will run the collector.\nYes\nOn the Configure a new on premises Db2 Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s= <server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p= <port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d= <database>\n--database= <database>\nThe name of the database to connect to.\nYes\nUsername\n-u= <user>\n--user= <user>\nThe username to use to connect to the database.\nYes\nPassword\n-P= <password>\n--password= <password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A  --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S= <databaseSchema>\n--schema= <databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e= <environment>\n--environment= <environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D= <databseid>\ndatabase-id= <databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC properties\n--jdbc-property= <driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value pair.\nNo\nOn the Finalize your DB2 Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Db2 command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nYou can add the following parameters to the Command or YAML file to use these additional features.\nParameter name\nDescription\nRequired?\n--enable-column-statistics\nEnable harvesting of column statistics. This is optional, and may greatly increase collector run time.\nNo\n--sample-string-values\nEnable sampling and storage of sample values for string-valued columns\nNo\n--target-sample-size=<targetSampleSize>\nTarget for number of rows to sample from tables\nNo\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98678-db2-and-the-data-world-collector.html"
  },
  {
    "title": "dbt Core and the data.world Collector",
    "content": "\nThe latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe dbt collector processes artifacts from your dbt Core project to harvest dbt assets and lineage relationships from dbt transformations.\nThe collector harvests metadata from dbt generated files.\nThe dbt Core collector will also identify how dbt moves data between tables (i.e., lineage). To accomplish this, the dbt collector needs to parse View SQL. Without specifying the target database information, no lineage relationships between columns specified through views can be harvested. The connection information is passed in via dbt\u2019s profiles.yml file or can be supplied with the data.world YAML file or CLI command.\nNote that the collector however does not harvest everything that the target database collector would harvest. For example, Snowflake can harvest profiling, tags, and policies that the dbt Core collector will not harvest. It is recommended to run both the dbt Core collector and the target database collector to build a comprehensive data catalog.\nThe collector supports the following dbt Core versions:\ndbt 1.0.5\ndbt 1.1.0\nThe collector supports the following authentication methods to the target databases:\nUsername and password authentication\nWhen authenticating to Snowflake, the collector also supports:\nUsername and key pair authentication.\nThe information cataloged by the collector includes metadata for the following dbt Core resources:\nObject\nInformation cataloged\nAnalysis\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nModel\nName, Description, Path, Root path, Package name,Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nProject\nName, Project version\nSnapshot\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nSeed\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nSource\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Source name, Resource type\nTest\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nModel\nProject containing dbt model\nTests testing the integrity of model, dbt resources (test, seed, model, snapshot, source) that are upstream of model\ndbt resources (test, seed, model, snapshot, source) that are downstream of model\nProject\nDbt resources (test, seed, model, snapshot, source) contained within project\nSnapshot\nProject containing dbt project\ndbt resources (test, seed, model, source) that are upstream of snapshot\ndbt resources (test, seed, model, source) that are downstream of snapshot\nSeed\nProject containing dbt project\ndbt resources (test, seed, model, snapshot, source) that are upstream of seed\ndbt resources (test, seed, model, snapshot, source) that are downstream of seed\nSource\nProject containing dbt project\ndbt resources (test, seed, model, snapshot) that are downstream of seed\ndatabase table that is the source of data for source\nTest\nProject containing dbt project\ndbt model that has its integrity tested by this test\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nObject\nLineage available\ndbt model materialized as view\nReferenced database tables and columns in dbt model materialized as view.\ndbt resource\ndbt resources that are upstream and downstream (for example, seeds that are upstream of models, and tests that are downstream of models) of dbt resource.\nThe collector also harvests column-level lineage for the following databases in the dbt collector:\nPostgreSQL\nRedshift\nSnowflake\nFor Eureka Explorer, these harvested lineage relationships display from the page of theupstreamordownstreamresource from dbt. For example, you can see and access Eureka Explorer from a downstream Snowflake table resource page to see what upstream Snowflake table was transformed as a result of a view associated with a dbt model. The dbt resource will also appear in Eureka Explorer.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nHarvesting metadata from dbt Core artifacts themselves requires that the artifact files be in a filesystem directory for which the user running the collector has at least read access. In order to harvest intra-database (column-level) lineage for dbt models materialized as views, the collector must be provided with a credential to dbt\u2019s target database that has SELECT privileges on those views and tables referenced by those views. This database credential can be supplied via CLI options or obtained from the profiles.yml file.\nprofiles.yml - It is located in the ~/.dbt directory by default. For more information see the dbt connection profiles documentation.\ndbt_project.yml - Is found at the top level of the dbt project.\ncatalog.json, manifest.json and run_results.json - These files can be generated by running the command dbt\u00a0docs generate. More information is available here and here.\nThe files catalog.json, manifest.json , and profiles.yml must be in the same directory on the host machine. For example, /artifact_directory.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select dbt Core. Click Next.\nOn the Configure a new on premises dbt Core Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndbt artifactory directory\n-d=<artifactDirectory>\n--artifact-directory=<artifactDirectory>\nThe directory containing the dbt catalog.json, manifest.json, profiles.yml, dbt_project.yml,\u00a0 and run_results.json.\nWithout run_results.json, the emitted catalog will not contain activity information, but will otherwise be complete.\nYes\ndbt profile file path\n-P=<profileFile>\n--profile-file=<profileFile>\nThe file containing profile definitions (defaults to dbt default of .dbt/profiles.yml\u00a0in the user's home directory)\nNo\ndbt profile\n-p=<profile>--profile=<profile>\nThe dbt profile to use to obtain database\u00a0location information (defaults to first profile found in profile definitions file)\nNo\ndbt target\n-g=<target>\n--target=<target>\nThe dbt profile target to use to obtain database location information (defaults to the\u00a0profile's 'target' value)\nNo\nOn the next screen, first select the Targetdatabase. Options available are: PostgreSQL, Snowflake, Bigquery. You would use these options to override the connection information from the dbt profile file or if the dbt profile file is not provided.\nFor the PostgreSQL database, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDatabase server\n--database-server=<databaseServer>\nThe server/host for the target database.\nNo\nServer port\n--database-port=<databasePort>\nThe port for the target database.\nNo\nDatabase\n--database=<database>\nThe name of the target database.\nNo\nUsername\n--database-user=<databaseUser>\nThe user credential to use in connecting to the\u00a0target database.\nNo\nPassword\n--database-password=<databasePassword>\nThe password credential to use in connecting to \u00a0the target database. Default value is environment variable ${DW_DBT_PASSWORD}.\nNo\nIf you selected the Target database as Snowflake, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nSnowflake Account\n--snowflake-account=<snowflakeAccount>\nThe Snowflake account/tenant. You can use\u00a0--database-server as an alternative.)\nNo\nDatabase server\n--database-server=<databaseserver>\nThe hostname of the database server to connect to.\nNo\nDatabse port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nAuthentication\nSelect from one of the following options:\nAuthenticate with a username & password\nAuthenticate using a private key file\nAuthenticate with a username & password\nUsername\n-u=<user>\n--user=<user>\nThe username used to connect to the database.\nNo\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database. We recommend that you use an environment variable\u00a0${DW_SNOWFLAKE_PASSWORD} for this parameter.\nNo\nAuthenticate using a private key file\nSnowflake Private Key File\n--snowflake-private-key-file=<snowflakePrivateKey>\nThe private key file to use for authentication with Snowflake (for example rsa_key.p8).\nNo\nSnowflake Private Key Password\n--snowflake-private-key-file-password=<snowflakePrivateKeyFilePassword>\nThe password for the private key file to use for authentication with Snowflake, if the key is encrypted and a password was set. Set it as an environment variable ${DW_SNOWFLAKE_PK_PASSWORD}.\nNo\nSnowflake Account\n--snowflake-account=<snowflakeAccount>\nThe Snowflake account/tenant.\nYou can use\u00a0--database-server\u00a0as an alternative.\nNo\nSnowflake Application\n--snowflake-application=<snowflakeApplication>\nThe application connection parameter to use in connecting to the target Snowflake database. Use datadotworld unless otherwise directed.\nNo\nSnowflake Role\n----snowflake-role=<snowflakeDatbaseRole>\nThe role to use in connecting to the target Snowflake database. This is case-insensitive.\nNo\nSnowflake Warehouse\n--snowflake-warehouse=<snowflakeDatbaseWarehouse>\nThe warehouse to use in connecting to the target Snowflake database. This is case-insensitive.\nNo\nIf you selected the Target database as BigQuery, set the following property and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nBigQuery account credentials file path\n--bigquery-credentials-file=<bigqueryCredentialsFile>\nThe file containing bigquery service account credentials. This applies only to models with\u00a0bigquery references.\nIf provided, the bigquery project id is read from this file, otherwise the bigquery project in the profile file is used.\nNo\nOn the Finalize your dbt Core Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe dbt Core command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command. The following sample command is generated using BigQuery as the target database. Your command will vary based on the target database you select while generating the command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nIf the command includes the --database_server parameter, make sure it is not followed by a trailing space (\\).\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/99390-dbt-and-the-data-world-collector.html"
  },
  {
    "title": "dbt cloud and the data.world Collector",
    "content": "\nThe latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe dbt cloud collector will connect to your dbt cloud project and harvest dbt assets and lineage relationships from dbt transformations.\nThe\u00a0dbt Cloud\u00a0collector will also identify how dbt moves data between tables (i.e., lineage). To accomplish this, the dbt collector needs to parse\u00a0View SQL.\u00a0Without specifying the target database information, no lineage relationships between columns specified through views can be harvested. The connection information can be supplied with the data.world YAML file or CLI command as an optional override.\nNote that the collector however does not harvest everything that the target database collector would harvest. For example, Snowflake can harvest profiling, tags, and policies that the dbt Cloud collector will not harvest. It is recommended to run both the dbt Coud collector and the target database collector to build a comprehensive data catalog.\nThe collector supports the following dbt versions:\ndbt 1.4\nThe collector supports authenticating to dbt cloud using API key.\nWhen the collector authenticates to Snowflake as the target database, the collector supports either:\nUsername and key pair authentication.\nUsername and password authentication\nThe information cataloged by the collector includes metadata for the following dbt resources:\nObject\nInformation cataloged\nAnalysis\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nModel\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nProject\nName, Project version\nSnapshot\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nSeed\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nSource\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Source name, Resource type\nTest\nName, Description, Path, Root path, Package name, Unique ID, Alias, Meta, Raw SQL, Compiled SQL, Enabled, Materialized, Resource type\nBy default, the data.world catalog will include catalog pages for the resource types below. Each catalog page will have a relationship to other related resource types. Note that the catalog presentation and relationships are fully configurable, so these will list the default configuration.\nResource page\nRelationship\nModel\nProject containing dbt model\nTests testing the integrity of model, dbt resources (test, seed, model, snapshot, source) that are upstream of model\ndbt resources (Test, Seed, Model, Snapshot, Source) that are downstream of model\nProject\nDbt resources (Test, Seed, Model, Snapshot, Source) contained within project\nSnapshot\nProject containing dbt project\ndbt resources (Test, Seed, Model, Source) that are upstream of snapshot\ndbt resources (Test, Seed, Model, Source) that are downstream of snapshot\nSeed\nProject containing dbt project\ndbt resources (Test, Seed, Model, Snapshot, Source) that are upstream of seed\ndbt resources (Test, Seed, Model, Snapshot, Source) that are downstream of seed\nSource\nProject containing dbt project\ndbt resources (Test, Seed, Model, Snapshot) that are downstream of seed\nDatabase table that is the source of data for source\nTest\nProject containing dbt project\ndbt model that has its integrity tested by this test\n\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nObject\nLineage available\ndbt model materialized as view\nReferenced database tables and columns in dbt model materialized as view\ndbt resource\ndbt resources that are upstream and downstream (for example, seeds that are upstream of models, and tests that are downstream of models) of dbt resource.\nWe also collect column-level lineage for the following databases in the dbt collector:\nSnowflake\nFor Eureka Explorer, these harvested lineage relationships display from the page of theupstream or downstream resource from dbt. For example, you can see and access Eureka Explorer from a downstream Snowflake table resource page to see what upstream Snowflake table was transformed as a result of a view associated with a dbt model. The dbt resource will also appear in Eureka Explorer.\nThis section talks about generating the account ID, project ID, and job ID. You will use the account id for --dbt-cloud-account parameter, project id for --dbt-cloud-project parameter, and job id for --dbt-cloud-run parameter.\nThe dbt cloud collector assumes that your dbt cloud instance has an environment and job set up with at least one successful run.\nUnder the Deploy menu at the top navigation, go to Jobs.\nFrom the Environment dropdown, select the environment that you want to run the collector against.\nSelect the Job associated with your Environment.\nFrom the URL of the Job page, copy the account ID, project ID, and job run ID from the following URL:\nhttps://cloud.getdbt.com/deploy/<accountID>/projects<projectID>/runs/<jobRunID>.\nThe account ID is represented as accountID, the project ID is represented as projectID, and the job run ID is represented as jobRunID.\nFrom the top right menu dropdown, select Profile Settings.\nNavigate to the API section. Click copy to the right of the API Key. You will use this api key for --dbt-cloud-api-key when setting up the collector to authenticate to dbt cloud.\nUnder the\u00a0Deploy\u00a0menu at the top navigation, go to\u00a0Jobs.\nFrom the\u00a0Environment\u00a0dropdown, select the environment that you want to run the collector against.\nSelect the\u00a0Job\u00a0associated with your\u00a0Environment and click Settings.\nUnder\u00a0Execution Settings, ensure that\u00a0Generate docs on run\u00a0is selected.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML\u00a0file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0DBT Cloud. Click\u00a0Next.\nOn the\u00a0Configure an on-premises DBT Cloud Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndbt cloud account ID\n--dbt-cloud-account=<account>\nThe dbt cloud account that owns the project from which to harvest dbt metadata artifacts.\nYes\ndbt cloud API key\n--dbt-cloud-api-key=<dbtCloudApiKey>\nA dbt cloud-issued api key with permissions to access the specified account\nYes\ndbt cloud project\n--dbt-cloud-project=<project>\nThe name or numeric identifier of the project from which to harvest dbt metadata artifacts.\nYes\ndbt cloud run\n--dbt-cloud-run=<runIdentifier>\nThe numeric identifier of the run that produced the artifacts to be harvested; if not specified, the most recent successful run that produced artifacts within the project will be harvested.\nNo\nOn the next screen, set the following advanced options and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nAuthentication: Select from one of the following authentication options.\nOption 1: Snowflake username and password overrides\nUsername\n--database-user=<databaseUser>\nThe user credential to use in connecting to the\u00a0target database.\nNo\nPassword\n--database-password=<databasePassword>\nThe password credential to use in connecting to \u00a0the target database.\nNo\nOption 2: Snowflake private key file overrides\nSnowflake key file path\n--snowflake-private-key-file=<snowflakePrivateKey>\nThe private key file to use for authentication with Snowflake (for example rsa_key.p8). Use this\u00a0option to override the dbt profile. Ignored for\u00a0non-Snowflake target dbs.\nNo\nSnowflake key file password\n--snowflake-private-key-file-password=<snowflakePrivateKeyFilePassword>\nThe password for the private key file to use for authentication with Snowflake, if the key is encrypted and a password was set Use this option to override the dbt profile or cloud configuration. It is ignored\u00a0for non-Snowflake target dbs.\nNo\nSnowflake application\n--snowflake-application=<snowflakeApplication>\nThe application connection parameter to use in connecting to the target Snowflake database. Use this option to override the dbt profile or cloud configuration. It is ignored\u00a0for non-Snowflake target dbs.\nUse datadotworld unless otherwise directed.\nNo\nSnowflake account\n<snowflakeAccount>--snowflake-account=\nThe Snowflake account/tenant.\nNo\nSnowflake role\n--snowflake-role=<snowflakeDatbaseRole>\nThe role to use in connecting to the target\u00a0Snowflake database. Use this option to override the dbt profile or cloud configuration. It is ignored\u00a0for non-Snowflake target dbs. This is\u00a0case-insensitive.\nNo\nSnowflake warehouse\n--snowflake-warehouse=<snowflakeDatbaseWarehouse>\nThe warehouse to use in connecting to the target Snowflake database. Use this option to override the dbt profile or cloud configuration. It is ignored\u00a0for non-Snowflake target dbs. This is case-insensitive.\nNo\nOn the Finalize your dbt Cloud Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe dbt Cloud command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizardbefore running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The catalog.json is a required file and was not generated for a job.\nSolution: Review and ensure that the Job Execution settings\u00a0are configured properly.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/130158-dbt-cloud-and-the-data-world-collector.html"
  },
  {
    "title": "dbt legacy metadata collector",
    "content": "With the release of the data.world Collector 2.85 we now use our Collector for dbt. This documentation is maintained for those customers who prefer to remain with the legacy collector, however for all new users we suggest using the new data.world Collector.\nThe computer running the catalog collector should have connectivity to the internet or access to the source instance, a minimum of 2G memory, and a 2Ghz processor.\nDocker must be installed. For more information see https://docs.docker.com/get-docker/. If you can't use docker, we have a java version available as well -- contact us for more details.\nThe files catalog.json, manifest.json , and profiles.yml must be in the same directory on the host machine, e.g., /tmp.\nRequest access to a download link from your data.world representative for the catalog collector. Once you receive the link, download the catalog collector Docker image (or programmatically download it with curl).\nLoad the docker image into the local computer\u2019s Docker environment:\nwhere X.Y is the version number of the dbt collector image.\nThe previous command will return an <image id>which needs to be renamed as 'dwbt'. Copy the &lt;image id&gt; and use it in the docker-load command:\nEach collector has parameters that are required, parameters that are recommended, and parameters that are completely optional. Required parameters must be present for the command to run. Recommended parameters are either:\nparameters that exist in pairs, and one or the other must be present for the command to run (e.g., --agent and --base)\nparameters that we recommend to improve your experience running the command in some way\nTogether, the required and recommended parameters make up the Basic parameters for each collector. The Basic parameters for this collector are:\n-a, --agent, --account=<agent> - The ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\n-P, --profile-file\u00a0<profileFile>\u00a0- The file containing profile definitions (defaults to dbt default of .dbt/profiles.yml\u00a0in the user's home directory)\n-g, --target <target> - The dbt profile target to use to obtain database location information (defaults to the\u00a0profile's 'target' value)\n-p, --profile=<profile> - the dbt profile to use to obtain database\u00a0location information (defaults to first profile found in profile definitions file)\nThe example below is an almost copy-and-paste command for any Unix environment that uses a Bash shell (e.g., MacOS and Linux). It uses the minimal set of parameters required to run the collector--your instance may require more. Information about the referenced parameters follows, and a complete list of parameters is at the end of the guide. Edit the command by adding any other parameters you wish to use, and by replacing the values for all your parameters with your information as appropriate. Parameters required by the collector are in bold. When you are finished, run your command.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.\n--help - Show the help text and exit.\n--upload-location=<uploadLocation>\u00a0-\u00a0The dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\n-a, --agent, --account=<agent> - The ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated\n-b, --base=<base> - The base URI to use as the namespace for any URIs generated (Must use this OR --agent)\n-P, --profile-file\u00a0<profileFile>\u00a0- The file containing profile definitions (defaults to dbt default of .dbt/profiles.yml\u00a0in the user's home directory)\n-g, --target <target> - The dbt profile target to use to obtain database location information (defaults to the\u00a0profile's 'target' value)\n-p, --profile=<profile> - the dbt profile to use to obtain database\u00a0location information (defaults to first profile found in profile definitions file)\n-t, --api-token=<apiToken> - The data.world API token to use for authentication; default is to use an environment variable named DW_AUTH_TOKEN\n-U, --upload - Whether to upload the generated catalog to the\u00a0 organization account's catalogs dataset or to another location specified with --upload-location (requires --api-token)",
    "url": "https://docs.data.world/en/105196-dbt-legacy-metadata-collector.html"
  },
  {
    "title": "Denodo and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Denodo tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports Virtual DataPort version 7.020200310 or 7.020200803\nThe collector supports basic authentication to Denodo.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe collector queries Denodo\u2019s built-in  VIEW_DEPENDENCIES() function to find the tables from which each view selects data. It harvests a lineage relationship between the view and each of these dependent tables.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC driver\nAppropriate JDBC driver for Denodo.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Denodo. Click Next.\nOn the Denodo Collector requires an additional driver file screen, provide the location of the directory where the generic driver is placed. Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source=${HOME}/dwcc-drivers\nThe location where you placed the JDBC driver.\nYes\nOn the Configure an on-premises Denodo Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following Advanced options and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDependency Query\n-q=<dependencyQuery>\n--dependencyQuery=<dependencyQuery>\nSQL query text used to fetch lineage. Query response should be identical to the\u00a0VIEW_DEPENDENCIES\u00a0stored procedure.\u00a0(Will use\u00a0VIEW_DEPENDENCIES(null, null)\u00a0if\u00a0 not supplied).\nNo\nServer environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database. It will be used to generate the ID for the database. This is optional and you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database.\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example,\u00a0property1=value1;property2=value2.\nNo\nOn the Finalize your Denodo Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Denodo command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98679-denodo-and-the-data-world-collector.html"
  },
  {
    "title": "Domo and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nTo run the Domo collector, you must have a Domo client ID and API secret, as described in the Domo developer docs. At a minimum, your API credentials must contain, data, dashboard, and user.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML\u00a0file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select Domo. Click\u00a0Next.\nOn the\u00a0Configure an on-premises Domo Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nDomo Client ID\n--domo-clientid=<clientId>\nDomo client ID for Domo API\nYes\nDomo secret\n--domo-clientsecret=<clientSecret>\nDomo client secret for Domo API\nYes\nOn the Finalize your Domo Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Domo collector command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98680-domo-and-the-data-world-collector.html"
  },
  {
    "title": "Dremio and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Dremio tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports Dremio version 4.7.2-202008180758160892-1a34c463.\nThe collector supports basic authentication to Dremio.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe collector obtains information about inter-table relationships from Dremio\u2019s built-in catalog graph. It writes a lineage relationship for any files or datasets represented as tables that are found in the graph.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nCollector run via docker requires version 3.0.6 of the Dremio jdbc driver, available for download at\u00a0https://download.dremio.com/jdbc-driver/. If you require use of a more recent version of the Dremio JDBC driver, please contact us at\u00a0support@data.world\u00a0for assistance.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Dremio. Click Next.\nOn the Dremio Collector requires an additional driver file screen, provide the location of the directory where the Dremio driver is placed. click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source= ${HOME}/dwcc-drivers\nThe location where you placed the JDBC driver.\nYes\nOn the Configure an on-premises Dremio Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer\n-s= <server>\n--server= <server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p= <port>\n--port= <port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d= <database>\n--database= <database>\nThe name of the database to connect to.\nYes\nUsername\n-u= <user>\n--user= <user>\nThe username to use to make the JDBC connection.\nYes\nPassword\n-P= <password>\n--password= <password>\nSpecify this option to provide the password for the database on the command line.\nYes\nSchemas to collect\n\nSelect from one of the following options:\u00a0Collect all schema,\u00a0Specify which schema to collect.\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S= <databaseSchema>\n--schema= <databaseSchema>\nSelect this option and then specify the names of the database schema to be catalog.\nYes\n(if --all-schemas\u00a0is not set)\nOn the next screen, set the following parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nGraph API Server\n-g=<graphApiServer>\n--graphApiServer= <graphApiServer>\nGraph API Server host, include protocol and port.\nNo\nServer environment\n-e= <environment>\n--environment= <environment>\nIf your provided server name is localhost use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseID>\n--database-id= <databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database).\nNo\nJDBC Properties\n--jdbc-property= <driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example,\u00a0property1=value1;property2=value2.\nNo\nOn the Finalize your Dremio Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Dremio command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98681-dremio-and-the-data-world-collector.html"
  },
  {
    "title": "Fivetran and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to discover Fivetran connectors, jobs, groups, sources, and destinations and perform impact analysis to understand how changes upstream to Fivetran connectors impact downstream systems.\nThe Fivetran collector supports the following method for authentication:\nFivetran API key [The owner of the API key needs read access to destinations and connectors. See the Fivetran documentation for more information.]\nYou will need the API Key and API secret information while running the collector.\nObject\nInformation cataloged\nGroups\nName, Identifier, Creation date\nConnectors\nIdentifier, Succeeded at datetime, Failed at datetime, Created at datetime, Sync frequency, Name, Version, Daily sync time, Schedule type, Setup state, Update state, Pause state, Sync state, Connector type, Connected by user identifier\nConnector Jobs\nRun state (Failed time, Success time)\nDestination\nName, Setup status, Destination type, Identifier, Region\nData source\nData source configuration properties, as configuration key and value pairs.\nDatabase tabular source\nDatabase name, Server, Port, JDBC URL\nNon-database tabular source\nDatasource ID, Name\nDatabase schema\nName\nDestination database\nDatabase name, Server, Port, JDBC URL\nColumns\nName\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nGroup\nDestinations within group, Connectors within group\nConnector\nSource associated to connector, group that contains this connector, connector job which was initiated by this connector.\nConnector Job\nConnector\nFivetran Data Source\nTables that this Fivetran Data Source connects to via Source (file or database), Connector that connects to this Fivetran Data Source\nData Source\nTables within data source. For non-database sources, tabular sources connected via data source\nDestination\nTables within destination, group that contains this destination\nTable\nColumn within the table\nColumn\nTable containing the column\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe Fivetran collector identifies the source columns that destination columns sourced their data from (via copy / load operation).\nThe collector catalogs all database sources and destinations. Cross-system column-level lineage is currently supported for PostgreSQL, SQL Server, and Snowflake.\nObject\nLineage available\nSource database columns and tables\nDownstream destinations that fields source their data from.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Fivetran. Click Next.\nOn the Configure an on-premises Fivetran Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nFivetran API key\n--fivetran-apikey=<apiKey>\nThe Fivetran API key used to authenticate to the REST API.\nYes\nFivetran secret\n--fivetran-apisecret=<apiSecret>\nThe Fivetran secret used to authenticate to the REST API.\nYes\nExclude groups\n--fivetran-exclude-group=<excludedGroupNames>\nExclude the specified Fivetran group's contents from the catalog.\nNo\nInclude only groups\n--fivetran-include-group=<includedGroupNames>\nInclude only the specified Fivetran group's contents in the catalog\nNo\nOn the Finalize your Fivetran Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nYou must ensure that you have set up these environment variables and directories before you run the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Fivetran command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/100928-fivetran-and-the-data-world-collector.html"
  },
  {
    "title": "Generic JDBC and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for tables and columns in the connected data source and make it searchable and discoverable in data.world.\nThe collector supports basic authentication.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nThe appropriate driver for your data source.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Generic. Click\u00a0Next.\nOn the Generic Collector requires an additional driver file screen, provide the location of the directory where the generic driver is placed. click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source= ${HOME}/dwcc-drivers\nThe location where you placed the JDBC driver.\nYes\nOn the\u00a0Configure a new on premises Generic Collector\u00a0screen, set the following properties and click\u00a0Next.\nOn the next screen, set the following parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nJDBC URL\n-j= <JDBC_URL>\n--jdbc-url=<JDBC_URL>\nThe JDBC URL for the database.\nYes\nUsername\n-u= <user>\n--user= <user>\nThe username to use to connect to the database.\nYes\nPassword\n-P= <password>\n--password= <password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A  --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following advanced parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e= <environment>\n--environment= <environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D= <databseid>\ndatabase-id= <databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nOn the\u00a0Finalize your Generic Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Generic command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98682-generic-jdbc-and-the-data-world-collector.html"
  },
  {
    "title": "Grafana and the data.world Collector",
    "content": "The\u00a0Grafana\u00a0collector is available\u00a0as a private beta release for select customers. Please\u00a0contact data.world\u00a0if you are interested in using this collector.\nThe latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nThe collector supports Grafana version 9.0.0.\nThe collector authenticates to Grafana using an API key. For the collector to run successfully, the API key needs to have Viewer role.\nTo generate the API key:\nComplete the following tasks to generate the API key that you will use for running the collector. See the Grafana documentation for all the details about doing this task.\nNavigate to https://<your_organization_name>.grafana.net/org/apikeys.\nClick Add API Key to add a new API key.\nProvide the key name.\nSet the Role as Viewer.\nIn the Time to live field, set the expiry time for the API key.\nNote down the API token that is generated after this task. We will use it while setting up the command for Grafana.\nThe collector catalogs the following information from Grafana.\nObject\nInformation cataloged\nDashboards\nOwner, created by date, style, type, version, url, slug (human-friendly portion of the dashboard URL).\nDashboard Panels\nTitle, description, type, associated dashboard, source if exists.\nDashboard Annotations\nTitle, query text, source if exists.\nDashboard Variables\nTitle, description, query text, source if exists.\nData Source\nType, title, source json.\nPlaylists\nTitle, dashboards that are part of the playlist.\nFolders\nTitle, dashboards that are part of the folder.\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the Grafana collector.\nObject\nLineage available\nDashboards\nThe collector identifies the associated annotations, variables, and any upstream data sources.\nData source\nThe collector identifies downstream annotations, dashboards, and panels.\nAnnotation\nThe collector identifies the associated upstream data source and the dashboards containing the annotation.\nVariable\nThe collector identifies the dashboard containing the variable.\nPanel\nThe collector identifies the associated dashboard annotations and variables, and upstream data sources.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Grafana. Click\u00a0Next.\nOn the\u00a0Configure an on-premises Grafana Collector screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nBase URL of the Grafana API\n--grafana-api-base-url=<baseUrl>\nBase URL of the Grafana API. Format: https://organizationName.grafana.net/api\nYes\nAuthentication token\n--grafana-api-token=<token>\nThe token for authentication to the Grafana API. This is the token you generated in this task.\nYes\nOn the\u00a0Finalize your Grafana Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nSample YAML file.\nThe Grafana command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/123204-grafana-and-the-data-world-collector.html"
  },
  {
    "title": "Hive and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Hive tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Hive.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Hive. Click\u00a0Next.\nOn the\u00a0Configure a new on premises Hive Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC URL for Hive Metastore\n--hive-metastore-jdbc-url= <hiveMetastoreJdbcUrl>\nThe JDBC URL for the Hive Metastore database.\u00a0The value you should pass is the same value you specify for\u00a0javax.jdo.option.ConnectionURL\u00a0in their Hive config.\nYes\nHive Metastore Username\n--hive-metastore-user= <hiveMetastoreUser>\nThe user to use in authenticating to the Hive Metastore.\nYes\nHive Metastore Password\n--hive-metastore-password= <hiveMetastorePassword>\nThe password to use in authenticating to the Hive\u00a0Metastore database.\nYes\nServer environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databaseId>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property= <driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example,\u00a0property1=value1;property2=value2\nNo\nOn the\u00a0Finalize your Hive Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate Configuration file (YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you generated a YAML file, click\u00a0Next.\nThe\u00a0Hive Collector command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizardbefore running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98683-hive-and-the-data-world-collector.html"
  },
  {
    "title": "Hive metastore and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Hive metastore tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Hive metastore.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nThe Hive Collector has the ability to capture the table metadata properties\u00a0while also harvesting other valuable table-level metadata from the Hive metastore. To catalog information from the metastore you need to use the following Collector parameters:\n--hive-metastore-jdbc-url=<hiveMetastoreJdbcUrl>- The JDBC URL for the Hive Metastore database. The value you should pass is the same value you specify for\u00a0javax.jdo.option.ConnectionURL\u00a0in their Hive config.\n--hive-metastore-password=<hiveMetastorePassword>- The password to use in authenticating to the Hive\u00a0Metastore database.\n--hive-metastore-user=<hiveMetastoreUser>- The user to use in authenticating to the Hive Metastore.\nYou must pass all three\u00a0--hive-metastore\u00a0options for the collector to attempt to harvest anything from the hive metastore. if\u00a0--hive-metastore-jdbc-url\u00a0isn\u2019t passed, the collector will write a warning and harvest the standard jdbc collector content--it won\u2019t prevent cataloging the basic jdbc db/schema/table/column objects, it just won\u2019t get the table-level metadata from the metastore.\nMake sure to supply a jdbc driver for the specific database as needed. In particular, if your metastore db is oracle or mysql, you will need to put the driver jar in the jdbc drivers directory (just as you would if you were running those databases\u2019 collectors). If your metastore db is postgres, derby, or sql server, we ship the necessary drivers with the data.world COllector.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Hive Metastore. Click\u00a0Next.\nOn the\u00a0Configure an on-premises Hive Metastore Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nHive Server Host\n--hive-server-host=<hiveServerHost>\nThe host of the Hive server. The collector does not connect to Hive, but needs this to properly identify the database.\nYes\nHive Server port\n--hive-server-port=<hiveServerPort>\nThe port of the Hive server. The collector does not connect to Hive, but needs this to properly identify the database. If not specified, the Hive default port (10000) is assumed.\nNo\nMetastore JDBC URL\n--metastore-jdbc-url=<metastoreJdbcUrl>\nThe JDBC URL for the metastore database.\nYes\nMetastore Database Username\n--metastore-database-user=<metastoreDatabaseUsername>\nThe user/account to use to query the metastore database.\nYes\nMetastore Database Password\n--metastore-database-password=<metastoreDatabasePassword>\nThe password to use to query the metastore database.\nYes\nHive Databases\n--hive-database=<hiveDatabase>\nThe Hive database(s) to catalog (specify option multiple times to catalog multiple specificdatabases, by default all databases are cataloged).\nNo\nOn the\u00a0Finalize your Hive Metastore Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file (YAML) or\u00a0Command line arguments (CLI). Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Hive Metastore command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizardbefore running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98684-hive-metastore-and-the-data-world-collector.html"
  },
  {
    "title": "Infor ION and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Infor ION tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector authenticates to the Infor ION database using the standard\u00a0.ionapi credential file. Consult the Infor ION documentation about how to obtain this file, or contact the database/tenant administrator. The file must be named\u00a0Infor\u00a0Compass\u00a0JDBC\u00a0Driver.ionapi\u00a0and must be in the same directory as the Compass JDBC driver (assumed to be\u00a0/jdbcdrivers) that is mounted into the docker container at\u00a0/usr/src/dwcc-config/lib\u00a0.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC driver\nThe computer\u00a0must\u00a0have the\u00a0appropriate JDBC driver\u00a0on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Infor ION. Click Next.\nOn the Infor ION Collector requires an additional driver file screen, provide the location of the directory where the generic driver is placed. Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source=<{HOME}/dwcc-drivers>\nThe location where you placed the JDBC driver.\nYes\nOn the Configure an on-premises Infor ION Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e=<environment>\n--environment=<environment>\nf your provided server name is&amp;#xA0;localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databaseid>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nSelect this option and then specify the names of the database schema to be catalog.\nYes\n(if --all-schemas\u00a0is not set)\nOn the Finalize your Infor ION Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Infor ION command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98685-infor-ion-and-the-data-world-collector.html"
  },
  {
    "title": "Information Schema Catalog Collector (ISCC) and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThere are occasionally times when a database configuration makes it difficult to connect the the data.world Collector directly to the data source. In those cases, the Information Schema Catalog Collector (ISCC) can be used to access the information schema of a database as a source for cataloging the database's metadata.\nThe information schema is an ANSI-standard set of read-only views of all the tables, views, columns, and procedures in an RDBMS. The ISCC works by parsing CSV files created from the information schema and using them as an input source for the data.world Collector. You can find more information on the information schema here. Using the data.world Collector directly is the preferred method for cataloging an RDMS, but the following instructions provide a secondary access method when needed.\nWe have tested this collector against a MS SQL Server database, but it can be used on any database for which you can generate the four CSV files described in this guide.\nThere are two parts to cataloging metadata from a database's information schema:\nGenerate or create the CSV files containing the database's metadata\nRun the the Collector against the CSV files\nIn this article we cover both parts in the order they need to be done.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, JDBC type, Column Type, Is Nullable, Default Value, column size, column index\nTable\nName, schema\nViews\nName, Schema\nSchema\nIdentifier\nDatabase\nType, name, identifier, server, port, environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nThere are a couple of different ways to create the CSV files necessary for the Information Schema Catalog Collector. The first, and easiest, is to run SQL queries directly against the information schema of the database. Below are the requirements for using this method:\nThe database supports Information Schema and SQL querying.\nYou have permissions to query the database.\nThe second way to create the CSV files is to build them manually. If you use this option, your CSV files must contain the following columns.\nFor tables.csv:\nTABLE_SCHEMA,\nTABLE_NAME\nTABLE_TYPE\nFor columns.csv:\nTABLE_SCHEMA\nTABLE_NAME\nCOLUMN_NAME\nORDINAL_POSITION\nIS_NULLABLE\nDATA_TYPE\nCOLUMN_DEFAULT\nCHARACTER_MAXIMUM_LENGTH\nNUMERIC_PRECISION\nFor table_constraints.csv:\nTABLE_SCHEMA\nTABLE_NAME\nCONSTRAINT_NAME\nCONSTRAINT_TYPE\nFor constraint_column_usage:\nTABLE_SCHEMA\nTABLE_NAME\nCOLUMN_NAME\nCONSTRAINT_NAME\nIf you are creating the CSV files manually you can find information about the data formats and how they are nulled in this document.\nTo generate the CSV files from SQL queries against a database's information schema, run the following four SQL queries against your database:\nselect * from information_schema.tables\nselect * from information_schema.columns\nselect * from information_schema.table_constraints\nselect * from information_schema.constraint_column_usage\nExport the results of each query as CSV files with the names:\ntables.csv\ncolumns.csv\ntable_constraints.csv\nconstraint_column_usage.csv\nThese files will be loaded into the csv-file-directory used with the data.world Collector.\nIf you use this option to create the CSV files to use with the data.world Collector, they must contain the following columns.\nFor tables.csv:\nTABLE_SCHEMA,\nTABLE_NAME\nTABLE_TYPE\nFor columns.csv:\nTABLE_SCHEMA\nTABLE_NAME\nCOLUMN_NAME\nORDINAL_POSITION\nIS_NULLABLE\nDATA_TYPE\nCOLUMN_DEFAULT\nCHARACTER_MAXIMUM_LENGTH\nNUMERIC_PRECISION\nFor table_constraints.csv:\nTABLE_SCHEMA\nTABLE_NAME\nCONSTRAINT_NAME\nCONSTRAINT_TYPE\nFor constraint_column_usage:\nTABLE_SCHEMA\nTABLE_NAME\nCOLUMN_NAME\nCONSTRAINT_NAME\nIf you are creating the CSV files manually you can find information about the data formats and how they are nulled in this document.\nOnce you have the CSV files you are ready to run Docker and the the data.world Collector against them just as for any other data source.\nThe CSV files created in a previous step. Two of them--tables.csv and columns.csv--are required, the other two are optional.About Information Schema Catalog Collector (ISCC)\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the ISCC collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Information Schema. Click Next.\nOn the Configure an on-premises Information Schema Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nCSV Directory\n--csv-directory=<csvFilesDirectory>\nThe directory containing CSV files extracted from the database to be catalogued.Options and requirements\nYes\nServer\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n--database=<database>\nThe name of the database from which the CSV files were extracted.\nYes\nDatabase Type\n--database-type=<databaseType>\nThe type of database that is being cataloged; supported databases: SQLSERVER, ORACLE, GENERIC (use generic for databases not explicitly listed).\nYes\nOn the Finalize your Information Schema Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file (YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Information Schema command screen gives you the command for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98686-information-schema-catalog-collector--iscc--and-the-data-world-collector.html"
  },
  {
    "title": "Kafka - Confluent Cloud and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata from Confluent Cloud. The collector harvests metadata from a Kafka cluster running in Confluent Cloud. The collector can optionally harvest metadata from Avro, json-schema, and Protobuf schemas stored in Confluent Schema Registry.\nThe collector supports V2 of the\u00a0Cluster APIs and Confluent Cloud Organization.\nThe collector authenticates to a Kafka cluster using Simple Authentication and Security Layer (SASL), with a username/password credential. By default, the collector assumes that SASL is used over Secure Sockets Layer (SSL). In cases where SSL is disabled (for example, internal test clusters in Kafka), you can disable SSL for the collector. Consult the Apache Kafka documentation for more information on Kafka security.\nThe collector catalogs the following information.\nNote that the collectors only harvest schemas in the Confluent Schema Registry registered under a subject that matches a topic\u2019s key or value, according to the default TopicNameStrategy naming strategy, described in the Confluent Schema Registry documentation. Schemas in the schema registry registered under other subjects are not currently harvested.\nObject\nInformation cataloged\nCluster\nIdentifier, Display name\nProducer\nIdentifier\nConsumer\nIdentifier, Client ID, Client host\nBroker\nIdentifier, Display name, Host, Port, Rack\nPartition\nPartition number\nSchema\nIdentifier, Title, Is Current Schema, Schema Version, Type (avro, json, producers), Schema text\nConsumer Group\nIdentifier, State, Partition assignor\nTopic\nName, Identifier, Is internal (whether the topic is internal)\nEnvironment\nIdentifier, Display name\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nCluster\nBrokers within Cluster\nTopics hosted by Cluster\nProducer\nPartition that receive messages from Producers\nConsumer Group\nConsumers that are members of Consumer Group\nConsumer\nPartition that Consumer is assigned to\nBroker\nCluster containing Broker\nPartitions having replicates on Broker\nTopic\nCluster hosting Topic\nPartitions that segment Topic\nSchema that constrains this Topic\u2019s values and keys\nPartition\nTopic segmented into this Partition\nConsumer that is assigned to Partition\nBroker that is replica for Partition\nSchema\nTopic key and value constrained by this schema\nOther schemas related to this schema\nEnvironment\nClusters provisioned within this environment\nFor the collector to access your Kafka clusters and Kafka resources, you will need to set up Kafka API keys. Each key is valid only for one specific Kafka cluster.\nFrom Confluent Cloud, navigate to the cluster you want to add the API key to using the instructions here.\nFor the collector to access your Confluent Cloud Schema Registry, you will need to set up Confluent Schema Registry API keys. Each key is valid for one specific Schema Registry. This is optional and needs to be set only if you want the the collector to harvest information from Confluent Schema Registry.\nFrom Confluent Cloud, navigate to the environment containing your schema registry. Add the API key using the instructions here.\nReview the Confluent permissions for Confluent resources.\nIn order for the Kafka collector to harvest metadata about a topic (including partitions, consumers, consumer groups, and schemas) the cluster user passed to the collector must have DESCRIBE permission on the topic. If the user does not have the DESCRIBE permission, the collector doesn't see that topic and cannot write any information for the topic to the catalog graph.\nIn order for the collector to harvest information about a topic\u2019s producers, the cluster user passed to the collector must have READ permission on the topic. If the user lacks READ permission, the collector will write a warning message indicating that producer information cannot be harvested. If a user has READ permission for a topic, that user automatically has DESCRIBE permission as well.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThere are a few different ways to run the data.world Collector--any of which can be combined with an automation strategy to keep your catalog up to date:\nCreate a configuration file (config.yml) - This option stores all the information needed to catalog your data sources. It is an especially valuable option if you have multiple data sources to catalog as you don't need to run multiple scripts or CLI commands separately.\nRun the collector though a CLI - Repeat runs of the collector requires you to re-enter the command for each run.\nThis section walks you through the process of running the collector using CLI.\n\nThe easiest way to create your Collector command is to:\nCopy the following example command in a text editor.\nSet the required parameters in the command. The example command includes the minimal parameters required to run the collector\nOpen a terminal window in any Unix environment that uses a Bash shell and paste the command in it and run in.\nThe following table describes the parameters for the command. Detailed information about the Docker portion of the command can be found here.\nParameter\nDetails\nRequired?\ndwcc:<CollectorVersion>\nReplace <CollectorVersion> in with the version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\n-a =<agent>\n--agent=<agent>\n--account=<agent>\nThe ID for the data.world account into which you will load this catalog. The ID is the organization name as it appears in your organization. This is used to generate the namespace for any URIs generated.\nYes\n--kafka-bootstrap-server=<kafkaBootstrapServer>\nThe bootstrap server for the kafka cluster to be\u00a0\u00a0cataloged.\nYes\n--kafka-cluster-api-key=<kafkaClusterApiKey>\nThe API key for the cluster to be cataloged.\nYes\n--kafka-cluster-api-secret=<kafkaClusterApiSecret>\nThe API secret for the cluster to be cataloged.\nYes\n--confluent-cloud-api-key=<confluentCloudApiKey>\nThe API key for Confluent Cloud.\nNo\n--confluent-cloud-api-secret=<confluentCloudApiSecret>\nThe API secret for Confluent Cloud.\nNo\n--confluent-schema-registry-api-url=<confluentSchemaRegistryApiUrl>\nThe base url where Confluent Schema Registry is listening.\nNo\n--confluent-schema-registry-password=<confluentSchemaRegistryPassword>\nThe password for authentication to Confluent\u00a0Schema Registry.\nNo\n--confluent-schema-registry-username=<confluentSchemaRegistryUsername>\nThe username for authentication to Confluent\u00a0\u00a0Schema Registry.\n\nNo\n--dry-run=<dryRun>\nSpecify this option to run the collector in dry run mode to test the connection details provided. No metadata is harvested in dry run mode.\nNo\n-n=<catalogName>\n--name=<catalogName>\nThe name of the catalog - this will be used to generate the ID for the catalog as well as the filename into\u00a0which the catalog file will be written.\nYes\n-o=<outputDir>\n--output=<outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nIn our example we use the /dwcc -outputas\u00a0it\u00a0is\u00a0running\u00a0in\u00a0a\u00a0Docker\u00a0container and\u00a0that\u00a0is\u00a0what\u00a0we\u00a0specified\u00a0in\u00a0the\u00a0script\u00a0for\u00a0a\u00a0Docker\u00a0mount\u00a0point.\nYou\u00a0can\u00a0change\u00a0this\u00a0value\u00a0to\u00a0anything\u00a0you\u00a0would\u00a0like\u00a0as long\u00a0as\u00a0it\u00a0matches\u00a0what\u00a0you\u00a0use\u00a0in\u00a0the\u00a0mount\u00a0point:\n-mount\u00a0type=bind,source=/tmp,target=/dwcc-output\u00a0...-o /dwcc-output\nIn this example, the output will be written to the /tmp directory on the local machine, as indicated by the mount point directive. The log file, in addition to any catalog files, will be written to the directory specified in the mount point directive.\nYes\n\n-L\n--no-log-upload\nDo not upload the log of the dwcc run to the organization account's catalogs dataset or to another location specified with --upload-location (ignored if --upload not specified)\nNo\n--site=<site>\nThe slug for the data.world site into which you will load this catalog this is used to generate the namespace for any URIs generated.\nNo\n-H=Host\n--api-host=Host\nThe host for the data.world API.\nNo\n-t=<apiToken>\n--api-token=<apiToken>\nThe data.world API token to use for authentication. The default is to use an environment variable named DW_AUTH_TOKEN.\nNo\n-U\n--upload\nWhether to upload the generated catalog to the organization account's catalogs dataset or to another location specified with --upload-location (This requires that the --api-token is specified.\nNo\n--upload-location=<uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account. This parameter is ignored if --upload is not specified.\nNo\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The provided Kafka information or credentials are not correct.\nSolution: Check the server / base URL information and API key/secret are correct..\nCause: The provided Schema Registry information or credentials are not correct.\nSolution: Check base URL information and api key/secret are correct, and permissions are properly set.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.",
    "url": "https://docs.data.world/en/158313-kafka---confluent-cloud-and-the-data-world-collector.html"
  },
  {
    "title": "Kafka - Confluent Platform and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nConfluent Platform is a data streaming platform that extends Apache Kafka. Use this collector to harvest metadata from a Kafka Confluent Platform cluster running on-premise or in an environment managed by the user (or the user\u2019s organization). The collector can optionally harvest metadata from Avro, json-schema, and Protobuf schemas stored in Confluent Schema Registry.\nThe collector supports version 3.3.2 of the Kafka Admin API and supports any Confluent Kafka cluster compatible with that version.\nThe collector authenticates to a Kafka cluster using Simple Authentication and Security Layer (SASL), with a username/password credential. By default, the collector assumes that SASL is used over Secure Sockets Layer (SSL). In cases where SSL is disabled (for example, internal test clusters in Kafka), you can disable SSL for the collector. Consult the Apache Kafka documentation for more information on Kafka security.\nThe collector catalogs the following information.\nNote that the collectors only harvest schemas in the Confluent Schema Registry registered under a subject that matches a topic\u2019s key or value, according to the default TopicNameStrategy naming strategy, described in the Confluent Schema Registry documentation. Schemas in the schema registry registered under other subjects are not currently harvested.\nObject\nInformation cataloged\nCluster\nIdentifier, Display name\nProducer\nIdentifier\nConsumer\nIdentifier, Client ID, Client host\nBroker\nIdentifier, Display name, Host, Port, Rack\nPartition\nPartition number\nSchema\nIdentifier, Title, Is Current Schema, Schema Version, Type (avro, json, producers), Schema text\nConsumer Group\nIdentifier, State, Partition assignor\nTopic\nName, Identifier, Is internal (whether the topic is internal)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nCluster\nBrokers within Cluster\nTopics hosted by Cluster\nProducer\nPartition that receive messages from Producers\nConsumer Group\nConsumers that are members of Consumer Group\nConsumer\nPartition that Consumer is assigned to\nBroker\nCluster containing Broker\nPartitions having replicates on Broker\nTopic\nCluster hosting Topic\nPartitions that segment Topic\nSchema that constrains this Topic\u2019s values and keys\nPartition\nTopic segmented into this Partition\nConsumer that is assigned to Partition\nBroker that is replica for Partition\nSchema\nTopic key and value constrained by this schema\nOther schemas related to this schema\nFor the collector to access your Confluent Platform, work with your Confluent Platform administrator to set up a user that the collector will use to authenticate to Confluent.\nFor the collector to access your Confluent Schema Registry, you will need to set up the standard Confluent Schema Registry API. Each key is valid for one specific Schema Registry. This is optional and needs to be set only if you want the the collector to harvest information from Confluent Schema Registry. Set up the API key using the instructions here.\nReview the Confluent permissions for Confluent resources.\nIn order for the Kafka collector to harvest metadata about a topic (including partitions, consumers, consumer groups, and schemas) the cluster user passed to the collector must have DESCRIBE permission on the topic. If the user does not have the DESCRIBE permission, the collector doesn't see that topic and cannot write any information for the topic to the catalog graph.\nIn order for the collector to harvest information about a topic\u2019s producers, the cluster user passed to the collector must have READ permission on the topic. If the user lacks READ permission, the collector will write a warning message indicating that producer information cannot be harvested. If a user has READ permission for a topic, that user automatically has DESCRIBE permission as well.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThere are a few different ways to run the data.world Collector--any of which can be combined with an automation strategy to keep your catalog up to date:\nCreate a configuration file (config.yml) - This option stores all the information needed to catalog your data sources. It is an especially valuable option if you have multiple data sources to catalog as you don't need to run multiple scripts or CLI commands separately.\nRun the collector though a CLI - Repeat runs of the collector requires you to re-enter the command for each run.\nThis section walks you through the process of running the collector using CLI.\n\nThe easiest way to create your Collector command is to:\nCopy the following example command in a text editor.\nSet the required parameters in the command. The example command includes the minimal parameters required to run the collector\nOpen a terminal window in any Unix environment that uses a Bash shell and paste the command in it and run in.\nThe following table describes the parameters for the command. Detailed information about the Docker portion of the command can be found here.\nParameter\nDetails\nRequired?\ndwcc: <CollectorVersion>\nReplace <CollectorVersion> in with the version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\n-a = <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog. The ID is the organization name as it appears in your organization. This is used to generate the namespace for any URIs generated.\nYes\n--kafka-bootstrap-server-port= <boostrapServerPort>\nA bootstrap server:port for the cluster to be cataloged. The port is optional if the server is listening on the default port 9092.\nYes\n--kafka-cluster-username= <clusterUsername>\nThe username to use in authenticating to the Kafka cluster.\nYes\n--kafka-cluster-password= <clusterPassword>\nThe password to use in authenticating to the Kafka cluster.\nYes\n--kafka-cluster-sasl-ssl\nA flag to indicate whether the Kafka cluster uses\u00a0 SASL SSL for authentication.\nYes\n(enabled by default)\n--confluent-schema-registry-api-url=<confluentSchemaRegistryApiUrl>\nThe base url where Confluent Schema Registry is listening\nNo\n--confluent-schema-registry-password= <confluentSchemaRegistryPassword>\nThe password for authentication to Confluent\u00a0Schema Registry\nNo\n--confluent-schema-registry-username= <confluentSchemaRegistryUsername>\nThe username for authentication to Confluent\u00a0\u00a0Schema Registry\n\nNo\n--dry-run= <dryRun>\nSpecify this option to run the collector in dry run mode to test the connection details provided. No metadata is harvested in dry run mode.\nNo\n-n= <catalogName>\n--name= <catalogName>\nThe name of the catalog - this will be used to generate the ID for the catalog as well as the filename into\u00a0which the catalog file will be written.\nYes\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nIn our example we use the /dwcc -outputas\u00a0it\u00a0is\u00a0running\u00a0in\u00a0a\u00a0Docker\u00a0container and\u00a0that\u00a0is\u00a0what\u00a0we\u00a0specified\u00a0in\u00a0the\u00a0script\u00a0for\u00a0a\u00a0Docker\u00a0mount\u00a0point.\nYou\u00a0can\u00a0change\u00a0this\u00a0value\u00a0to\u00a0anything\u00a0you\u00a0would\u00a0like\u00a0as long\u00a0as\u00a0it\u00a0matches\u00a0what\u00a0you\u00a0use\u00a0in\u00a0the\u00a0mount\u00a0point:\n-mount\u00a0type=bind,source=/tmp,target=/dwcc-output\u00a0...-o /dwcc-output\nIn this example, the output will be written to the /tmp directory on the local machine, as indicated by the mount point directive. The log file, in addition to any catalog files, will be written to the directory specified in the mount point directive.\nYes\n\n-L\n--no-log-upload\nDo not upload the log of the dwcc run to the organization account's catalogs dataset or to another location specified with --upload-location (ignored if --upload not specified)\nNo\n--site= <site>\nThe slug for the data.world site into which you will load this catalog this is used to generate the namespace for any URIs generated.\nNo\n-H= Host\n--api-host= Host\nThe host for the data.world API.\nNo\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. The default is to use an environment variable named DW_AUTH_TOKEN.\nNo\n-U\n--upload\nWhether to upload the generated catalog to the organization account's catalogs dataset or to another location specified with --upload-location (This requires that the --api-token is specified.\nNo\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account. This parameter is ignored if --upload is not specified.\nNo\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The provided Kafka information or credentials are not correct.\nSolution: Check the server / base URL information and username and password are correct.\nCause: The provided Schema Registry information or credentials are not correct.\nSolution: Check base URL information and api key/secret are correct, and permissions are properly set.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/158314-kafka---confluent-platform-and-the-data-world-collector.html"
  },
  {
    "title": "Looker and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nThe collector supports the following Looker API version:\nLooker API v 4.0\nIn order to scan a Looker account you will need to set up a specific role in Looker with these permissions checked:\nThe Looker permissions are set under Admin > Roles > New Permission Set in Looker:\nThe permissions indicated will enable you to create catalogs for all the models and explores that you have access to. Here is more information from the Looker documentation about how access to content is managed:\nData Access, which controls which data a user is allowed to view, is primarily managed via Model Sets. Model Sets make up one half of a Looker role which is applied to users and groups. Data access can be further restricted within a model using access filters to limit which rows of data they can see, as though there was an automatic filter on their queries. You can also restrict access to specific Explores, joins, views, or fields using access grants.\nTo restrict access using an access grant you will need to:\nDefine an access grant\nApply the required_access_grants to the explore.\nHere is an example of how an access grant could be structured based on a user attribute called \u201cdepartment\u201d:\nMore info on the access_grant can be found here.\nThen you can apply the required_access_grants to the explore:\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Looker. Click\u00a0Next.\nOn the\u00a0Configure a new on premises Looker Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nLooker URL Base Path\n--looker-api-base-url=<baseUrl>\nBase URL of the Looker API,\u00a0for example: https://looker-customer.cloud.looker.com/api/4.0/\nYes\nThe Looker Client ID\n--looker-clientid=<clientId>\nLooker client ID for Looker app. When the credential is for a non-admin user, we do not get any information about the database objects used by Explores. For everything else, the catalog will contain all the objects that the credential\u2019s user has access to.\nYes\nThe Looker secret\n--looker-clientsecret=<clientSecret>\nLooker client secret for the Looker API. It is recommended you use a variable for this parameter as it has sensitive information.\nYes\nOn the\u00a0Finalize your Looker Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or Command line arguments (CLI).\u00a0Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nSample YAML file.\nThe Looker command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98687-looker-and-the-data-world-collector.html"
  },
  {
    "title": "MANTA and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest lineage relationships from Manta including metadata for databases, tables, and columns and make it searchable and discoverable in data.world.\nThe collector supports manta versions R35, R38.1, and R40.1. Because of known issues, the collector does not work for versions R37 and R38.\nThe collector uses MANTA\u2019s batch export integration functionality which requires MANTA version R30 or above, and a license that enables the batch export integration functionality.\nThe collector supports both username & password and token-based authentication.\nToken-based authentication works for connections that a user has set up in Manta, but does not work when the user imported a dump file.\nFor username/password authentication, make sure you set the following parameters:\n--manta-user\n--manta-password\nFor token-based authentication, make sure you set the following parameters:\n--manta-token-url\n--manta-client-id\n--manta-client-secret\nThe collector only harvests metadata from connections that a user has set up in Manta. It does not work when the user imports an export file or dump file from another instance of Manta.\nThe Manta collector supports the following database: PostgreSQL, Microsoft SQL Server, Teradata, Oracle, Hive, DB2, Snowflake.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nDatabase\nTable, Type, Name\nSchema\nName\nTable\nName\nColumns\nColumn Type, Name, Index, JDBC type\nFile resource\nName\nThe MANTA collector harvests information in MANTA scans for the MANTA scan sources that we support (intra-database lineage for relational databases, and ETL processes from Informatica PowerCenter and Talend).\nFor relational databases, the collector harvests lineage relationships between views and the tables from which each view selects data, and stored procedures with tabular results and tables referenced by each procedure.\nFor ETL technologies, the collector harvests lineage relationships between source and sink tables in each ETL process.\nThis section talks about generating the client secret and ID and setting up account roles.\nYou need to perform these tasks if you are using the token-based authentication.\nCreate a new client to access the Manta APIs. Login to the Manta Knowledge Base, and follow the Manta documentation for complete instructions.\nNote the value of the Client ID. You will use this for the --manta-client-id parameter for the collector.\nNext, generate the Client Secret. Note the value of the Client Secret. You will use this value for the --manta-client-secret parameter for the collector.\nNavigate to Clients.\nEnsure that the following roles are set:\ndefault-roles-manta\nConfigurator_READ\nPROCESS_MANAGER_EXECUTE\nPROCESS_MANAGER_READ\nPROCESS_MANAGER_WRITE\nAlternatively, you can set the AdminGuiComposite role.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Manta. Click\u00a0Next.\nOn the\u00a0Configure a new on premises Manta Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nAuthentication\nSelect from:\nAuthentication using a MANTA Token\nAuthentication using username and password\nAuthentication using MANTA Token\nYes\n(or set the Authentication using Username and Password parameters)\n\n\n\nMANTA Token URL\n--manta-token-url=<mantaTokenUrl>\nURL to obtain a jwt token to authenticate with\u00a0manta.\nExample:\nhttp://localhost:9090/auth/realms/manta/protocol/openid-connect/token.\nMANTA ClientId\n--manta-client-id=<mantaClientId>\nClient ID name for the oauth connection.\nMANTA Client Secret\n--manta-client-secret=<mantaClientSecret>\nClient Secret used to obtain a jwt authentication token.\nAuthentication using Username and Password\nYes\n(or set the Authentication using MANTA Token parameters)\n\n\nMANTA Username\n--manta-user=<mantaUse>\nMANTA user to use for API authentication\nMANTA Password\n--manta-password=<mantaPassword>\nPassword to use for API authentication\nMANTA API URL\n--manta-api-url=<mantaAdminApiBaseUrl>\nURL of MANTA Admin API, for example: http://host:8181/manta-admin-gui/public/process-manager/v1/\nYes\nOn the next screen, set the following advanced properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nPerform Manta Analysis Scan\n--manta-scan=<true_or_false>\nIf present, have MANTA perform an analysis scan to refresh the MANTA graph prior to exporting\u00a0(default \u00a0is to catalog existing repository contents without a new MANTA scan)\nNo\nMANTA viewer URL\n--manta-viewer-url=<mantaViewerUrl>\nURL of MANTA Viewer UI, for example: http://host/manta-dataflow-server/viewer\nNo\nManta max parallel scenarios\n--manta-max-parallel-scenarios=<maxParallelScenarios>\nMaximum number of scenarios to extract from the MANTA graph at once to optimize performance.\u00a0Specifying this option and passing an integer value will configure the\u00a0MANTA API\u00a0to export the specified number of scenarios in the MANTA graph in parallel. The default value is 4; adjusting this up or down can improve performance.\nNo\nMANTA database ID mappings\n--manta-db-id-mapping=<databaseIdMappings>\nMappings of the form [server]/[database name]=[database-id], used to associate a database-id\u00a0 with a database found in the MANTA graph that has the specified server and database names.\u00a0(You only need provide to this if the database name in the MANTA graph\u00a0 is not sufficiently unique to completely\u00a0 identify the database.)\nNo\nMANTA port mappings\n--manta-port-mapping=<portMappings>\nMappings of the form [server]/[database name]=[port], used to associate a port\u00a0 with a database found in the MANTA graph that has the specified server and database names.\u00a0(You only need provide to this if the database listens on a port other than the default port\u00a0 for that type of database.)\nNo\nOn the\u00a0Finalize your Manta Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next.\nThe next screen gives you an option to download the\u00a0YAML configuration file\u00a0or copy the\u00a0CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Manta command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizardbefore running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nSample command with username and password parameters.\nSample command with MANTA Token parameters.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98688-manta-and-the-data-world-collector.html"
  },
  {
    "title": "Marquez and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nThe collector supports Marquez API version 0.20.0.\nWe develop using admin credentials.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the Marquez collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Marquez. Click Next.\nOn the Configure a new on premises Marquez Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nMarquez API base URL\n--marquez-api-base-url=<baseUrl>\nThe base URL of the Marquez API.\nYes\nAPI Authentication token\n--marquez-api-key=<apiKey>\nThe API key/token for authentication to the Marquez API.\nYes\nOn the Finalize your Marquez Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nSample YAML file.\nThe Marquez command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98689-marquez-and-the-data-world-collector.html"
  },
  {
    "title": "Monte Carlo and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe Monte Carlo collector harvests resources from your Monte Carlo environment such as Monitors and Incidents, and the tables associated with the Monitors and Incidents.\nThe Monte Carlo collector authenticates to Monte Carlo using API key.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nMonitor\nName (Monitor Description or UUID), Description, Created Time, Monitor Status Type, External link, Paused State, Snoozable state, Snoozed state, Is Template Managed, Monitor type, Description, Schedule type, User the monitor was created by\nIncident\nTitle, Incident ID, Incident Time, External Link, Incident Feedback, Incident Reaction Type, Owner Email, Severity, Incident type and sub-types\nTable\nExternal Link, Incidents by Status Summary, Incidents Under Investigation Summary, Monitors Summary, Table ID, Schema and database that tables belong to\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nIncident that occurred on the table\nIncident\nTable that Incident occurred on\nMonitor\nTable that is monitored\nYou will need to set up an API key for your user to connect to Monte Carlo. The collector will harvest resources from the domains that your user has access to. We recommend that you use service account keys.\nTo generate a Monte Carlo API key:\nLog in to your Monte Carlo instance.\nFrom the top navigation, click Settings.\nFrom the left navigation, click on API.\nClick Create Key.\nAssign a description and set an expiration.\nClick Create.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Monte Carlo. Click\u00a0Next.\nOn the\u00a0Configure an on-premises Monte Carlo Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nThe Monte Carlo API Key ID\n--montecarlo-api-key-id= <keyID>\nMonte Carlo key id provided when creating API key.\nYes\nThe Monte Carlo secret\n--montecarlo-api-secret= <secret>\nMonte Carlo secret provided when creating API key.\nYes\nOn the next screen, set the following Advanced properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nMonte Carlo API base URL\n--montecarlo-api-endpoint=<endpoint>\nBase URL of the Monte Carlo API.\nNo\nMonte Carlo domain name\n\u2013-montecarlo-domain= <domainNames>\nThe name of the Monte Carlo domain to catalog resources from.\nNo\nMonte Carlo GraphQL page size\n--montecarlo-graphql-page-size= <pageSize>\nSet the page size for Monte Carlo graphql queries which support pagination.\nDefault is 5000.\nNo\nMonte Carlo incident lookback days\n--montecarlo-incident-lookback-days= <incidentLookbackDays>\nNumber of days in the past to harvest incidents\u00a0from. If not specified, all incidents are\u00a0harvested.\nNo\nOn the\u00a0Finalize your Monte Carlo Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI). Click\u00a0Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Monte carlo command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: This issue generally happens when the Monte Carlo user associated with the API key that the collector uses to authenticate to Monte Carlo does not have permissions to a certain domain.\nSolution: Ensure the user has access to the specific domains.\nCause: Large deployments of Monte Carlo may result in API timeouts.\nSolution: Lower page size for the parameter --montecarlo-graphql-page-size. The default is 250. Alternatively, adjust the --montecarlo-incident-lookback-days parameter to reduce the number of cataloged incidents.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98690-monte-carlo-and-the-data-world-collector.html"
  },
  {
    "title": "MS SQL Server and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector for the following data sources:\nMS SQL Server\nAzure Synapse Analytics: Note that the collector currently only supports SQL databases with dedicated SQL pools.\nThe collector supports basic authentication to Azure Synapse Analytics and MS SQL Server.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema, Created date, Modified date\nViews\nName, Description, SQL definition\nSchema\nIdentifier, Created date, Modified date\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nFunctions\nName, Description, Function Type\nStored Procedures\nName, Description, Stored Procedure Type\n\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe collector identifies, for every column in a View, the column(s) in other tables or views from which that view\u2019s column selects (sources) its data.\nThe collector traces these relationships from a View\u2019s columns to ultimate source Table columns across SQL expressions and subqueries.\nAdditionally, the collector establishes relationships between a View and any columns in source Tables that sort the rows in the View (via SQL ORDER BY), filter the rows in the View (via SQL WHERE and HAVING clauses), and aggregate the rows in the View (via SQL GROUP BY).\nNote that the collector currently does not establish view-to-table relationships in the JDBC collectors. This can be done transitively (e.g., in SPARQL) by noting the column-level relationships, since each column is associated with one and only one table or view.\nThe user you are using to run the collector needs at least SELECT ON DATABASE permission to access the metadata. Users need additional VIEW DEFINITION permission to harvest column-level lineage from views.\nTo create a user and set the permissions:\nCreate a new login <loginName> with password <password>.\nCreate a new user.\nGrant SELECT ON DATABASE permissions to harvest catalog resources such as tables, views, and columns.\nGrant VIEW DEFINITION permissions to harvest column-level lineage from views.\nTo find the fully qualified server name:\nNavigate to the Azure portal.\nNavigate to the Azure Synapse workspace you want to connect to.\nClick on Overview.\nThe full server name is listed as Dedicated SQL endpoint. For example <synapse-workspace>.sql.azuresynapse.net\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select SQL Server. Click Next.\nOn the Configure a new on premises Microsoft SQLServer Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe host name of the database server to connect to. For Azure Synapse, provide the server information you obtained here.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A, --all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if --schema is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nSelect this option and then specify the names of the database schema to be catalog.\nYes\n(if --all-schemas is not set)\nInclude information schema\n--include-information-schema\nwhen --all-schemas is specified, include the database's Information Schema in catalog collection (ignored if --all-schemas is not\u00a0specified).\nNo\nUsername\n-u=<user>\n--user=<user>\nThe username to use to make the JDBC connection.\nYes\nPassword\n-P=<password>\n--password=<password>\nSpecify this option to provide the password for the database on the command line.\nYes\nOn the next screen, set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDisable lineage collection\n--disable-lineage-collection\nSkip harvesting of intra-database lineage metadata.\nNo\nSQL Server extended properties collection flag\n--collect-extended-properties\nHarvest information about extended properties from SQL Server.\nNo\nEnable sample string values collection\n--sample-string-values\nEnable sampling and storage of sample values for\u00a0columns with string values.\nNo\nEnable column statistics collection\n--enable-column-statistics\nEnable harvesting of column statistics (this is\u00a0optional, and may greatly increase collector run time)\nNo\nServer environment\n-e=<environment>\n--environment=<environment>\nif your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs. it helps differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nTarget sample size for Profiling\n--target-sample-size=<targetSampleSize>\nTarget for number of rows to sample from tables.\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example, property1=value1;property2=value2\nNo\nOn the Finalize your Microsoft SQLServer Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate Configuration file ( YAML) or Command line arguments (CLI). Click\u00a0Next..\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Microsoft SQLServer Collector command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of the collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: Permissions are not set properly.\nSolution: Check that the user has permissions to retrieve view definitions.\nWithin SQL Server, run the following query:\nReplace <schemaName> and <viewName> with a known View in a Schema.\nThe result set should include the expected View.\nIf not, run the following query to grant View Definition to the user that the collector authenticates with.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98691-ms-sql-server-and-the-data-world-collector.html"
  },
  {
    "title": "MySQL and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to directly harvest metadata from your Reltio Master Data Management platform.\nThe MySQL collector supports basic authentication to MySQL.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, foreign), column size, column index\nTable\nName, description, primary key, schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, name, identifier, server, port, environment, JDBC URL\nFunction\nName, Description, Function Type\nStored Procedure\nName, Description, Stored Procedure Type\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nThe computer\u00a0must\u00a0have the appropriate JDBC driver on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nRun the following SQL statements to set up a new user with appropriate permissions to harvest from MySQL.\nCreate a new user:\nGrant the permissions to the user:\nMake sure you download the appropriate JDBC driver for MySQL on the machine from where you will run the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select MySQl. Click Next.\nOn the\u00a0MySQL Collector requires an additional driver file screen, set the JDBC driver directory. Click\u00a0Next.\nOn the Configure an on-premises MySQL Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following properties and click\u00a0Next. Use the Add property button to add multiple properties.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC properties\n--jdbc-property=\u00a0<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value pair.\nNo\nOn the\u00a0Finalize your MySQL Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generated a YAML file, click\u00a0Next.\nThe\u00a0MySQL command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98692-mysql-and-the-data-world-collector.html"
  },
  {
    "title": "Netezza and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest tables, views, and columns from Netezza.\nBefore you begin, make sure you have obtained the Netezza JDBC driver from IBM.\nThe Netezza collector supports username and password authentication.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, Data Type, Is Nullable, Default Value, Key type (Primary, foreign), Column size\nTable\nName, Description, Primary key, Schema\nViews\nName, Description, SQL definition\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTables\nColumn\nColumns\nTable, View\nViews\nColumn\nYou need an account with a role that has SELECT privileges on the resources that you want the collector to harvest metadata from. See the Netezza GRANT command documentation for details.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nDriver\nObtained the Netezza JDBC driver from IBM.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nWe use the Generic collector to generate the Command or YAML file for Netezza.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Generic. Click\u00a0Next.\nOn the Generic Collector requires an additional driver file screen, provide the location of the directory where the Netezza driver is placed. Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source= ${HOME}/dwcc-drivers\nThe location where you placed the JDBC driver.\nYes\nOn the\u00a0Configure a new on premises Generic Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nJDBC URL\n-j= <JDBC_URL>\n--jdbc-url=<JDBC_URL>\nThe JDBC URL for the database. Specify the location of the target Netezza database to connect to. This parameter accepts:\nServer - hostname, either IP or domain name. No trailing slashes or protocol (https://).\nPort - typically 5480\nDatabase - target Natezza database\nYes\nUsername\n-u= <user>\n--user= <user>\nThe username to use to connect to the database.\nYes\nPassword\n-P= <password>\n--password= <password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A  --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following advanced parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e= <environment>\n--environment= <environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D= <databseid>\ndatabase-id= <databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nOn the\u00a0Finalize your Generic Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Generic command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/130159-netezza-and-the-data-world-collector.html"
  },
  {
    "title": "Open API and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nThe collector expects Open API specification inputs (aka swagger files) that conform to Open API spec version 2.0. Open API specification 3.X is currently not supported.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nEndpoint\nName\nOperation\nName, description, Produces, Parameters, Responses\nParameter\nName, Description, Parameter Location, Parameter Type, Parameter Required\nResponse\nName, Description\nSchema\nName, Description, Schema Type\nSchema Property\nName, Description, Schema Property Type, Schema Format, Property Required, Schema Nullable\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nEndpoint\nOperations contained within Endpoint\nOperation\nEndpoint containing Operation\nParameters associated with Operation\nResponses associated with Operation\nParameter\nOperation associated with Parameter\nResponse\nOperation associated with Response\nSchema that has Response\nSchema\nResponses that are defined by Schema\nSchema Properties that are contained within Schema\nSchema Property\nSchema that contains Schema Property\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select OpenAPI. Click Next.\nOn the Configure a new on premises OpenAPI Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nPath or URL to Open API Swagger File\n-J=<jsonSpecification>\n--json-spec=<jsonSpecification>\nOpenAPI json specification (swagger file) file path or URL.\nYes\nOn the next screen, set the following advanced options and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nOpenAPI URL Base path\n--openapi-base-path=<apiBasePath>\nAPI base path. Overrides value in specification.\nNo\nOpenAPI Host\n--openapi-host=<apiHost>\nAPI host. Override value in specification.\nNo\nOn the Finalize your OpenAPI Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe OpenAPI command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98693-open-api-and-the-data-world-collector.html"
  },
  {
    "title": "Oracle and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Oracle tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nOracle database 12.1.0.1 JDBC driver version available here.\nThe Oracle collector supports basic authentication to Oracle.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nRun the following SQL statements to set up a new user with appropriate permissions to harvest from Oracle.\nCreate a new user.\nGrant permissions to the user.\nUpdate <schema> and <table_name> for each schema, table, or view you want to harvest.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nOracle database 12.1.0.1 JDBC driver available here. The computer should have the appropriate JDBC driver on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Oracle. Click Next.\nOn the Oracle Collector requires an additional driver file screen, set the following.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\nsource=${HOME}/dwcc-drivers\nThe driver required to run the collector. You must download this driver yourself and place it in a directory on the machine that will run the collector.\nYes\nOn the Configure an on-premises Oracle Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to make the JDBC connection.\nYes\nPassword\n-P=<password>\n--password=<password>\nSpecify this option to provide the password for the database on the command line.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes (if --schema is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if --all-schema is not set)\nOn the next screen, set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDatabase is an Oracle SID\n--oracle-sid-mode\nIndicates that the database is actually an Oracle SID.\nNo\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databaseid>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example, property1=value1;property2=value2\nNo\nOn the Finalize your Oracle Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file (YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Oracle command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\n\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98694-oracle-and-the-data-world-collector.html"
  },
  {
    "title": "PostgreSQL and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for PostgreSQL tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to PostgreSQL.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, foreign), column size, column index\nTable\nName, description, primary key, schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, name, identifier, server, port, environment, JDBC URL\nFunction\nName, Description, Function Type\nStored Procedure\nName, Description, Stored Procedure Type\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe collector identifies, for every column in a View, the column(s) in other tables or views from which that view\u2019s column selects (sources) its data.\nThe collector traces these relationships from a View\u2019s columns to ultimate source Table columns across SQL expressions and subqueries.\nAdditionally, the collector establishes relationships between a View and any columns in source Tables that sort the rows in the View (via SQL ORDER BY), filter the rows in the View (via SQL WHERE and HAVING clauses), and aggregate the rows in the View (via SQL GROUP BY).\nNote that the collector currently does not establish view-to-table relationships in the JDBC collectors. This can be done transitively (e.g., in SPARQL) by noting the column-level relationships, since each column is associated with one and only one table or view.\nRun the following SQL statements to set up a new user with appropriate permissions to harvest from PostgreSQL\nCreate a new role.\nRun the following SQL statement to grant permissions to the new role for each database and schema.\nCreate a new user.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select PostgreSQL. Click Next.\nOn the Configure a new on premises PostgreSQL Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if --schema is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog (will catalog user's default schema if not specified)\nYes\n(if --all-schemas is not set)\nInformation schema\n--include-information-schema\nwhen --all-schemas is specified, include the database's Information Schema in catalog collection (ignored if --all-schemas is not\u00a0specified).\nNo\nUsername\n-u=<user>\n--user=<user>\nThe username to use to make the JDBC connection.\nYes\nPassword\n-P=<password>\n--password=<password>\nSpecify this option to provide the password for the database on the command line.\nYes\nOn the next screen, set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDisable lineage collection\n--disable-lineage-collection\nSkip harvesting of intra-database lineage metadata.\nNo\nEnable sample string values collection\n--sample-string-values\nTo enable harvesting of sample values and histograms for columns containing string data.\nNo\nEnable column statistics collection\n--enable-column-statistics\nTo enable harvesting of column statistics (i.e., data profiling).\nNo\nTarget sample size for column statistics\n--target-sample-size=<targetSampleSize>\nTo control the number of rows sampled for computation of column statistics and string-value histograms. For example, to sample 1000 rows, set the parameter as: --target-sample-size=1000\nNo\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs. It helps differentiate it from other environments.\nNo\nDatabase ID\n\n-D=<databseId>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database. This is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database.\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection.\nNo\nOn the Finalize your PostgresSQL Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate the Configuration file (YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe PostgresSQL command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard.before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98695-postgresql-and-the-data-world-collector.html"
  },
  {
    "title": "Power BI and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata from Power BI. Users can then:\nDiscover Power BI reports and dashboards across your enterprise\u2019s Power BI workspaces.\nPerform impact analysis to understand how changes to upstream data sources impact Power BI reports.\nThe collector supports Power BI Cloud API v 1.0.\nThere are two separate ways to authenticate to Power BI:\nService principal\nUser and password\nThe collector will harvest metadata for all Power BI apps and workspaces to which the supplied account has access.\nThe collector catalogs the following information.\nObject\nInformation collected\nWorkspaces\nTitle\nApps\nTitle\nReports\nTitle, External URL, Embed URL\nReport Pages\nTitle\nDashboards\nTitle, External URL, Embed URL\nDashboard tiles\nTitle, Embed URL\nData Sources\nTitle, Data source type\nDatasets\nTitle, External URL\nDataflows\nTitle, Last modified, Description\nPower BI Tables (Datasets and Dataflows)\nTitle, Is hidden, Description\nPower BI Columns\nTitle, Data type, Column type, Is hidden, Expression\nFile\nFile path, File name\nFile directory\nDirectory path\nBy default, the data.world catalog will include catalog pages for the resource types below. Each catalog page will have a relationship to other related resource types. Note that the catalog presentation and relationships are fully configurable, so these will list the default configuration.\nResource page\nRelationship\nPower BI Column\nPower BI Table\nData source\nApp, dataset, dataflow\nTile\nDashboard, report, dataset\nDashboard\nTile, workspace\nReport\nTile, Workspace, report pages, dataset\nReport Pages\nReport\nDataset\nTile, workspace, report, table, data source, Dataflow\nWorkspace\nReport, dataset, dataflow, dashboard\nDataflow\nWorkspace, table, dataset, data source\nPower BI Table\nDataset, Dataflow, Power BI Column\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the Power BI collector.\nObject\nLineage available\nReport\nAssociated Dataset\nDashboard Tile\nAssociated Dataset\nDataset\nAssociated Dataflow, Dashboard Tile, Report, and Dataset\nDataflow\nAssociated Column, Dataset, and Dataflow\nNote: The collector is able to harvest lineage from\u00a0Power BI expressions which use parameters in place of database server name, schema name, database table, or database name.\nColumn\nAssociated columns that the column sources its data from or calculates its values from\nNote: The collector is able to harvest lineage\u00a0from Power BI expressions which use parameters in place of database server name, schema name, database table, or database name.\nPower BI does not include user workspace when using Service Principal authentication.\nIf both workspace-include and workspace-exclude are specified, included workspaces take precedence.\nDataflows require the user/service principal to be added to the workspace with at least contributor access. When authenticating with username/password, the app registration needs to have least Dataflow.Read. All permissions in API permissions.\nOnly administrators of the tenant can grant admin consent.\nThere are two separate ways to authenticate to Power BI:\nService principal\nUser and password\nThis section will walk you through the process for both authentication types.\nTo register a new application:\nGo to the Azure Portal.\nSelect Azure Active Directory.\nClick the App Registrations option in the left sidebar.\nClick New Registration and enter the following information:\nApplication Name: DataDotWorldPowerBIApplication\nSupported account types: Accounts in this organizational directory only\nClick Register to complete the registration.\nTo create a Client Secret:\nGo to the Azure Portal.\nOn the application page, select Certificates and Secrets.\nClick on Secret and add a description.\nSet the expiration to Never.\nClick on Create, and copy the secret value. You will use this value while setting the parameters for the collector.\nTo get the Client ID from the Azure portal:\nClick on the Overview tab in the left sidebar of the application home page.\nCopy the Client ID from the Essentials section. You will use this value while setting the parameters for the collector.\nPerform this task only if you are using user and password for authentication.\nTo add permissions:\nClick on API Permissions, and select Add Permission.\nSearch for the Microsoft Graph and select the following permissions:\nApplication permission: Application.Read.All\nDelegated permission: User.Read (assigned by default)\nSearch for the Power BI service, and click on Delegated permissions. Select the following permissions:\nApp.Read.All\nDashboard.Read.All\nDataflow.Read.All\nDataset.Read.All\nReport.Read.All\nTenant.Read.All\nWorkspace.Read.All\nClick on the Grant Admin consent button, which is located next to the Add permission button. This allows the data.world collector to run as a daemon without having to ask the user permission on every crawler run.\nOnly administrators of the tenant can grant admin consent.\nPerform this task only if you are using the service principal for authentication.\nIf you are using service principal as your authentication type, ensure that you enable service principals to use the Power BI APIs. For detailed instructions for doing this task, please see this documentation.\nSet up metadata scanning to enable access to the detailed data source information (like tables and columns) provided by Power BI through the read-only admin APIs.\nBefore metadata scanning can be run over an organization's Power BI workspaces, it must be set up by a Power BI administrator. Setting up metadata scanning involves two steps:\nIf using service principals for authentication, enabling service principal authentication for read-only admin APIs.\nEnabling tenant settings for detailed dataset metadata scanning.\nFor details about doing this task, please see this documentation.\nTo find the tenant ID, click the question mark in the Power BI app and then choose About Power BI.\nThe tenant ID can be found at the end of the Tenant URL. You will use this value while setting the parameters for the collector.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Power BI. Click Next.\nOn the Configure an on-premises Power BI screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nSelect from one of the following authentication options.\nYes\nOption 1: Authenticate using Azure Username & Password\nAzure Username\n--azure-username= <username>\nAzure Active Directory username for Power BI Cloud authentication.\nYes\nAzure Password\n--azure-password= <password>\nAzure Active Directory password for Power BI Cloud authentication. By default this is set as an environment variable. ${DW_AZURE_PASSWORD}\nYes\nAzure Tenant ID\n--azure-tenantid= <tenantId>\nSet this if you want to specify the Azure tenant ID while using the user name and password authentication.\nNo\nOption 2: Authenticate using Azure tenant ID (Service principal)\nAzure Tenant ID\n--azure-tenantid= <tenantId>\nAzure Active Directory application tenant ID for the Power BI app.\nYes\nClient ID and Client secret\nAzure Client ID\n--azure-clientid= <clientId>\nAzure Active Directory application client ID for Power BI app.\nYes\nAzure Client Secret\n--azure-secret= <clientSecret>\nAzure Active Directory application client secret for Power BI app. By default this is set as an environment variable ${DW_AZURE_SECRET}.\nYes\nOn the next screen, set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nSkip harvesting lineage metadata\n--disable-expression-lineage\nSkip harvesting lineage metadata from Power BI source expressions.\nNo\nCatalog contents of user's My Workspace\n--include-user-workspace\n--user-workspace-include\nCatalog contents of user's My Workspace in Power BI (Default is to skip the user's workspace).\nNote: This parameter is only supported when you are using username and password authentication type. It is not supported for service principal authentication.\nNo\nPowerBI catalog all workspaces and apps\n--all-workspaces-and-apps\nCatalog all workspaces and apps in a tenant,\u00a0rather than only the workspaces and apps the credentials used have explicit access to. This option only works if the credentials used have admin privileges.\nNo\nInclude Power BI Workspace(s)\n--include-workspace=  <includedWorkspaceNames>\n--workspace-include= <includedWorkspaceNames>\nInclude the specified Power BI workspace's contents\u00a0in the catalog. Use the parameter multiple times\u00a0for multiple workspaces. For example,  --workspace-include=\"workspaceA\" --workspace-include=\"workspaceB\"\nNo\nExclude Power BI Workspaces\n--exclude-workspace=<excludedWorkspaceNames>\n--workspace-exclude= <excludedWorkspaceNames>\nExclude the specified Power BI workspace's contents from the catalog. Use the parameter multiple\u00a0times for multiple workspaces. For example, --workspace-exclude=\"workspaceA\"\u00a0--workspace-exclude=\"workspaceB\". If both --include-workspace and --exclude-workspace are used, --include-workspace takes precedence.\nNo\nOn the Finalize your Power BI Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe PowerBI collector command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nYou can add the following parameter to the\u00a0Command\u00a0or\u00a0YAML file\u00a0to use this additional feature.\nParameter name\nDescription\nRequired?\n--max-parseable-expression-length=<maxParseableExpressionLength>\nSet the maximum number of characters in a Power BI\u00a0expression that will be parsed for lineage\u00a0metadata. Expressions longer than this will be skipped. Default is 32000.\nNo\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\n\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nErrors observed: The following errors are observed:\nAPI error encountered getting Datasources. HTTP error code 403 Forbidden.\nError invoking Power BI API to authenticate. HTTP status: 401.\nCause: There was an authorization issue with Power BI related to permissions.\nSolution: Review the authentication methods used to connect to Power BI and ensure you have completed all the set up instructions.\nCause: Unable to parse expression for table {Table name} because it has no expression, so also unable to determine the source. The source table cannot be cataloged.\nSolution: Enable access to the detailed data source information (like tables and columns) provided by Power BI through the read-only admin APIs. For details about doing this task, please see this documentation.\nCause: The collector parser hit a stack size limit due to a complex SQL statement or DAX expression.\nSolution: Add the -e DWCC_JVM_OPTIONS=\"-Xss2m\" parameter to the command to increase the stack size. For example, the command will look like: docker run -it --rm -e DWCC_JVM_OPTIONS=\"-Xss2m\". This sets the stack size to 2 MB.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98696-power-bi-and-the-data-world-collector.html"
  },
  {
    "title": "Power BI Gov and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nThe data.world Collector harvests metadata from your source system. Please read over the data.world Collector FAQ to familiarize yourself with the Collector.\nThe collector supports Power BI Cloud API v 1.0. The collector only supports 365 Government Community Cloud (GCC) instance.\nAuthenticate to Power BI Gov using Service principal.\nThis section walks you through the process of setting up the authentication.\nImportant things to note:\nThe Collector currently uses Azure Active Directory to authenticate to the Power BI Cloud API. You will need to create an Application Registration in Azure AD, enable the Power BI API authentication for it, and create a client secret.\nThe collector harvests metadata for all Power BI apps and workspaces to which the supplied account has access.\nTo register a new application:\nGo to the Azure Portal.\nSelect Azure Active Directory.\nClick the App Registrations option in the left sidebar.\nClick New Registration and enter the following information:\nApplication Name: DataDotWorldPowerBIApplication\nSupported account types: Accounts in this organizational directory only\nClick Register to complete the registration.\nTo create a Client Secret:\nGo to the Azure Portal.\nOn the application page, select Certificates and Secrets.\nClick on Secret and add a description.\nSet the expiration to Never.\nClick on Create, and copy the secret value.\nTo get the Client ID from the Azure portal:\nGo to the Azure Portal.\nClick on the Overview tab in the left sidebar of the application home page.\nCopy the Client ID from the Essentials section.\nEnable access to the detailed data source information (like tables and columns) provided by Power BI through the read-only admin APIs. For details about doing this task, please see this documentation.\nPerform this task only if you are using the service principal for authentication.\nIf you are using service principal as your authentication type, ensure that you enable service principals to use the Power BI APIs. For detailed instructions for doing this task, please see this documentation.\nThe information cataloged by the collector includes:\nObjects:\ndata source\nworkspace\ndashboard\ndashboard tile\nreport\nreport page\nRelationships:\nworkspace to analysis\ntile to report\ntile to dashboard\nreport to data source\nreport to page\ndata source to report\ndashboard tile to data source\nanalysis to workspace\ndashboard to tile\ntile to dashboard\nreport to tile\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe Power BI collector identifies the datasets that reports and dashboard tiles source their data from.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0PowerBI Gov. Click\u00a0Next.\nOn the\u00a0Configure an on-premises PowerBI\u00a0Gov screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nAzure Tenant Id\n--azure-tenantid= <tenantId>\nAzure Active Directory application tenant id for Power BI app. To find the tenant ID, click the question mark in the\u00a0Power BI\u00a0app and then choose\u00a0About Power BI. The tenant ID will be found at the end of the Tenant URL.\nYes\nAzure Client ID\n--azure-clientid= <clientid>\nAzure Active Directory application client ID for Power BI app.\nYes\nAzure Client Secret\n--azure-secret= <clientSecret>\nAzure Active Directory application client secret for Power BI app. By default this is set as an environment variable\u00a0${DW_AZURE_SECRET}.\nYes\nOn the next screen, set the following advanced properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nCatalog contents of user's My Workspace\n--include-user-workspace\n--user-workspace-include\nThis parameter is not in use and should not be set.\nNo\nInclude PowerBI Workspaces\n--include-workspace= <includedWorkspaceNames>\n--workspace-include= <includedWorkspaceNames>\nInclude the specified Power BI workspace's contents\u00a0in the catalog. Use the parameter multiple times\u00a0for multiple workspaces. For example,\n--workspace-include=\"workspaceA\"\n--workspace-include=\"workspaceB\"\nNo\nExclude PowerBI Workspaces\n--exclude-workspace= <excludedWorkspaceNames>\n--workspace-exclude= <excludedWorkspaceNames>\nExclude the specified Power BI workspace's contents from the catalog. Use the parameter multiple\u00a0times for multiple workspaces. For example,\n--workspace-exclude=\"workspaceA\"\n--workspace-exclude=\"workspaceB\"\nIf both\u00a0--include-workspace\u00a0and\u00a0--exclude-workspace\u00a0are used,\u00a0--include-workspace takes precedence.\nNo\nOn the\u00a0Finalize your PowerBI Gov Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector.Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nSample YAML file.\nThe PowerBI Gov command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nYou can add the following parameter to the\u00a0Command\u00a0or\u00a0YAML file\u00a0to use this additional feature.\nParameter name\nDescription\nRequired?\n--max-parseable-expression-length= <maxParseableExpressionLength>\nSet the maximum number of characters in a Power BI\u00a0expression that will be parsed for lineage\u00a0metadata. Expressions longer than this will be skipped. Default is 32000.\nNo\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98697-power-bi-gov-and-the-data-world-collector.html"
  },
  {
    "title": "Presto and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Presto tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Presto.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Presto. Click Next.\nOn the Configure an on-premises Presto Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following advanced properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value. Separate the name=value pairs with a semicolon (;). For example, property1=value1;property2=value2\nNo\nOn the Finalize your Presto Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file (YAML) or Command line arguments (CLI). Click Next\nYou must ensure that you have set up these environment variables and directories\u00a0before you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Presto command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98698-presto-and-the-data-world-collector.html"
  },
  {
    "title": "Redshift and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Redhsift tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Redshift.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, foreign), column size, column index\nTable\nName, description, primary key, schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, name, identifier, server, port, environment, JDBC URL\nFunction\nName, Description, Function Type\nStored Procedure\nName, Description, Stored Procedure Type\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe collector identifies, for every column in a View, the column(s) in other tables or views from which that view\u2019s column selects (sources) its data.\nThe collector traces these relationships from a View\u2019s columns to ultimate source Table columns across SQL expressions and subqueries.\nAdditionally, the collector establishes relationships between a View and any columns in source Tables that sort the rows in the View (via SQL ORDER BY), filter the rows in the View (via SQL WHERE and HAVING clauses), and aggregate the rows in the View (via SQL GROUP BY).\nNote that the collector currently does not establish view-to-table relationships in the JDBC collectors. This can be done transitively (e.g., in SPARQL) by noting the column-level relationships, since each column is associated with one and only one table or view.\nMake sure you download the appropriate JDBC driver for Redshift on the machine from where you will run the collector.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nThe computer should have the appropriate JDBC driver on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Redshift. Click Next.\nOn the Redshift Collector requires an additional driver file screen, set the following and Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\nsource=${HOME}/dwcc-drivers\nThe driver required to run the collector. You must download this driver yourself and place it in a directory on the machine that will run the collector.\nYes\nOn the Configure an on-premises Redshift Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nInformation schema\n--include-information-schema\nWhen\u00a0--all-schemas\u00a0is specified, include the database's Information Schema in catalog collection (ignored if\u00a0--all-schemas\u00a0is not\u00a0specified).\nNo\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nOn the next screen, set the following optional properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDisable lineage collection\n--disable-lineage-collection\nSkip harvesting of intra-database lineage metadata.\nNo\nEnable sample string values collection\n--sample-string-values\nto enable harvesting of sample values and histograms for columns containing string data\nNo\nEnable column statistics collection\n--enable-column-statistics\nTo enable harvesting of column statistics (i.e., data profiling)\nNo\nTarget sample size for Profiling\n--target-sample-size=<targetSampleSize>\nTo control the number of rows sampled for computation of column statistics and string-value histograms. For example, to sample 1000 rows, set the parameter as: --target-sample-size=1000\nNo\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection.\nNo\nOn the\u00a0Finalize your Redshift Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML)\u00a0or\u00a0Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Redshift command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98699-redshift-and-the-data-world-collector.html"
  },
  {
    "title": "Reltio and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata from your Reltio Master Data Management platform. The collector harvests metadata from the Reltio configuration file, which is exported from the Reltio Master Data Management platform.\nThe Reltio metadata collector runs against a static configuration file which contains the following information:\nClasses (what Reltio calls entity types) and properties (what Reltio calls attributes). Essentially, a type structure that expresses key business objects in your domain.\nAssociation classes (what Reltio calls relation types) that are classes that express relationships between other entities. For example, Employee entity has a relationship to Company entity, and the relationship has start date and end date attributes.\nInheritance relationships between entity types.\nSpecial objects called survivorship groups that express how conflicting data for a given attribute is resolved (that is, if two sources of data conflict, take the one that\u2019s most recent. or take the value from System A over System B.) Basically, a directed graph of objects that is associated with an attribute on an entity.\nRoles that are groupings of entities to express transient typing. For instance, a Person and an Organization can both be in the role of Customer. But a given person can be a customer today and an employee tomorrow.\nAttributes (which can be defined apart from entities, allowing them to be reused) that have properties like type, name, label, hidden, faceted, required, searchable.\nData cleansers that can transform the values of attributes from one form to another. A common example is a cleanser for street address that translates ST, St., and Str to Street. An Entity Type is configured with zero to many cleansers that can be arranged in chains.\nMatchers for entities that determine if two entities are the same. So two person entities might be the same if their SSN attributes are equal or if their cleansed names are fuzzy-matched within a certain threshold.\nSurvivorship strategies that express how to merge data from two entities that have been judged to be the same. Basically, which attribute values from the merged entities \u201cwin\u201d as the value of the attribute for the final, combined entity.\nGraph Types that appear to define specific hierarchies of relationships with distinct semantic meaning.\nInteraction Types that define events in which member Entity Types participate. Each interaction/event can have its own attributes.\nThe Reltio collector focuses on Entity Types, Attributes, Roles, and Relationships and does the following:\nIdentifies the matchers and cleansers assigned to each class by name.\nWorks on a static configuration.\nIdentifies the first cleanser in a chain, if there is a multi-level chain.\nIdentifies the survivorship strategy for each attribute (property) by name.\nIt does not produce glossary terms.\nBefore you begin running the collector, you need to export the configuration file.\nFrom the Reltio Console, navigate to UI Modeler. Click Import / Export UI Config Files.\nExport the Reltio UI Configuration file. You will use this file while running the collector. See the Relito documentation for more details.\nPlace the exported file in a directory on the machine from where you plan to run the collector.\nMake sure the user running the collector has read access to the exported Reltio configuration file.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Reltio. Click Next.\nOn the Configure a new on premises Reltio Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nPath to Reltio config file\n--configuration= <reltionConfiguration>\nPath to the Reltio configuration JSON file.\nYes\nOn the Finalize your Reltio Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nSample YAML file.\nThe Reltio command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98700-reltio-and-the-data-world-collector.html"
  },
  {
    "title": "Salesforce and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Salesforce tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Salesforce.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC Driver\nAppropriate driver for Salesforce. Please contact data.world Support team to get the driver.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Salesforce Collector requires an additional driver file screen, set the following property. Click Next.\nOn the Configure an on-premises Salesforce Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nUsername\n-u=<user>\n--user=<user>\nThe username to use to make the JDBC connection.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe password of the user.\nYes\nSalesforce Security Token\n-T=<securityToken>\n--security-token=<securityToken>\nPersonal security token for the provided salesforce credentials.\nYes\nOn the next screen set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nSalesforce Login URL\n--salesforce-login-url=<login-url>\nURL to the Salesforce server used for logging in. The default, if not specified is, https://login.salesforcecom/services/Soap/c/51.0. Any custom login URLs should be of similar format, that is: xyz.salesforce.com/services/Soap/c/API_VERSION.\nNo\nSet if configuration is running against sandbox\n--use-sandbox=true_or_false\nSelect this option if the Salesforce credentials are for a sandbox environment.\nNo\nOn the\u00a0Finalize your Salesforce Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Salesforce command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98701-salesforce-and-the-data-world-collector.html"
  },
  {
    "title": "SAP HANA and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest tables, views, and columns from SAP HANA.\nBefore you begin, contact data.world support team to obtain the SAP HANA driver.\nThe SAP HANA collector supports username and password authentication.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, Data Type, Is Nullable, Default Value, Key type (Primary, foreign), Column size\nTable\nName, Description, Primary key, Schema\nViews\nName, Description, SQL definition\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTables\nColumn\nColumns\nTable, View\nViews\nColumn\nYou need an account with a role that has SELECT privileges on the resources that you want the collector to harvest metadata from. See SAP HANA Database Roles.\nYou can set up a role with either of the following privileges:\nSystem privileges (CATALOG READ)\nObject privileges (SELECT METADATA)\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nDriver\nContact data.world support team to obtain the SAP HANA driver.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nWe use the Generic collector to generate the Command or YAML file for SAP HANA.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Generic. Click\u00a0Next.\nOn the Generic Collector requires an additional driver file screen, provide the location of the directory where the SAP HANA driver is placed. Click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\n--mount type=bind,source= ${HOME}/dwcc-drivers\nThe location where you placed the JDBC driver.\nYes\nOn the\u00a0Configure a new on premises Generic Collector\u00a0screen, set the following properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nJDBC URL\n-j= <JDBC_URL>\n--jdbc-url=<JDBC_URL>\nThe JDBC URL for the database. Specify the location of the target SAP HANA database to connect to. This parameter accepts:\nServer - SAP HANA hostname. It can be either the IP or domain name. No trailing slashes or protocol (https://).\nPort for SAP HANA server. It is typically 30015.\nDatabase - target SAP HANA database\nYes\nUsername\n-u= <user>\n--user= <user>\nThe username to use to connect to the database.\nYes\nPassword\n-P= <password>\n--password= <password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A  --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following advanced parameters and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e= <environment>\n--environment= <environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D= <databseid>\ndatabase-id= <databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nOn the\u00a0Finalize your Generic Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe Generic command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes\n(required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/130160-sap-hana-and-the-data-world-collector.html"
  },
  {
    "title": "Snowflake and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to directly harvest metadata from Snowflake data warehouse including tabular objects, policies, and tags, as well as functions and stored procedures from Snowpark. You can also harvest column statistics and table query counts.\nThe collector supports the following authentication methods:\nUsername and password authentication. For details, see the Snowflake documentation.\nUsername and Key pair authentication. For details, see the Snowflake documentation.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, Data Type, Is Nullable, Default Value, Key type (Primary, foreign)\nTable\nName, Description, Primary key, Schema\nViews\nName, Description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nUser-defined functions\n(this feature is a part of Snowpark)\nName, Description, Owner, Function Type, Signature, Definition, Return Type, Language, Packages\nStored Procedures\n(this feature is a part of Snowpark)\nName, Description, Owner, Signature, Stored Procedure Type, Definition, Return Type, Language, Packages\nProfiling and sampling specific information\nIf you include the profiling and sampling specific parameters while running the collector, the following additional information is harvested for Columns.\nThe user/role must have read access to data to be able to harvest profiling information (column statistics).\nObject\nInformation cataloged\nColumns\nDistinct values, Non-null count, Integer value (min, max, avg), Decimal value (min, max, avg), String value (min, max), String length (min, max, avg)\nCataloging Snowflake Tags and Policies\nFollowing additional information is cataloged when you run the collector with the --policy-collection and --tag-collection parameters.\nObject\nInformation cataloged\nColumns\nSnowflake masking policies\nSnowflake Tags\nName\nSnowflake Tag Values\nName, Value\nSnowflake Masking Policies\nName, Date created, Snowflake tags, Owner, Type, Policy body, Schema, Database\nSnowflake Row access policies\nName, Date created, Snowflake tags, Owner, Type, Policy body, Schema, Database\nCataloging Snowflake table query counts\nFollowing additional information is cataloged when you run the collector with the --table-usage-collection and --table-usage-lookback-days parameters.\nObject\nInformation cataloged\nTable\nQuery Count\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nSnowflake Row access policies\nTables\nSnowflake Masking Policies\nColumns, Tag value\nTable\nColumns, Tag value\nSnowflake Tags\nTag value\nColumns\nTable, Tag Value\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the Snowflake collector.\nNote that the collector traces these relationships from a View\u2019s columns to ultimate source Table columns across SQL expressions and subqueries.\nObject\nLineage available\nColumn in view\nThe collector identifies the associated column in an upstream view or table:\nWhere the data is sourced from\nThat sort the rows via ORDER BY\nThat filter the rows via WHERE/HAVING\nThat aggregate the rows via GROUP BY\nUser-defined function\nA Function and:\nViews referenced in Function\nTables referenced in Function\nFunctions referenced in Function\nA View and:\nFunction referencing View\nThe Snowflake collector harvests Snowflake object tags, Snowflake tag-based masking policies, Snowflake masking policies, and Snowflake row access policies. This information enhances the data governance experience to discover all tags and policies applied across their Snowflake tables and columns in a data catalog\nIn the following example you can see how a Snowflake Tag-Based Masking Policy applied to sensitive data columns: routing numbers, bank name, and bank account number. In this view, you can also see the associated tag (Classification:confidential), and technical details about the policy, like the Policy Body which explains how the Policy works.\nFeature\nDetails\nSnowflake object tags\nTags enable data stewards to track sensitive data for compliance, discovery, protection, and resource usage use cases through either a centralized or decentralized data governance management approach. For details, see Snowflake documentation.\nSnowflake masking policies\nMasking policies define what data is masked and not shown to users allowing authorized users to access sensitive data at query runtime. For details, see the Snowflake documentation.\nSnowflake tag based masking policies\nWhen a masking policy is applied to a Snowflake tag, the columns with the same tag are automatically protected based on the conditions set in the masking policy. For details, see the Snowflake documentation.\nSnowflake row access policies\nAccess policies are applied to rows of data, that is, who can see the rows of data, and masking policies define what data is masked and not shown to users. For details about this see the Snowflake documentation.\nTable query count\nFor each table, the Snowflake collector computes percentile popular and a query count based on the user access history table in Snowflake. This information can then be used to assign popularity ranking to Snowflake tables in data.world. For details about this see the Snowflake documentation.\nWhen this data is harvested and is displayed in data.world, you see the following information in your catalog: Snowflake masking policies, Snowflake row access policies, Snowflake tag values, and Snowflake tags.\nWhen you browse to a table harvested from Snowflake, you will notice that the appropriate policies are applied to the tables. In the following example, the policy and tag values are applied to the Order table.\nUsers can click through to the tag values and policies to see their details.\nNote that users can also use the Search feature in the application to locate all these resources easily.\n\nThe collector supports the following authentication methods:\nUsername and password authentication. For details, see the Snowflake documentation.\nUsername and Key pair authentication. For details, see the Snowflake documentation.\nWe recommend you create a dedicated Snowflake user for running the collector. You will need specific permissions to create this new user.\nYou need at least USERADMIN or higher permissions to create a new user. See the Snowflake user creation documentation. If you plan to modify the settings of an existing user, you will need OWN permissions to make such updates. See the Snowflake Admin User Management documentation\nAdditionally, to create key pair authentication for this user, you need a minimum of SECURITYADMIN to alter the user settings. See the Snowflake documentation.\nTo set permissions:\nIn the following query, replace <warehouse_name>, <database_name>, and <password>\nThe following query grants permissions for all Snowflake schemas, tables, external tables, and views to the DDW_ACCOUNT_ROLE. The scope of the grant statements can be modified to grant permissions to specified objects which would limit the metadata cataloged from Snowflake.\nRun all the queries.\nTest the collector using the DDW_ACCOUNT user and the password you filled in.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the command or YAML file for running the collector from Windows or Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Snowflake. Click Next.\nOn the Configure a new on premises Snowflake Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nSnowflake is case-sensitive so database and schema names need to be specified with the same case as they are in Snowflake.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nSnowflake role\n-r=<role>\n--role=<role>\nThe role used to execute the query.\nYes\nAuthentication\nSelect from one of the following options:\nAuthenticate with a username & password\nAuthenticate using a private key file\nYes\nAuthenticate with a username & password\nUsername\n-u=<user>\n--user=<user>\nThe username used to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database. Default value is an environment variable ${DW_SNOWFLAKE_PASSWORD}\nYes\nAuthenticate using a private key file\nLocation of private key file\n--private-key-file=<PrivateKeyFile>\nThe private key file to use for authentication. You will need to mount a local directory containing the private-key-file with a directory on the Docker container. The --private-key-file should be the path to the directory on the container with the private key. For instance, for a private-key-file called rsa_key.p8, add to the CLI command --mount type=bind,source=/tmp,target=/snowflake-key and set --private-key-file=\"/snowflake-key/rsa_key.p8\"\nYes\nEnv Variable Password of your private key file\n--private-key-file-password=<PrivateKeyFilePassword>\nThe password for the private key file, if the key\u00a0is encrypted and a password was set. Default value is an environment variable ${DW_SNOWFLAKE_PK_PASSWORD}.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A\n--all-schemas\nCatalog all schemas to which the user has access.\nYes\n(if --schema is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nSelect this option and then specify the names of the database schema to be catalog.\nYes\n(if --all-schemas is not set)\nInformation schema\n--include-information-schema\nSet this if you want to also collect this Information schema for the database. You can set this option only when you have selected the Collect all schema  (--all-schemas) option.\nNo\nOn the next screen, set the following optional properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nDisable lineage collection\n--disable-lineage-collection\nSkip harvesting of intra-database lineage metadata.\nNo\nCollect Snowflake tag information\n--tag-collection\nHarvest information about Snowflake tags\nNo\nEnable sample string values collection\n--sample-string-values\nto enable harvesting of sample values and histograms for columns containing string data\nNo\nCollect Snowflake policy information\n--policy-collection\nHarvest information about Snowflake masking and row-access policies\nNo\nCollect Snowflake table usage information\n--table-usage-collection\nHarvests metadata about Snowflake table usage in queries (popularity). Calculates, for each table in the database being harvested, the percentage of tables in the database that have been queried no fewer times than the subject table.\nNo\nTable usage lookback days\n--table-usage-lookback\nNumber of days in the past at which to begin harvesting table usage (default=7 days).\nNo\nEnable column statistics collection\n--enable-column-statistics\nTo enable harvesting of column statistics (i.e., data profiling)\nNo\nTarget sample size for Profiling\n--target-sample-size=<targetSampleSize>\nTo control the number of rows sampled for computation of column statistics and string-value histograms. For example, to sample 1000 rows, set the parameter as: --target-sample-size=1000\nNo\nSnowflake warehouse\n-w=<warehouse>\n--warehouse=<warehouse>\nThe Snowflake warehouse to use when connecting\u00a0(user's assigned default if not specified)\nNo\nServer environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is localhost, use this to give a friendly name to the environment in which your database server runs. It helps differentiate it from other environments.\nNo\nDatabase ID\n-D=<databaseId>\n--database-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database. This is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database.\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection.\nNo\nOn the Finalize your Snowflake Collector configuration screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a Configuration file( YAML) or Command line arguments (CLI). Click Next.\nYou must ensure that you have set up these environment variables and directories before you run the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click Done. If you are generating a YAML file, click Next.\nThe Snowflake command screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nThe following eror message is observed in the error logs: WARN: Access to the snowflake.account_usage database/schema is denied. To harvest Snowflake tag, policy, and/or table usage information into the catalog, use a role with adequate permissions (current role is X).\nCause: To harvest policies, tags, and table usage, the role that the collector uses must have read access\u00a0 to the snowflake database. The role the collector uses does not have permissions to the snowflake database.\nSolution: Run the recommended statement to grant permissions to the snowflake database.\nDescription: The account and role used to run the collector does not have permissions to a specific table.\nSolution: Run the following query to check that the account and its role used for the collector has permissions to the specific table.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98702-snowflake-and-the-data-world-collector.html"
  },
  {
    "title": "SQL Server Reporting Services (SSRS) and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to discover SQL Server Reporting Services (SSRS) metadata, such as, reports, linked reports, data sources, KPI, folders, and datasets across your enterprise.\nThe collector supports SQL Server Reporting Services\u00a02022 configured for Native mode. SSRS running in Sharepoint Mode is not supported.\nThe collector supports Basic authentication for SSRS.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nReport\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Data Sources, Favorite, Content Type, Policies\nLinked Report\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Content Type, Policies\nData Source\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Parent Folder ID, Connection String, Connection String Overridden, Data Source Type, Credential Retrieval, Content Type, Enabled, Original Connection String Expression Based, Policies\nDataset\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Policies\nKPI\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Content Type, Currency, Value Format, Visualization, Content Type, Policies\nFolder\nID, Name, Description, Creation Time, Modified Time, Created By, Modified By, Path, Type, Hidden, Size, Content Type, Policies\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nReport\nIntegrated Data Source contained within the Report\nReferenced Datasets contained within the Report\nIntegrated Datasets contained within the Report\nLinked Report\nThe Report that the Linked Report links to / parameterizes\nDataset\nReferenced Data Source embedded within the Dataset\nKPI\nDataset referenced by KPI\nData source associated with the KPI Report\nData Source\nDataset that contain a Data Source\nFolder\nSSRS Resources (KPI, Datasets, Data Sources, Reports, Linked Report) Contained in Folder\nSub-folders contained within the Folder\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nObject\nLineage available\nDataset\nData Source that the dataset sources its data from\nReports that use its data from the dataset\nData source\nDatasets that sources data from this data source\nReports that use data from this data source\nReport\nDatasets from which this report sources data\nData sources from which this report sources data\nThe collector harvests from SSRS using Basic Authentication:\nFollow the instructions in the Microsoft documentation to set up basic authentication on your SSRS environment.\nCreate a user that the collector will use for authentication and harvesting of SSRS objects.\nApply the Browser role to the user you just created.\nThese authentication details are used while configuring the CLI for the collector.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThere are a few different ways to run the data.world Collector--any of which can be combined with an automation strategy to keep your catalog up to date:\nCreate a configuration file (config.yml) - This option stores all the information needed to catalog your data sources. It is an especially valuable option if you have multiple data sources to catalog as you don't need to run multiple scripts or CLI commands separately.\nRun the collector though a CLI - Repeat runs of the collector requires you to re-enter the command for each run.\nThis section walks you through the process of running the collector using CLI.\nThe easiest way to create your Collector command is to:\nCopy the following example command in a text editor.\nSet the required parameters in the command. The example command includes the minimal parameters required to run the collector\nOpen a terminal window in any Unix environment that uses a Bash shell and paste the command in it and run in.\nThe following table describes the parameters for the command. Detailed information about the Docker portion of the command can be found here.\nParameter\nDetails\nRequired?\ndwcc:<CollectorVersion>\nReplace <CollectorVersion> in with the version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\n-a =<agent>\n--agent=<agent>\n--account=<agent>\nThe ID for the data.world account into which you will load this catalog. The ID is the organization name as it appears in your organization. This is used to generate the namespace for any URIs generated.\nYes\n--ssrs-api-base-url=<baseUrl>\nThe Base URL of the SSRS API. For example, https://ssrs-server.mycompany.com/api/3.10\nYes\n--username=<username>\nSSRS username for authentication.\nYes\n--password=<password>\nPassword for the user.\nYes\n--pagination-limit=<paginationLimit>\nThe number of records per SSRS API page request. Default limit of per page is 200 records and max limit of per page is 2000 records.\nNo\n--dry-run=<dryRun>\nSpecify this option to run the collector in dry run mode to test the connection details provided. No metadata is harvested in dry run mode.\nNo\n-n=<catalogName>\n--name=<catalogName>\nThe name of the catalog - this will be used to generate the ID for the catalog as well as the filename into\u00a0which the catalog file will be written.\nYes\n-o=<outputDir>\n--output=<outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nIn our example we use the /dwcc -outputas\u00a0it\u00a0is\u00a0running\u00a0in\u00a0a\u00a0Docker\u00a0container and\u00a0that\u00a0is\u00a0what\u00a0we\u00a0specified\u00a0in\u00a0the\u00a0script\u00a0for\u00a0a\u00a0Docker\u00a0mount\u00a0point.\nYou\u00a0can\u00a0change\u00a0this\u00a0value\u00a0to\u00a0anything\u00a0you\u00a0would\u00a0like\u00a0as long\u00a0as\u00a0it\u00a0matches\u00a0what\u00a0you\u00a0use\u00a0in\u00a0the\u00a0mount\u00a0point:\n-mount\u00a0type=bind,source=/tmp,target=/dwcc-output\u00a0...-o /dwcc-output\nIn this example, the output will be written to the /tmp directory on the local machine, as indicated by the mount point directive. The log file, in addition to any catalog files, will be written to the directory specified in the mount point directive.\nYes\n\n-L\n--no-log-upload\nDo not upload the log of the dwcc run to the organization account's catalogs dataset or to another location specified with --upload-location (ignored if --upload not specified)\nNo\n--site=<site>\nThe slug for the data.world site into which you will load this catalog this is used to generate the namespace for any URIs generated.\nNo\n-H=Host\n--api-host=Host\nThe host for the data.world API.\nNo\n-t=<apiToken>\n--api-token=<apiToken>\nThe data.world API token to use for authentication. The default is to use an environment variable named DW_AUTH_TOKEN.\nNo\n-U\n--upload\nWhether to upload the generated catalog to the organization account's catalogs dataset or to another location specified with --upload-location (This requires that the --api-token is specified.\nNo\n--upload-location=<uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account. This parameter is ignored if --upload is not specified.\nNo\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nCause: The large size of an SSRS API results may take long time to harvest metadata. The collector paginates through the results which may lead to many API calls.\nSolution: Increase the\u00a0page\u00a0size (--pagination-limit) to reduce the number of API calls.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/162968-sql-server-reporting-services--ssrs--and-the-data-world-collector.html"
  },
  {
    "title": "SQL Anywhere and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for SQL Anywhere tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to SQL Anywhere.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\nJDBC driver\nThe computer should have the appropriate JDBC driver on its file system.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nMake sure you download the appropriate JDBC driver for SQLAnywhere on the machine from where you will run the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select SQLAnywhere. Click Next.\nOn the SQLAnywhere Collector requires an additional driver file screen, set the following:\nField name\nCorresponding parameter name\nDescription\nRequired?\nJDBC driver directory\nsource=${HOME}/dwcc-drivers\nThe driver required to run the collector. You must download this driver yourself and place it in a directory on the machine that will run the collector.\nYes\nOn the Configure an on-premises SQLAnywhere Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following optional properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value.\nNo\nOn the\u00a0Finalize your SQLAnywhere Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML)\u00a0or\u00a0Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe SQLAnywhere command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98703-sql-anywhere-and-the-data-world-collector.html"
  },
  {
    "title": "Tableau and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to:\nDiscover Tableau workbooks and dashboards across your enterprise\nPerform impact analysis to understand how changes to upstream data sources impact Tableau reports\nThe collector supports Tableau API versions 3.7-3.10 on Tableau Server v 2022.1\nThe Tableau collector supports the following methods for authentication:\nSAML via Personal Access Token\nUsername and password\nThese authentication details are used while generating the CLI or YAML file for the collector.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nDatabases\nName, Description, Database Connection Type\nDatabase tables\nName\nDatabase columns\nName\nProjects\nName, Description\nWorkbooks\nName, Description, Creator Email, Creator Name, Creator Tableau User, Preview Image, and Workbook URL\nDashboards\nName, Creator Email, Creator Name, Creator Tableau User, Preview Image, and Dashboard URL\nViews\nName, Creator Email, Creator Name, Creator Tableau User, Number of Views, Number of Favorites, Preview Image, and View URL\nFields\nName, Identifier, Description\nCalculated fields\nName, Description, Calculation Formula\nDimensions\nName, Identifier, Description\nMeasures\nName, Identifier, Description\nMetrics\nName, Identifier, Creator, Creation Date, Modified Date, Metrics UrlField Data Type, Field Format, Field Type\nCustom SQL tables\nName, Identifier, Description, Query\nEmbedded data sources\nName, Identifier, Description\nPublished data sources\nName, Identifier, Description\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nDatabases\nSchemas contained within database\nTables contained within database\nDatabase tables\nViews that use database table\nSchema containing database table\nDatabase containing the database table\nDatabase columns\nTable that a database column is part of\nProjects\nViews contained within the project\nWorkbooks contained within the project\nDashboards contained within project\nSubprojects contained within project\nWorkbooks\nProjects that contain workbook\nData sources embedded within workbook\nViews contained within workbook\nDashboards\nFields used by dashboard\nProjects containing dashboard\nTables used by dashboard\nWorkbooks containing dashboard\nViews embedded in dashboard\nViews\nFields used by view\nProjects containing view\nTables used by view\nWorkbooks containing view\nDashboards which embed the view\nFields\nData Sources containing field\nViews using field\nCalculated fields\nViews that use the calculated field\nData sources that contain the calculated field\nDimensions\nData sources containing dimension\nTable related to dimension\nMeasures\nData Source containing measure\nViews using measure\nCustom SQL tables\nViews using Custom SQL table\nEmbedded data sources\nFields contained within embedded data source\nWorkbook embedding embedded data source\nPublished data sources\nFields contained within published data source\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nObject\nLineage available\nDatabase columns and tables\nFields that use database columns and tables\nDashboards\nFields and tables that dashboards source their data from\nViews\nFields and tables that views source their data from\nFields\nColumns, tables, and other fields that a field uses its data from\nEmbedded data sources\nPublished data sources\nDepending on the size of your Tableau instance, you may want to exclude or include specific resources from your catalog.\nExclude object types: Use the --tableau-exclude parameter to exclude harvesting of certain object types. The supported object types are: View, Dashboard, Database, PublishedDataSource, EmbeddedDataSource, CalculatedField, ColumnField, BinField, GroupField, DatasourceField, CustomSQLTable, Metric\nFilter to specific Tableau site: Use the --tableau-site parameter to filter to a specific site.\nFilter to specific Tableau projects: Use the --tableau-project parameter to harvest from multiple tableau projects. Use the parameter multiple times for multiple projects.\nGraphQL page size: Use the --tableau-graphql-page-size parameter to adjust the GraphQL page size. The maximum page size is 1000.\nIncrease Docker resources: If you run into out of memory errors, increase the memory on the machine running the collector, or increase the java heap size when running a jar file, or use filtering.\nThe collector harvests from Tableau using the Tableau Metadata API. The Metadata API is always enabled for Tableau Cloud. However, by default, the Metadata API is disabled by default for Tableau Server.\nFor detailed instructions, see the Tableau documentation.\nPrerequisites:\nYou must be on Tableau Server 2019.3or later\nThe REST APImust not be disabled.\nYou must use an account with server admin role to enable the Metadata API on Tableau Server using the Tableau Services Manager (TSM) command line interface (CLI).\nTo enable metadata API:\nOpen a command prompt as an admin on the initial node where TSM is installed in the cluster.\nRun the following command:\nIf you do not have the Data Management license, you will need to enable derived permissions to see related external assets. For details see the Tableau Cloud documentation and Tableau Server documentation.\nSet up a new user in Tableau with the Server Admin role.\nCreate a Personal Access Token (PAT) for the new user. See Tableau docs for details.\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the\u00a0Choose metadata collector type you would like to setup\u00a0screen, select\u00a0Tableau. Click\u00a0Next.\nOn the Configure an on-premises Tableau Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nTableau API Base URL\n--tableau-api-base-url=<baseUrl>\nBase URL of the Tableau API. For example: http://8bank/api/3.10/\nYes\nAuthentication\nSelect one of the following options and set the corresponding authentication details.\nAuthenticate using Username and Password\nAuthenticate using a private access token (PAT)\nYes\nAuthenticate using Username and Password\n\nYes\n(or set the Authenticate using a private access token (PAT) parameters)\n\n\nUsername\n--tableau-username=<username>\nTableau username for authentication.\nPassword\n--tableau-password=<password>\nTableau password for authentication.\nAuthenticate using a private access token (PAT)\nYes\n(or set the Authenticate using Username and Password parameters.)\n\n\nPAT Name\n--tableau-pat-name=<personalAccessTokenName>\nTableau personal access token name\nPAT Secret\n--tableau-pat-secret=<personalAccessTokenSecret>\nTableau personal access token secret\nOn the next screen, set the following advanced properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nSkip cataloging of preview images\n--tableau-skip-images=<true_or_fase>\nWhether to skip the cataloging of preview images.\nNo\nTableau Site\n--tableau-site=<siteID>\nID or name of the Tableau site to catalog (if not provided, will catalog all sites accessible to the user).\nNo\nTableau Pagination Size\n--tableau-graphql-page-size=<size>\nPage size to use for paginated graphql (metadata api) queries.\nNo\nTableau Projects\n--tableau-project=<projectId>\nID or name of the Tableau project to catalog. If not provided, will catalog all projects. Use the parameter multiple times for multiple projects.\nNote:\u00a0Sub-projects (projects nested within another project) must be specified individually.\nNo\nExclude Object Types\n--tableau-exclude=<exclusions>\nExclude Tableau object types from being cataloged.\nThe supported object types are: View, Dashboard, Database, PublishedDataSource, EmbeddedDataSource, CalculatedField, ColumnField, BinField, GroupField, DatasourceField, CustomSQLTable, Metric.\nUse the parameter multiple times to exclude multiple object types.\nNo\nOn the\u00a0Finalize your Tableau Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate a\u00a0Configuration file( YAML)\u00a0or\u00a0Command line arguments (CLI).\u00a0Click\u00a0Next\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Tableau command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were 9 before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that these are just sample commands for showing the syntax. You must generate the command specific to your setup from the application UI.\nSample command with username and password parameters.\nSample command with PAT parameters.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\n\nCause: The large size of a Tableau environment results in a long time to harvest metadata.\nSolution: Increase the graphql page size to reduce the number of API calls.\nCause: There is not enough memory allocated to the collector on the machine that the collector is running. Note that the collector may run for a long time even if there is sufficient memory due to the size of the source system.\nSolution: Increase memory of the machine running the collector. Run docker system info to see the total memory available for Docker. The collector will use 80% of what is allocated to the container. You can run docker stats to see how much memory is used by the container when the collector runs.\nCause: The connection to Tableau API expired due to a long run.\nSolution: No action is required as the collector re-authenticates automatically to Tableau.\nThe following error message is observed: Showing partial results. The request exceeded the \u2018n\u2019 node limit. Use pagination, additional filtering, or both in the query to adjust results.\nCause: When you increase the graphql page size, you may run into warnings messages in the logs due to nested queries.\nSolution:  Try a smaller page size or increase the max node limit. Increase the max node limit by setting metadata.query.limits.count which defaults to 20,000.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98704-tableau-and-the-data-world-collector.html"
  },
  {
    "title": "ThoughtSpot and the data.world Collector",
    "content": "\nThe latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to directly harvest metadata ThoughtSpot Answers and Liveboards into data.world\nThe collector authenticates to ThoughtSpot using username and password. The collector will have access to all the resources that the ThoughtSpot account has access to.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nAnswer\nTitle, Description, Tags, SQL query, Created by, Creation date, Last modified by, Last modified date, External URL\nLiveboard\nTitle, Description, Tags, SQL query, Created by, Creation date, Last modified by, Last modified date, External URL\nLogical Column\nTitle, Description, Tags, Column Index, Column Type Name, Column Precision, Is Primary Key, Creation date, Last modified by, Last modified date, Created by\nLogical table\nTitle, Description, Tags, Created by, Creation date, Last modified by, Last modified date\nWorkbook\nTitle, Description, Tags, Created by, Creation date, Last modified by, Last modified date\nFormula\nTitle, Formula text, Created by, Last modified by, Last modified date, Created date\nTag\nTitle, Count of resources with tag, Description, Creation date, Last modified by, Last modified date\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nAnswer\nRelationship to Tags that the Answer is associated with\nLiveboard\nRelationship to Tags that the Liveboard is associated with\nLogical Column\nRelationship to Logical Tables that the Logical Column is part of\nRelationship to Tags that the Logical Column is associated with\nRelationship to Formulae that the Logical Column is associated with\nLogical Table\nRelationship to Logical Columns contained in Logical Table\nRelationship to Tags that the Logical Table is associated with\nWorkbook\nRelationship to Logical Columns contained in Workbook\nRelationship to Tags that the Workbook is associated with\nFormula\nRelationship to the Logical Column that contains the Formula\nTag\nRelationship to Answer, Liveboard, Logical Column, Logical Table, or Workbook resources that the Tag is associated with\nEureka Explorer Lineage is available to\u00a0Enterprise customers\u00a0on\u00a0certain plans. Please contact your Customer Success specialist to find out how to enable\u00a0Explorer lineage\u00a0for your organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a collector is run.\nThe following lineage information is collected by the Thoughtspot collector.\nObject\nLogical column\nAssociated source column in an upstream source\nAssociated downstream Answers and Liveboards\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select ThoughtSpot. Click Next.\nOn the Configure a new on-premises ThoughtSpot Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nHost name\n--hostname=<hostname>\nThe host name for the Thoughtspot account to collect. For example, instanceName.thoughtspot.cloud\nYes\nUsername\n--username=<username>\nThe username for the Thoughtspot account to collect.\nYes\nPassword\n--password=<password>\nThe password for the Thoughtspot account to collect.\nYes\nOn the\u00a0Finalize your ThoughtSpot Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML) or\u00a0Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0ThoughtSpot command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\n\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/140888-thoughtspot-and-the-data-world-collector.html"
  },
  {
    "title": "Vertica and the data.world Collector",
    "content": "The latest version of the Collector is 2.150. To view the release notes for this version and all previous versions, please go here.\nUse this collector to harvest metadata for Vertica tables and columns across the enterprise systems and make it searchable and discoverable in data.world.\nThe collector supports basic authentication to Vertica.\nThe collector catalogs the following information.\nObject\nInformation cataloged\nColumns\nName, Description, JDBC type, Column Type, Is Nullable, Default Value, Key type (Primary, Foreign), Column size, Column index\nTable\nName, Description, Primary key, Schema\nViews\nName, description, SQL definition\nSchema\nIdentifier\nDatabase\nType, Name, Identifier, Server, Port, Environment, JDBC URL\nBy default, the harvested metadata includes catalog pages for the following resource types. Each catalog page has a relationship to the other related resource types. If the metadata presentation for this data source has been customized with the help of the data.world Solutions team, you may see other resource pages and relationships.\nResource page\nRelationship\nTable\nColumns\nColumns\nTable\nMake sure that the machine from where you are running the collector meets the following hardware and software requirements.\nItem\nRequirement\nHardware\nRAM\n8 GB\nCPU\n2 Ghz processor\nSoftware\nDocker\nClick here\u00a0to get\u00a0Docker.\nJava Runtime Environment\nOpenJDK 17 is supported and available here.\ndata.world specific objects\nDataset\nYou must have a ddw-catalogs (or other) dataset set up to hold your catalog files when you are done running the collector.\nThis section walks you through the process of generating the\u00a0command\u00a0or\u00a0YAML file\u00a0for running the collector from\u00a0Windows\u00a0or\u00a0Linux or MAC OS.\nTo generate the command or YAML file:\nOn the Organization profile page, go to the Settings tab > Metadata collectors section.\nClick the Help me set up a collector button.\nOn the On-prem collector setup prerequisites screen, read the pre-requisites and click Next.\nOn the On which platform will this collector execute? screen, select if you will be running the collector on Windows or Mac OS or Linux. This will determine the format of the YAML and CLI that is generated in the end. Click Next.\nOn the Choose metadata collector type you would like to setup screen, select Vertica. Click Next\nOn the Configure an on-premises Vertica Collector screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\ndata.world API token\n-t= <apiToken>\n--api-token= <apiToken>\nThe data.world API token to use for authentication. Default is to use an environment variable named\u00a0${DW_AUTH_TOKEN}.\nYes\nOutput Directory\n-o= <outputDir>\n--output= <outputDir>\nThe output directory into which any catalog files\u00a0should be written.\nNo\nCollection Name\n-n= <catalogName>\n-n= <catalogName>\nThe name of the collection where the collector output will be stored.\nYes\nAutomatic upload location\n--upload-location= <uploadLocation>\nThe dataset to which the catalog is to be uploaded, specified as a simple dataset name to upload to that dataset within the organization's account, or [account/dataset] to upload to a dataset in some other account (ignored if --upload not specified)\nYes\ndata.world API host\n-H= <apiHost>\n--api-host= <apiHost>\nThe host for the data.world API. NOTE: This parameter is required for single-tenant installations. For example, \"api.site.data.world\" where \"site\" is the name of the single-tenant install.\nYes\n(for single-tenant installations)\nOn the next screen, set the following properties and click Next.\nField name\nCorresponding parameter name\nDescription\nRequired\nServer\n-s=<server>\n--server=<server>\nThe hostname of the database server to connect to.\nYes\nServer port\n-p=<port>\n--port=<port>\nThe port of the database server (if not the default).\nNo\nDatabase\n-d=<database>\n--database=<database>\nThe name of the database to connect to.\nYes\nUsername\n-u=<user>\n--user=<user>\nThe username to use to connect to the database.\nYes\nPassword\n-P=<password>\n--password=<password>\nThe environment variable of the password used to connect to the database.\nYes\nSchemas to collect\n\nSelect from one of the following options: Collect all schema, Specify which schema to collect\n\nYes\nCollect all schema\n-A --all-schemas\nCatalog all schemas to which the user has access.\nYes (if\u00a0--schema\u00a0is not set)\nSpecify which schema to collect\n-S=<databaseSchema>\n--schema=<databaseSchema>\nThe name of the database schema to catalog.\nYes (if\u00a0--all-schema\u00a0is not set)\nOn the next screen, set the following optional properties and click\u00a0Next.\nField name\nCorresponding parameter name\nDescription\nRequired?\nServer Environment\n-e=<environment>\n--environment=<environment>\nIf your provided server name is\u00a0localhost, use this to give a friendly name to the environment in which your database server runs to help differentiate it from other environments.\nNo\nDatabase ID\n-D=<databseid>\ndatabase-id=<databaseId>\nA unique identifier for this database - will be used to generate the ID for the database (this is optional, you only need to provide this if the database name used for the connection is not sufficiently unique to completely identify the database)\nNo\nJDBC Properties\n--jdbc-property=<driverProperties>\nJDBC driver properties to pass through to driver connection, as name=value.\nNo\nOn the\u00a0Finalize your Vertica Collector configuration\u00a0screen, you are notified about the environment variables and directories you need to setup for running the collector. Select if you want to generate\u00a0Configuration file ( YAML) or\u00a0Command line arguments (CLI). Click\u00a0Next.\nYou must ensure that you have set up these environment variables and directories\u00a0before\u00a0you run\u00a0the collector.\nThe next screen gives you an option to download the YAML configuration file or copy the CLI command. Click\u00a0Done. If you are generating a YAML file, click\u00a0Next.\nThe\u00a0Vertica command\u00a0screen gives you the command to use for running the collector using the YAML file.\nYou will notice that the YAML/CLI has following additional parameters that are automatically set for you.\nExcept for the collector version, you should not change the values of any of the parameter listed here.\nParameter name\nDetails\nRequired?\n-a= <agent>\n--agent= <agent>\n--account= <agent>\nThe ID for the data.world account into which you will load this catalog - this is used to generate the namespace for any URIs generated.\nYes\n\n--site= <site>\nThis parameter\u00a0should be\u00a0set only for Private instances.\u00a0Do not\u00a0set it for public instances and single-tenant installations. Required for private instance installations.\nYes (required for private instance installations)\n-U\n--upload\nWhether to upload the generated catalog to the\u00a0 organization account's catalogs dataset.\nYes\n-L\n--no-log-upload\nDo not upload the log of the Collector run to the organization account's catalogs dataset.\nYes\ndwcc: <CollectorVersion>\nThe version of the collector you want to use (For example, datadotworld/dwcc:2.113)\nYes\nAdd the following additional parameter to test run the collector.\n--dry-run If specified, the collector does not actually harvest any metadata, but just checks the database connection parameters provided by the user and reports success or failure at connecting.\nVerify that you have set up all the required environment variables that were identified by the Collector Wizard before running the collector. Alternatively, you can set these credentials in a credential vault and use a script to retrieve those credentials.\nVerify that you have set up all the required directories that were identified by the Collector Wizard.\nBefore you begin running the collector make sure you have the correct version of collectors downloaded and available.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the command you are using to run the collector.\nPlace the YAML file generated from the Collector wizard to the correct directory.\nFrom the command line, run the command generated from the application for executing the YAML file.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from Docker, you can edit the collector version in the generated command to run the collector with the newer version.\nGo to the server where you have setup docker to run the collector.\nMake sure you have download the version of collectors from here. This version should match the version of the collector specified in the command you are using to run the collector.\nThis version should match the version of the collector specified in the YAML/CLI generated from the collector wizard.\nMake sure you have download the correct version of collectors. This version should match the version of the collector specified in the YAML/CLI generated from the collector wizard.\nFrom the command line, run the command generated from the application. Here is a sample command.\nNote that is just a sample command for showing the syntax. You must generate the command specific to your setup from the application UI.\nThe collector automatically uploads the file to the specified dataset and you can also find the output at the location you specified while running the collector.\nAt a later point, if you download a newer version of collector from docker, you can edit the collector version in the generated command to run the collector with the newer version.\nThe catalog collector may run in several seconds to many minutes depending on the size and complexity of the system being crawled. If the catalog collector runs without issues, you should see no output on the terminal, but a new file that matching *.dwec.ttl should be in the directory you specified for the output. If there was an issue connecting or running the catalog collector, there will be either a stack trace or a *.log file. Both of those can be sent to support to investigate if the errors are not clear. A list of common issues and problems encountered when running the collectors is available here.\nSome enterprise systems support the use of Secure Sockets Layer (SSL) encrypted communications on all external traffic. If you are harvesting metadata from a source system that requires SSL, you will need to add a CA certificate or self-signed certificate.\nObtain the root certificate for your source system issued by your company. Typically your system administrator should be able to provide you with this.\nIf the collector is run via Docker, extend the Docker image and install the custom certificate.\nFirst, prepare a Dockerfile with the instructions for Docker to install the custom certificate and extend the Docker image.\nEnsure you are on the machine where you have downloaded the Docker Image and plan to execute the Collector.\nIn a directory create the new Dockerfile with the following parameters for your custom SSL Certificate:\nReplace <collector_version> with the version of the Collector you want to use (For example,\u00a0datadotworld/dwcc:2.120)\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nReplace <custom_certificate_file_name> with the name of your custom SSL Certificate file.\nFor example, the command will look like:\nNext, execute the the Dockerfile to install the certificate and extend the data.world Collector Docker Image.\nUsing your terminal of choice, ensure you are in the directory containing the Dockerfile created in step 1.\nNext, create the new extended Docker image, called dwcc-cert  in this example, by executing the following command:\nImportant things to note:\nThe command must be all lowercase.\nThe command must include the period (.) at the end, which directs Docker to use the local directory for the Dockerfile created above.\nFor the new image, the command uses the name dwcc-cert You can change the name if you want.\nFinally, run the collector using the custom Certificate.\nGet the standard docker run command for the Data Source you are collecting from.\nChange the docker run command to use dwcc-cert image instead of dwcc image.\nSample command for Tableau.\nIf you are using YAML file for running the collector, edit the command to use dwcc-cert image instead of dwcc image.\nIf the collector is run via jar, add the certificate to the JVM truststore.\nFrom the terminal, navigate to the directory containing the certificate.\nRun the following command to add the SSL certificate to the truststore:\nReplace <custom_certificate_file_path> with the path to the custom SSL Certificate.\nFor example, the command will look like:\nFinally, run the collector using the original jar file command. Note that this command does not need any modifications.\nIssue\nThe following error occurs while running the collector:\nDescription\nThere was an issue connecting to the source system using the SSL certificate.\nSolution\nCheck to make sure the SSL certificate has not expired.\nEnsure you have the correct SSL certificate for the source system.\nKeep your metadata catalog up to date using cron, your Docker container, or your automation tool of choice to run the catalog collector on a regular basis. Considerations for how often to schedule include:\nFrequency of changes to the schema\nBusiness criticality of up-to-date data\nFor organizations with schemas that change often and where surfacing the latest data is business critical, daily may be appropriate. For those with schemas that do not change often and which are less critical, weekly or even monthly may make sense. Consult your data.world representative for more tailored recommendations on how best to optimize your catalog collector processes.",
    "url": "https://docs.data.world/en/98705-vertica-and-the-data-world-collector.html"
  },
  {
    "title": "Run the data.world Collector without Docker (.jar file)",
    "content": "For the majority of implementations of the data.world Collector, it will be run from inside a Docker container. If, however, you cannot run Docker, you can still use the Collector from a .jar file on your server. To get started, please contact our support team or speak to your CS representative to get the correct .jar file for your implementation. All other prerequisites and instructions remain the same except for the requirement to have Docker installed.",
    "url": "https://docs.data.world/en/98652-run-the-data-world-collector-without-docker---jar-file-.html"
  },
  {
    "title": "How to run the data.world Collector command",
    "content": "If you use Docker with the Collector there are a couple ways to run the collector:\nCreate a YAML configuration file (config.yml) - This option stores all the information needed to catalog your data sources in an easy-tocreate and easy-to-read format. It is an especially valuable option if you have multiple data sources to catalog as you don't need to run multiple scripts or CLI commands separately.\nRun the collector though a CLI - Makes regular, repeating runs of the collector very laborious and time-consuming as the commands are re-entered for each run. Links to the instructions for each of the data sources we support can be found in Currently supported data sources .\nThe config.yaml file provdes a way to manage complex metadata collector runs from a CLI. Having all of your options saved in a file provides the following benefits:\nA saved file eliminates the tedium of retyping the commands every time you run the data.world Collector.\nTroubleshooting the configuration is easier due to the file structure.\nYou can run multiple Collectors at once from the same file.\nA YAML config file is a place to store all the configuration and credential information needed to run the data.world Collector for your source(s). Running the Collector from a YAML file is very similar to running it through the CLI: The file supports and uses all the same command-line options, and has all the same prerequisites.\nIt is composed of two sections which contain:\nGlobal options - These are used to express collector parameters that should remain the same for all collector runs in the file. For example, typically all of the collector runs for an organization should use the same value for account. Other parameters which are commonly shared across all collectors and convenient to specify globally are upload and API token.\nCommand options - These are used to specify the configuration for a single run of a specific collector. Each command-options section is an element in a YAML array labeled\u00a0commands. The name of the element is the name of the collector command to run. The following example config file contains a basic, generic set of options to run a data.world Collector and illustrates the syntax used:\nA config.yaml file to run a data.world Collector would look something like this:\nSome options do not require values when used through via the CLI, e.g., --upload, --no-log-upload. In the YAML file, these options must use \"true\" or false\", as shown with upload in the above example.\nValues of properties in a configuration file can reference system environment variables, which are then substituted at collector runtime. The syntax for referencing an environment variable is : ${ENV_VARIABLE}. For example,\u00a0${MY_VAR}\u00a0for an environment variable named\u00a0MY_VAR. The purpose of this feature is to allow you to configure collectors using configuration file while avoiding to store sensitive values, like passwords, in the file.\nFor example, to substitute an environment variable\u00a0DB_PASSWORD\u00a0as the value for a database password passed to a JDBC collector (like postgres), the YAML command will look like:\nWhen the collector uses this configuration file, it looks for a system environment variable named\u00a0DB_PASSWORD\u00a0and, if found, substitute its value into the file. If no such environment variable is found, a warning message will be logged and blank value is used.\nNote that formatting and indenting are significant in YAML. See YAML Aint't Markup Language (YAML\u2122) Version\u00a01.2 for a good overview of YAML file structure and formatting.\nThere are a few things to keep in mind when you use Docker to run your metadata collector:\nNo Collector options are allowed on the command line after config.yml.\nWe have licensing permission to distribute some JDBC drivers with our collectors. Drivers that are not supplied need to be mounted. (See the example of the command used to run a collector for your data source on the the data.world Collector configuration page for that collector. Here is a list of the sources we currently support with links to the configurations for them.\nThe config.yml is run with Docker and so uses the same Docker commands you would use for your data source. Examples of the commands used for each data source are provided on their respective configuration pages. However when you run Docker with a config file, you need to add one more mount statement to indicate that the file is mounted in the container. If you store your the data.world Collector configurations in the directory\u00a0/dwcc-configs, then you would add the mount directive --mount type=bind,source=/dwcc-configs,target=/dwcc-configs\u00a0and specify the file as\u00a0--config-file /dwcc-configs/config.yaml.\nHere is an example of a command used to run the config file:\nDo not forget to replace x.y in datadotworld/dwcc:x.y with the version of the Collector you want to use (e.g., datadotworld/dwcc:2.113).\nIf your\u00a0config.yml\u00a0file includes environment variable references, make sure to define those environment variables when you run the docker container. To continue the example above, if your database password is defined as an environment variable on your host, then pass it into the container (for substitution with the example yaml file) using  -e DB_PASSWORD=$DB_PASSWORD.\nThe command will look like this:",
    "url": "https://docs.data.world/en/98654-how-to-run-the-data-world-collector-command.html"
  },
  {
    "title": "Discover and catalog sensitive data",
    "content": "This is a Beta feature and is not generally available to all customers at this time.\nThe Sensitive Data Discovery collector (DWCC_SDD) uses machine learning to automatically detect and tag any sensitive data in the system, such as personal Identifiable Information (PII) Protected Health Information (PHI),  and Payment Card Industry (PCI)  Once the data is detected and tagged, users can view the tags as they use the data and can be aware of it.\nSee our Implementation Guide for details on how to set up and use Sensitive Data Discovery with your data.",
    "url": "https://docs.data.world/en/114771-discover-and-catalog-sensitive-data.html"
  },
  {
    "title": "The data.world Collector FAQ",
    "content": "Unless you use the Connection Manager to catalog your metadata, you will be using a command-line program called the data.world Collector to collect your data. The Collector pulls only metadata from your source--it doesn't collect any data.\nThe data.world Collector is distributed as an image on Dockerhub. If you run the Collector from Docker, the run command will attempt to find the image locally, and if it doesn't find it, it will go to Dockerhub and download it automatically:\nIf you are running the Collector from a .jar file, you will get the correct file from customer support.\nIf you are unsure what version of the Collector to use, the most current releases are always listed in the\u00a0Catalog collector change log. However If you don't know the complete version name, or if you would like to see a list of the the Collector versions, you can go to our Dockerhub repositories. There are two repositories, one for released versions and one for release candidate versions:\ndatadotworld/dwcc- Contains all of the officially released versions of the data.world Collector\ndatadotworld/dwcc-rc - Contains the \"release candidate\" versions. Release candidates are test versions, they are not officially supported and released. They are primarily used for quick customer fixes until the official release comes out.\nDo not use the versions named Latest from either repository--only specify numeric releases (e.g., dwcc:2.36). The Latest tag refers to the most recent docker image that has been already downloaded to the host machine. Using Latest in the command instead of a specific version number does not pull the latest version from the Docker repository.\nDo not use a release candidate (rc) version of the Collector unless you have been explicitly directed to do so by your customer success or support representative.\nThe name you specify on the CLI should match exactly the version name on Dockerhub. For example:\nThe name of the Collector version 2.36 is datadotworld/dwcc:2.36\nThe name of the third RC collector version of 2.37 is datadotworld/dwcc-rc:2.37-rc-0003 (RC versions are padded to four digits).\nDocker is\u00a0an open source containerization platform or application build and deployment tool. It is based on the idea of that you can package your code with dependencies into a deployable unit called a container. A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Container images become containers at runtime--and in the case of Docker containers, images become containers when they run on\u00a0Docker Engine. Available for both Linux and Windows-based applications, containerized software will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.\nDatabase sources require a JDBC driver to run the Collector. We bundle a JDBC driver with some of the database collectors we catalog. For licensing reasons, we cannot bundle other drivers. The drivers we include are:\nDatabricks\nHive (for Hive and Hive Metastore)\nPostgreSQL\nPresto\nSnowflake\nSQL Server\nIf you are cataloging another sources, please check with the database vendor for the proper driver to use with your version. You will need to obtain and license the driver yourself, and pass the full path to that directory as the value of that system property.\nYou can use one Collector to catalog as many data sources as you have. All you need to do is change the name of the catalog source and the parameters in the command-line.\nDebian Buster (the development codename for Debian 10).\nThey are not stored anywhere, the command is run and authenticates with the service it is collecting from. The user enters the authentication information when they run the command, or whatever scheduled job is running it passes the credentials in, each time.\nA collector should run as often as the organization wants to see updates to metadata reflected in their catalog. Here are some more specific guidelines that we\u2019ve seen from our customer use-cases:\nWhat kind of collector is it?\nHow often should you run it?\nA collector that gathers metadata about a database.\nUnless there are lots of schema changes, these are typically run once a week or once a month.\nA data analytics tool.\nIf you have an active user base that makes a lot of dashboards or reports, weekly to daily might make sense.\nThe docker image needs to be located on a machine that has access to the database or analytics tool it is cataloging. If it is located on a different machine than the source it is gathering metadata from, network access between the docker image machine and the machine that the metadata source is on needs to be created.\nHere\u2019s an example:\nA customer put the docker image onto an EC2 instance. They wanted to use that image to catalog a Salesforce instance. The EC2 instance needed to have access to make requests from the salesforce API for the collector to be able to successfully collect metadata.\nThe best answer is \u201cit depends\u201d.\nSome factors that can affect how long it takes to run the collector:\nHow many tables are in the database\nHow many fields those tables have\nHow many reports and dashboards there are in a data analytics tool\nHow many resources the server that the thing you are cataloging has\nSince the docker image is, at it\u2019s core, a simple java runtime environment, the code itself does not take a long time to boot up and run. Some collectors have filters that can isolate certain projects in a source to decrease the runtime.\nIf the metadata source being cataloged is configured to use a self-signed certificate to secure TLS connections, then the data.world Collector needs to be told to trust that certificate (since the native Java runtime trust store will not recognize it). The the data.world Collector documentation contains instructions on how to accomplish this. Note that this only applies if (typically on-premise) metadata sources have been set up to use self-signed certificates for TLS.\nThe typical example would be a postgres or SQL Server database on-premise.\nThe license is for enterprises that need Docker Desktop. Linux environments can run Docker without Docker Desktop. If you need to use Docker on a machine running MacOS or Windows, and you are a qualifying enterprise, (larger than 250 employees OR greater than $10 million in annual revenue) you will need to purchase a license, according to Docker\u2019s terms.\nTo display the licensing information for any version of a Collector after 2.24, run the following command in your terminal window:\ndocker run -it --rm datadotworld/dwcc:X.XX display-license\nwhere X.XX is the version number for the Collector.\nIt is now possible for users to set the level (severity) of log messages written to the console and log file from the data.world Collector. By default, we write \u201cinfo\u201d level messages; users can choose to write only errors (level=\u201cERROR\u201d), errors+warnings (level=\u201cWARN\u201d), or all messages including debug trace (level=\u201cDEBUG\u201d). This is useful if we want to have customers run the Collector with debug logging turned on, for troubleshooting problems etc.\nIf you are using Docker, to set the level to something other than \"info\", add the statement -e log_level=DEBUG to your run Docker... statement.\nIf you manually installed a Docker image for the data.world Collector instead of pulling it as part of the run command in the CLI, you can validate that it is an authorized version by using the hash on the file. The hash for every released version after 2.36 is provided right below the version number in the Catalog collector change log:\nTo compare the hash from your version to the authorized version run the following command from your CLI:\ndocker inspect datadotworld/dwcc:x.y where x.y is the version of the release (e.g., 2.36)\nYou will get back something that looks like this:\nCompare the value in Digest with the value in RepoDigests and if they are the same, you have an authorized version. If they are not the same, contact support.\nWe scan the schema declared, or all schema if using the -A parameter, but we only capture records within the target database.",
    "url": "https://docs.data.world/en/98667-the-data-world-collector-faq.html"
  },
  {
    "title": "Troubleshooting the collectors",
    "content": "If you are having difficulty running a collector, the following list of common problems can help you troubleshoot what went wrong. If your issue is unanswered, please contact support@data.world for more assistance.\nThis sections list some of the common errors you may see in the Command Line while running the collector.\nCLI error\nCause\nSolution\nzsh: command not found: [command]\nA parameter used in the command was not recognized by the terminal.\nCheck for a missing newline forward slashes (\\) to delineate a new line. The forward slash should be at the end of a line before a line break.\nMissing required options: [options]\nA required parameter to run the collector was not specified.\nAdd the command the required parameters to the command and set the parameter values correctly.\nUnknown option: [option]\nA parameter was specified that is not supported by the collector.\nRemove the unsupported parameter from the command.\ndocker: Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: [path]\nA specified directory path provided for linking a local host directory to the container directory does not exist.\nEnsure the directory exists. Source is the host directory path. Target is a path in the Docker container.\nCheck for proper casing of path. Some systems have case sensitive paths.\ndocker: invalid reference format.\nThe command is malformed. A possible issue here is that there are trailing spaces after a line continuation character (\\).\nRemove trailing spaces after the \\ character.\nUnable to connect to database [No suitable driver for [location]\nA driver is required to connect to a system and it was not specified.\nFor Docker, add --mount type=bind,source=/path/where/jar/was/downloaded,target=/usr/src/dwcc-config/lib\nFor jar, add -Djdbc.driver-directory=<your driver directory>\nUnable to connect to database [driver]) Communication link failure. Failed to connect to server. Reason: No more data available..]\nCollector was unable to connect to the source system.\nCheck that the credentials provided for running the collector are correct.\nCheck to make sure that the location information are correct.\nERROR: The selected output directory: [path] does not exist.\nThe output path that stores the catalog output does not exist.\nCheck that you have mounted a source directory on your machine to a target directory on the container that will store the catalog output.\nCheck that the path specified by --output or  -o is the path specified by the target directory on the container.\nERROR: Config file [path] does not exist\nThe config file contains the parameters that run the collector. The file path does not exist.\nCheck that you have mounted a source directory on your machine to a target directory on the container.\nCheck that the file exists in the source directory on your machine.\nCheck that the file path specified by --config-file is the file path on the target directory of the container.\nStackOverflowError\nThe collector parser hit a stack size limit due to a complex SQL statement or DAX expression.\nAdd the -e DWCC_JVM_OPTIONS=\"-Xss2m\" parameter to the command to increase the stack size. For example, the command will look like: docker run -it --rm -e DWCC_JVM_OPTIONS=\"-Xss2m\". This sets the stack size to 2 MB.\n\nThis sections list some of the common errors you may see in the log files while running the collector.\nError in log file\nCause\nSolution\njava.lang.RuntimeException [details]\nAn error occurred. More information is specified in the details.\nRead the error details for clues to resolve the issue. If there are no clear steps to troubleshoot, continue to the next steps.\nRun the collector with debug mode on:\nFor Docker, add -e log_level=DEBUG\nFor jar, add -Dlog_level=DEBUG.\nExample: java -Dlog_level=DEBUG -jar [path]See\nif debug logs continue useful information to troubleshoot. Open a support ticket if blocked.\ndwcc was unable to upload the catalog to data.world via the API at\nhttps://api.data.world/v0/data.world\nAPI exception: API token unauthorized\nAutomatic upload of collector output did not work. The collector could not connect to data.world using the API token.\nCheck that the API token is correct and not expired.\ndata.world API exception:\u00a0 http status 400\nAutomatic upload of collector output did not work. There was an issue uploading the catalog to data.world.\nCheck that the dataset specified by --upload-location exists.\nMake sure to use the dataset name referred to in the url.\nFor instance, if the dataset name is Dataset Space, the portion of the url identifying the dataset is dataset-space. Use dataset-space rather than Dataset Space.\nNote that this should be lower-case.\norg.open_kos.CollectorException: Database error during cataloging\nThere was an error connecting to the source system.\nCheck that the source system location information, credentials, and any roles are correct.\nUse the --dry-run option to validate that the source system location information and credentials are correct.\nConfirm that the source system is network routable from the machine where the collector is running. This may include any firewall rules.\n401 or 403 errors\nThere was an authorization issue while connecting to a system.\nThis typically means the location information specified is correct, but there was a credential issue.\nCheck that the source system credentials, and any roles are correct.\nCheck that the credentials have the right permissions (typically read permissions) to the objects that the collector will harvest from the system.\nIf you encounter OutOfMemoryError messages in the console output, it indicates that the process was terminated due to memory constraints. To resolve this issue, we need to address the memory allocation for various components: the Heap, Docker runtime, and the underlying OS/VM.\nSTEP 1: Allocating Sufficient Resources on the Operating System/VM Instance\nMake sure that the underlying operating system or virtual machine running Docker has enough resources allocated to support the containers. Increasing the memory available to the OS/VM instance provides resources for Docker and the hosted containers. Follow the documentation for the OS/virtual machines to make adjustments to the allocated resources.\nSTEP 2: Increasing Docker Memory\nEnsure that Docker has sufficient memory allocated to it. Depending on the platform and configuration, Docker may have a default memory limit, often around 2 GB, which might be insufficient. We recommend increasing it to minimum of 8 GB using Docker Desktop's settings. You can verify the current Docker memory limit by running\u00a0docker system info\u00a0or by checking the Docker Desktop configurations. If your setting is already at 8 GB or higher and you are still running into issues, increase the memory to a higher number and try again.\nSTEP 3: Adjusting Heap Memory\nIf increasing the Docker Memory doesn't resolve the issue, modify the Java heap memory settings using the\u00a0docker run -e _JAVA_OPTIONS='-Xmx<value to be set>'\u00a0option. By increasing the heap memory, you provide more memory for the Java application running inside the Docker container. For example, to set the Java heap memory to 12GB use: docker run -e _JAVA_OPTIONS='-Xmx12g'",
    "url": "https://docs.data.world/en/98669-troubleshooting-the-collectors.html"
  },
  {
    "title": "About data virtualization",
    "content": "When you connect a database to data.world using Connection manager or an integration from the Integration Gallery, your data continues to live at its source location and is not stored in data.world. This configuration is frequently referred to as Data virtualization.\nThe Connection manager is the best way to create a live connection that will be owned by an organization. If you need a connection that you will own personally, you will need to create it from the Integration Gallery.\nOne of the benefits of data virtualization is that it allows you to view and query data on data.world that would exceed the dataset size limits on data.world. It also ensures that you have access to your most current data without needing to worry about scheduling synchronizations, or the processing it time it would take to import/refresh the data.\nWhen you query a live table using data.world, our system will translate your query from our native SQL dialect into the SQL dialect of the target system. That system will then execute the query on its own hardware and return the results to data.world for display. Another benefit of virtualization is that it makes managing permissions and access to the data easier.\nCloud database providers frequently charge either by the amount of time that queries run on their systems or by the total amount of data scanned during the query. If this describes your database service then executing queries against live tables in data.world will also incur charges on those systems.",
    "url": "https://docs.data.world/en/105178-about-data-virtualization.html"
  },
  {
    "title": "Connect to data from the Integration Gallery",
    "content": "To create a new virtual data connection through the integration gallery, go to the Database connectors section of the Integration Gallery and select the one you want to use:\nFrom the connector screen select + Enable Integration to get to the configuration screen.\nNote that some database connections cannot be used for live connections--they are only for collecting metadata. A full list of all data connections, how they can be configured, and how they can be used can be found here.\nIf the integration is already flagged as Enabled, select Manage to go to the screen where you can add a new connection or edit your existing connections:\nHere are the links to the configuration parameters used in the data.world Connection Manager, and in the Integration Gallery:\nAthena\nAzure Synapse\nBigQuery\nDatabricks Delta LakeDatabricks connection\nDenodo\nInfor Ion\nMS SQL Server\nMySQL\nOracle\nPostgreSQL\nRedshift\nSnowflake\nTableau Metadata\nWhen you create a connection from the Integration gallery you are, by default, the owner of the connection. However you can also set any organization of which you are an administrator as the owner of the connection.\nThere are two compelling reasons for having most connections owned by an organization:\nThere is no loss of access to data when an employee leaves and their account is deactivated.\nFederation across data sources is faster and more efficient if it uses the same connection.\nOrganizational-level connections are shared between admins of the organization and can be used by by all of them to create new live tables. Non-administrator users can only query and preview existing live tables.\nOrganization-owned connections can only be used to add data to datasets owned by that organization. If you are in organizations A and B, you cannot add data to a dataset owned by B using a connection owned by A.\nWith a personal connection, only the connection owner may create new live tables with the connection, and other members of the organization can query and preview live tables.\nEnter all the parameters into the configuration window and select Test configuration to make sure it works. If it does, select Configure to save it. You can now use this connection any time you add data.\nConnections owned by an organization can be managed in the Connection manager whether they were created in the Integration Gallery or in Connection manager. Personally-owned connections must be managed from the Integration gallery.\nTo edit a connection from the Integration gallery go to Integrations and select My Integrations:\nOn the screen for the integration select the Manage tab. From there you can Add a new connection for this data source, or edit or delete one of the existing connections.\nYou will need your original credentials (password or key file) to make changes to an existing connection.",
    "url": "https://docs.data.world/en/105157-connect-to-data-from-the-integration-gallery.html"
  },
  {
    "title": "Additional connectors",
    "content": "",
    "url": "https://docs.data.world/en/98768-additional-connectors.html"
  },
  {
    "title": "Connect to data.world datasets using the JDBC driver dw-jdbc",
    "content": "dw-jdbc is a JDBC driver for connecting to datasets hosted on data.world. It can be used to provide read-only access to any dataset provided by data.world from any JVM language. dw-jdbc supports query access both in SQL (data.world's dialect - see details in our data.world SQL tutorial) and in SPARQL 1.1, the native query language for semantic web data sources.\nJDBC connects to data source based on a provided JDBC url. data.world JDBC urls have the form\njdbc:data:world:[language]:[user id]:[dataset id]\nwhere [language] is either \"sql\" or \"sparql\",[user id] is the data.world id of the dataset owner, and [dataset id] is the data.world identifier for the dataset.\ndw-jdbc can be built from the command-line using mvn clean install.\nWhen you need an API token for an integration, you can get it from your profile settings. Click on your\u00a0user icon\u00a0and choose\u00a0Settings:\nBoth Read/Write and Admin tokens are provided. For the metadata catalog collector you can use the Read/Write token for the metadata catalog collector if you have write permissions to your organization's ddw-catalogs dataset.\nMore resources for using Java/JVM/JDBC-enabled tools:\nGithub Repo\nmeta.data.world post",
    "url": "https://docs.data.world/en/98769-connect-to-data-world-datasets-using-the-jdbc-driver-dw-jdbc.html"
  },
  {
    "title": "Connecting to an AWS RDS database through an SSH tunnel",
    "content": "If you have a database hosted on Amazon Web Services(AWS) that is not publicly accessible, you can allow data.world to connect to it by using an SSH tunnel\nIn this scenario, you will launch a publicly accessible SSH server (sometimes called a bastion server) in the same AWS Virtual Private Cloud (VPC) as your AWS database.\nYou will then configure data.world to connect to your SSH server instead of directly to the database. The public SSH server will forward data.world's requests to the private database.\nBy using this type of connection, you can keep your database hidden from the public internet and instead rely on your SSH server to handle the security and access control for connections to that database.\nLogin to your AWS Management Console.\nFor RDS databases(including SQL Server, MySQL, PostgreSQL, or Oracle), click on Database>RDS in the center of the page.\nClick on Databases on the left side of the Amazon RDS page.\nClick the DB Instances link in the middle of the page to see a list of active databases.\nClick on the name of the database in the DB identifier column to open its details page.\nWithin the Connectivity and Security section at the bottom of the page, take note of the following information:\nEndpoint & port > Endpoint\nEndpoint & port > Port\nNetworking > VPC\nGo to the AWS Management Console main page and click Compute>EC2 from the center of the page.\nIn the middle of the page, select Launch Instance.\nThe Instance configuration requires the following steps:\nChoose AMI (Amazon Machine Image): Any Linux based AMI will be appropriate for this setup - SSH is the only program required.\nChoose Instance Type: This will be a low memory and storage application. Depending on your usage requirements, a free tier t2.micro instance may be sufficient.\nConfigure Instance: ensure the following settings are configured:\nNetwork: select your database's VPC from Task 1\nSubnet: Choose a public subnet\nAuto-assign Public IP: Use subnet setting(Enable)\nAdd Storage: Accept defaults\nAdd Tags: Accept defaults\nConfigure Security Group:\nAssign a security group: Select Create a new security group\nSecurity group name: data.world to SSH\nDescription: Bastion server for forwarding requests from data.world to private database\nModify the first line of the security rules to show the following values:\nType: SSH\nProtocol: TCP\nPort Range: 22\nSource: Enter your public IP address followed by /32. You can use a Google search to find your IP\nDescription: My IP for configuration\nClick Add Rule and enter:\nType: SSH\nProtocol: TCP\nPort Range: 22\nSource: 52.3.83.134/32\nDescription: data.world inbound connection\nClick Add Rule and enter:\nType: SSH\nProtocol: TCP\nPort Range: 22\nSource: 52.205.195.10/32\nDescription: data.world inbound connection\nClick Add Rule and enter:\nType: SSH\nProtocol: TCP\nPort Range: 22\nSource: 52.205.207.86/32\nDescription: data.world inbound connection\nReview: Verify the above settings were entered correctly and create the instance.\nWhen launching the instance, you will be prompted to select an existing key pair or create a new key pair. If you are using a previous key pair, you will need to have a copy of the key .pem file stored on your local computer from when you created the keys. If you're creating a new key pair, download that .pem file now.\nGo to the AWS Management Console and select Compute>EC2.\nSelect Instance>Instances on the left side.\nFind the instance you just created from the list in the main section of the page. Click on it to load its details in the lower part of the page.\nYou'll need to know the Public DNS and Private IP addresses shown for the next step - so keep them handy by creating a new, duplicate browser tab to complete the next set of steps.\nIn your new tab, select Network & Security>Security Groups on the left side of the EC2 page.\nClick Create Security Group\nEnter the following values:\nSecurity group name: SSH to your database name\nDescription: \"allows traffic from the SSH server to database\"\nVPC: this is the same VPC used for the SSH server and database\nWith the Inbound tab selected at the bottom of the window, click Add Rule\nA new row will populate in the list of rules. Enter the following values:\nType: Custom TCP Rule\nProtocol: TCP\nPort Range: the port number of the database\nSource: Custom; in the blank box to the right enter the Private IP address with /32 added to the end\nFrom the AWS Management Console, click on Database>RDS\nSelect Databases on the left side\nClick on the link to your database in the DB identifier column\nOn the database details page, click on the Modify button on the top right\nScroll down to the Network & Security section; from the Security group drop down menu, add the security group that you created in the previous section (e.g. \"SSH to your database name\")\nSave the changes by scrolling to the bottom of the page and clicking the Continue button\nOn the following page, choose when to apply the changes, then click Modify DB instance\nOn MacOS or Linux, open a new Terminal window. For Windows, use an SSH client such as Putty or OpenSSH\nWithin the terminal, navigate to the location where you downloaded the .pem key file you generated\nSet the permissions for the key file to be not publicly viewable, using the appropriate name if your key pair is different than our example of ssh_tunnel:\nConnect to your AWS SSH server from the terminal using the default user for your EC2 instance. AWS will generate the command tailored to your specific instance - to find that command, you can navigate to your EC2 Instance details page and click the Connect button at the top.\nThe general form will be:\nFor other options, please see additional guidance from Amazon at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html\nOnce you're connected to your server, create a user group named datadotworld:\nCreate a user named ddw:\nSwitch to the user named ddw\nCreate a hidden directory called .ssh to upload your public key, setting its permissions appropriately:\nEnable read and write permissions for the owner on that file\nCreate an empty file called authorized_keys and provide read and write access to its owner:\nGet the public key value from data.world by doing the following:\nOpen a new browser tab\nWhile logged into data.world, go to https://data.world/integrations/categories/database and click on the tile of the database type you'd like to connect\nIf that integration has not yet been enabled, click the Enable Integration button (otherwise go the Manage tab and choose Add new connection)\nIn the new window that opens, click the Advanced Settings tab and check the Use SSH Tunnel box\nCopy the SSH public key\nLeave this window open as you'll return to it to complete the configuration\nBack in the terminal, add the public key to your authorized_keys file with the following command:\nInclude the quotation marks but replace <Your Public Key> with the key you just copied from data.world in the previous step\nReturn to the database configuration tab on data.world that you opened in the previous step\nWithin the Advanced Settings, enter the following values:\nSSH host: this is the Public DNS(IPv4) value in your EC2 instance created in Task 2\nSSH user: the name of the SSH user you created in Task 4 - that's \"ddw\" if you followed our suggestion\nGo to the General Settings tab and enter the following values:\nDisplay name: your choice - this is how your database connection will appear in data.world\nHost/IP: the Endpoint value\nPort: the Port value\nConnection username: a valid user in your database instance\nConnection password: the database password for that user\nTest the configuration and save.\nWith your database configured, you can now use any of the Add data mechanisms to import data into a dataset or project on data.world.",
    "url": "https://docs.data.world/en/98774-connecting-to-an-aws-rds-database-through-an-ssh-tunnel.html"
  },
  {
    "title": "Postgres Proxy",
    "content": "Postgres Proxy lets you connect PostgreSQL data science and business intelligence tools directly to data.world.\nPostgres Proxy works by setting up resources as if they were PostgreSQL databases. As a result, you can create live connections between data.world datasets and Power BI, Tableau, or any other tool that supports Postgres connections.\nCombined with data.world\u2019s federated query capability, Postgres Proxy can help you create powerful analyses from a wide variety of data sources.\nIn this section, you'll find setup guides to help you connect data.world to Power BI, Tableau Desktop for Mac, and Tableau Desktop for Windows.",
    "url": "https://docs.data.world/en/99604-postgres-proxy.html"
  },
  {
    "title": "Configuring Postgres Proxy on data.world",
    "content": "To use Postgres Proxy on data.world, you first need to create a PostgreSQL connection in your BI Tool for the dataset or project you want to analyse. Open your BI tool and create the connection with the following parameter values:\nhost: postgres.data.world\nport: 5432\ndb: agentid/datasetid\nuser: {your data.world user id}\npass: {read/write token}\nFor single tenant customers, set host to\u00a0postgres.{site}.data.world\nYou can find the agentid/datasetid in the URL for the resource:\nThe read/write token is located in the Advanced settings in your profile. More information about your API tokens can be found here.",
    "url": "https://docs.data.world/en/99605-configuring-postgres-proxy-on-data-world.html"
  },
  {
    "title": "Connect to PowerBI using Postgres Proxy",
    "content": "On this page, you'll learn how to connect PowerBI to data.world using Postgres Proxy.\nBefore you connect PowerBI, make sure you have access to the dataset you want to connect, as well as your data.world Read/Write API key, which you can find in the Advanced tab of your profile settings.\nFollow these steps to connect PowerBI to data.world:\nIn PowerBI, navigate to Get Data.\nSelect PostgreSQL database:\nOn the PostgreSQL database, enter the following values:\nItem\nDetails\nServer\npostgres.data.world:5432\nDatabase\nThe organization and dataset in <organization/dataset> format. You can find these values in the subdirectory/second half of your dataset's URL. For https://data.world/ddw-doccorp/mydataset, you would enter ddw-doccorp/mydataset .\nData Connectivity Mode\nDirectQuery\nSelect OK, then enter your data.world username in the User name field, followed by your Read/Write API token for the password. Click Connect.\nPower BI displays the tables you can bring in. Select your desired tables, then click Load.\nOnce you have connected PowerBI to data.world, Postgres Proxy maintains a live connection to your dataset. You can test the connection by modifying data in your data.world dataset; changes you make should update live in PowerBI.\nIf your PowerBI tables aren't updating as expected, verify that you still have access to the source dataset and that you're using the latest version of Windows.",
    "url": "https://docs.data.world/en/110905-connect-to-powerbi-using-postgres-proxy.html"
  },
  {
    "title": "Connect to Tableau for Windows using Postgres-Proxy",
    "content": "This page walks you through the Postgres Proxy setup for Tableau for Windows. For Tableau for Mac, refer to Connect to Tableau using Postgres Proxy.\nBefore you connect Tableau Desktop for Windows to data.world with Postgres Proxy, you will first need to create an OBDC Data source.\nFollow these steps to create the data source:\nIn your Windows search bar, search for ODBC Data Sources and select ODBC Data Sources (64-bit).\nWindows opens the ODBC Data Source Administrator modal. Navigate to the User DSN tab.\nFrom the User DSN tab, select Add. Windows opens a new Create New Data Source window.\nIn the Create New Data Source window, select a PostgreSQL ODBC drive option, then click Finish.\nWindows opens the PostgreSQL {ANSI/Unicode/Other} ODBC Driver (psqlODBC) Setup window. You can select any option, though data.world recommends a Unicode option to prevent issues with international characters.\nOn the ODBC driver setup window, you'll need to enter values for Data Source, Database, Server, User Name, SSL mode, and Port. The following table lists all values in the setup window:\nItem\nRequired\nDetails\nData Source\nYes\nA descriptive name for the Data Source.\nDatabase\nYes\nThe data.world Owner and Dataset that will be the source of data for this connection in the format owner/dataset.\nThe Owner may be an Organization or Individual user. You can retrieve the value from the dataset's URL.\nServer\nYes\nThe data.world server to establish the connection with.\nFor Multi-tenant users, the value will be postgres.data.world.\nFor Private Instance users, the value will be postgres.data.world.\nFor Single-tenant or VPC users, the value will be postgres.{organization-site}.data.world.\nReplace {organization-site} with the subdomain value of your data.world URL, <https://{organization-site}.data.world>.\nUser Name\nYes\nThe data.world username for the user credentials establishing the connection. Enter the username for the data.world account with access to the dataset listed in Database, likely your own or that of a service account.\nSSL Mode\nYes\nThe SSL connection type. Set to Allow.\nPort\nYes\nSet to 5432. This is the port used in the connection to the host server.\nDescription\nNo\nAn optional description of the ODBC Data Source being created.\nPassword\nNo\nLeave this field blank.\nOnce you've completed all fields, select Datasource. A new Advanced Options window appears. Select Page 3.\nOn Page 3, enter password=<api-token>, entering your API token after password=. Click Connection test.\nWhen the Connection successful message appears, select OK. Ignore the MSDTC error, if it appears.\nWindows returns you to the Advanced Options window. Click OK.\nWindows returns you to the PostgreSQL ODBC Driver Setup page. Finishing configuring any remaining values, then click Save, followed by Test.\nYou should now see the new ODBC Data source listed on the User DSN tab.\n\nNow that you've finished setting up an ODBC Data source, follow these steps to connect Tableau to your targeted dataset.\nIn Tableau Desktop, find Connect to a server > Other Databases (ODBC).\nThe Other Databases (ODBC) modal window appears. Choose the PostgreSQL ODBC User Data Source you just created, then click Connect.\nTableau processes the connection and populates the Connection Attributes section with values. Leave the values unchanged, then click Sign In.\nTableau then connects you to the data.world dataset.\n",
    "url": "https://docs.data.world/en/142141-connect-to-tableau-for-windows-using-postgres-proxy.html"
  },
  {
    "title": "Connect to Tableau for MAC using Postgres Proxy",
    "content": "This page walks you through the Postgres Proxy setup for Tableau for MAC. For Tableau for Windows, refer to Connect to Tableau for Windows using Postgres-Proxy\nFollow these steps to connect Tableau to data.world using Postgres Proxy:\nBefore you connect Tableau, make sure you have access to the dataset you want to connect, as well as your data.world Read/Write API key, which you can find in the Advanced tab of your profile settings.\nIn Tableau Desktop, navigate to Connect > To a Server, then click More.\nSearch for ODBC, then select Other Databases (ODBC).\nWhen prompted, select the following options.\nItem\nDetails\nDriver\nPostgreSQL Unicode\nServer\npostgres.data.world for private instances and multi-tenant customers; postgres.<your_customer_name>.data.world for single-tenant customers.\nPort\n5432\nDatabase\nThe organization and dataset in <organization/dataset> format. You can find these values in the subdirectory/second half of your dataset's URL. For https://data.world/ddw-doccorp/mydataset, you would enter ddw-doccorp/mydataset.\nUsername\nThe data.world username for the user credentials establishing the connection. Enter the username for the data.world account with access to the dataset listed in Database, likely your own or that of a service account.\nPassword\nLeave this blank.\nString Extras\n\nEnter password=<your api key> from your settings page.\nClick Sign In.\nTableau then connects you to the data.world dataset.\n",
    "url": "https://docs.data.world/en/221018-connect-to-tableau-using-postgrs-proxy.html"
  },
  {
    "title": "Manage resources",
    "content": "On data.world, all discrete items that you can act upon are resources. Resources can be metadata or data resources. For example, business terms, collections, datasets, projects, and insights are all resources. In this section we will look at all the ways you can manage both your data and metadata resources.",
    "url": "https://docs.data.world/en/98780-manage-resources.html"
  },
  {
    "title": "New navigation for collections, glossary, and resources pages",
    "content": "The collections, glossary terms, and resources pages have a new page level navigation available on the left side to navigate long pages with ease. New dedicated tabs are also now available to navigate related and contained resources quickly and efficiently in an info-dense tabular view.\nNote that datasets and projects pages are not affected by this change.\nTo navigate the new pages:\nBrowse to any resource, glossary term, or collection you have access to. From the Resource page, opt in to preview these features.\nOn the Overview tab, you now have access to a new left-side navigation that makes it easy for you to browse the different sections on the page. This comes in handy when the page has a large number of sections with lots of content.\nNext, you will notice new dedicated tabs that display the Related resources and Contained resources. For resources like Tables, you will see a Columns tab which display the columns in the table.\nTables will always have Columns and Related tabs.\nAnalysis will always have the Related tab.\nBusiness terms  will always have Related tab.\nCollections - will always have Contains tab and may have Related tab.\nGeneric resources may have Related tab.\n\nThe Related tab only shows up when relationships are defined for the resource. For Tables, Analysis, Business Terms, and Columns relationships are defined by default and hence the Related tab is always visible on them. For every other resource type (including Collections), users have to define the relationships and the tab is only displayed when these relationships are defined.\nFrom here you can navigate to the different related resources, remove the resources, or Add new resources. Use the Expand feature to view the resources in full screen mode.\nWhen you are in full screen mode, you can navigate to the different related resource from that view. Click the Collapse button to back to the normal view.\nThe Contains tab shows for Collections and displays all the resources contained in the collection. Navigate to the different sections in the tab to view the different resources contained in the collection. Use the Expand feature to view the resources in full screen mode.\nIf you have the appropriate permissions, you also get an option to do quick edit of the resources.\nFor Tables, you get a Columns tab that lets you view all the columns in the table. Leverage the Expand feature to view all the columns in full screen mode.\nOn all these tabs, all the fields show hover text that include complete details about the field when the text cannot be displayed in the column view. For fields like Created and Updated hover over the field values to see the actual date and time information.",
    "url": "https://docs.data.world/en/115761-new-navigation-for-collections,-glossary,-and-resources-pages.html"
  },
  {
    "title": "Manage collections",
    "content": "",
    "url": "https://docs.data.world/en/109294-manage-collections.html"
  },
  {
    "title": "About collections",
    "content": "A collection is a way to organize your catalog metadata resources into logical groupings and hierarchies. You can think of them like folders or directories. Collections have their own landing pages that can be configured to add sections and fields to provide context about the group of metadata resources in the collection.\nYou can create different types of collections and organize your collections in a hierarchical structure. You can also apply access control to collections, granting edit or administration access to the collection and the metadata resources contained within to a group or individual. Go here to learn more about these features.\nOn the Organization Profile Page you can view tiles for all of your collections. To manage all the collections in an organization, go to the Collections tab. Here you can use filters to narrow down the list of available collections and create new collections using the New button.\nClick a collection on the Organization Profile Page to see resources that are assigned to that collection.\nThe Overview tab has an About this collection section and Technical details section.\nIf the collection is part of a collection hierarchy, you will also be able to view the Collection Hierarchy details and navigate it.\nThe Contains tab displays the list of tables, business terms, and analysis that are in the collection. From here you can launch the Quick Edit window to edit the resources in the collection.\nThe Related tab displays the list of related resources to a collection term. This tab is only displayed if the relationships are defined for the collection.\nOn the Settings tab you can see who has access to the collection and give members and groups access to the collection. You can also delete the collection from here.\nThe application provides a simple way for navigating to collections within collections.\nTo navigate the collection hierarchy:\nBrowse to the collection page that has child collections or parent collections.\nYou will see the collection with the list of expandable and collapsible menu of all collections within it.\nFrom the list, click a name of the collection to navigate to the collection.\nWhen you reach a point where the collection does not have any child collection, you will see the breadcrumb to navigate back to the parent collections.",
    "url": "https://docs.data.world/en/109295-about-collections.html"
  },
  {
    "title": "Planning collection & permissions for collections",
    "content": "Types of collections: First, plan the collections structure for your organization. Determine what types collections you need. For example, you can follow the following paradigm.\nCreate the different types of collections by following this documentation.\nType\nUsage\nSource collections\nFor each collector output.\nDomain collections\nOrganizational categories that map to the domains of your business or organization. Domain collections are likely curated by data stewards and can include subdomain collections (that is, hierarchical collections).\nGlossary collections\nFor business glossary in the organization.\nHierarchy of collections: Second, plan if you want to create a flat list of collections or a hierarchy of collections. Collection hierarchy is a feature allows you to create collections within collections to better organization your resources in small logical groups under a larger umbrella.\nDecide if you want to set access permissions on collections to govern who can view the content of the collections, edit them, or give others access. Setting granular permissions on collections help you achieve the following business needs:\nYou want to put people in charge of a smaller footprint of the catalog, and have a good reason why those same people should not have administrative permissions for the entire catalog.\nYou want to split up roles between different groups for things like running collectors vs curating a glossary.\nYou want to make sure that people are only receiving notifications for things they are responsible for and want to direct things like suggestion approvals to a specific audience.\nYou want to hide part of the catalog from certain groups. Note that hiding certain parts of the catalog from members will impact their experience of the catalog and is not a recommended way of setting up the catalog. For example, it may happen that members will not be able to see all related resources for a catalog resource. It will also impact lineage as members will not be able to see lineage if they do not have view access to the resources that are part of lineage.\nPermissions for collections and datasets and projects are managed separately. Datasets and Projects cannot be added to collections at this time and hence their permissions are managed differently.\nLevels of access\nYou can grant permissions for metadata resources at two levels:\nCatalog level  - For the entire metadata catalog: Permissions can be granted to organization groups.\nCollection level - For a specific collection within the catalog: Permissions can be granted to individual organization members or to organization groups.\nSupported access levels for metadata catalog resources.\nPermission\nWhat the member can do\nNot set\nMembers cannot search for or navigate to the resource. The resource does not exist for the member.\nNOTE: If members have access to the collection through organization level or parent collection level settings, they will be able to access the collection.\nView\nMembers can search for and navigate to the collection, its metadata resources, terms and its subcollections.\nEdit\nView permissions\n+ Member can edit the collection, its metadata resources, terms and its subcollections. Can create metadata resources and terms. Can approve suggested changes.\nManage\nEdit permissions\n+ Members can create subcollections. Can delete the collection, metadata resources and terms and subcollections in the collection. Can manage permissions for collection.\nWhen resources belong to multiple collections:\nIf a resource belongs to two collections and you have Edit access on one collection and View access on another, you will get the highest level of access, that is, you will be able to edit the resource.\nLikewise, if at the organization level you are granted Edit access to all catalog resources and given View access to a specific collection, you will be able to edit the collection and the resources in it.\nIf at the organization level you are NOT granted any access to catalog resources and given View access to a specific collection, you will be able to view the collection and the resources in it.\nAlso note that when you have access to tables through a collection, you automatically get access to the columns for those tables, even if the columns are not part of the same collection.\nWhen you have collection hierarchy setup:\nIf you have View access to the parent collection and no access to the child collection, you will be able to see both the parent collection and the child collection and resources in both the collections.\nHowever, if you do not have access to the parent collection, but can View the child collection in it, you will be able to view only the child collection and the resources in it. You will not have access to the parent collection or any resources in that parent collection.\nOn the child collection page, you will see that the collection has a parent, but that parent will not be something you can view. Clicking on the parent from the child collection will display a page not found error (a 404 page) with a notice that the user may not have access to the resource they are trying to visit.\nIf a collection has two child collections, and you have View access to the parent collection and Edit access to only one child collection, you will be able to view the parent collection and both child collections, but you will be able to only edit the collections (and the resources in it) for which you have Edit access.\nWatch this video for an overview of how access control works:",
    "url": "https://docs.data.world/en/109467-planning-collection---permissions-for-collections.html"
  },
  {
    "title": "Creating collections",
    "content": "\nTo create a new collection of metadata resources:\nGo to the Organization Profile page.\nClick the New button on the Overview tab > Collections section or on the Collections tab.\nIn the Add a new resource window, provide the following details and click Continue.\nCollection title: Provide a name for the collection.\nOwner: By default the owner is set as the organization where you are creating the collection.\nChoose type: Select a type. The default option is Collection. You will see more options if you have added different type of collections.\nChoose parent: You need to set this option only if you want to add the collection as a subcollection of another collection. Leave it blank if you don't want to convert the collection into a subcollection.\nThe Edit collection window opens where you get an option to add a description to your collection as well as any relevant tags. You can either skip this step, or make your changes and click the Save changes button. Once your changes have been made your new collection is ready to use.\nOnce you save the collection, the Collection Details page opens. From here you can click the New resources button to add new business terms, analysis, or custom resources to the collection. To add existing resources to the collection, you have to navigate to the resource and add it to the new collection from that resource page.\nThe resources added to the collection are available in the Contains tab. Navigate to the different sections to see the list of specific resources added to the collection. Sort the lists by using the available sorting options. Use the\u00a0Expand\u00a0feature to view the resources in full screen mode.\nAll the fields show hover text that include complete details about the field when the text cannot be displayed in the column view. For fields like Created and Updated hover over the field values to see the actual date and time information.\nWhen you are in full screen mode, you can navigate to the different related resource from that view. Click the Collapse button to back to the normal view.\nCreate subcollections to set up a hierarchy of collections.\nA collection can have a maximum of 5 subcollections. Only users with the Edit catalog or Manage catalog access at the organization level can create subcollections.\nTo create a subcollection:\nGo to collection page for which you want to add a subcollection.\nClick the New subcollection button. Follow the process of creating a collection. The value in the Choose parent collection field is set to the collection you are in and cannot be changed.\nAfter you have created a collection, you can move it to other collections by editing the collection.\nIn the Edit collection window, go to the Hierarchy tab and select the new parent collection.",
    "url": "https://docs.data.world/en/98782-creating-a-collection.html"
  },
  {
    "title": "Setting permissions for collections",
    "content": "After you have created your collections, assign permissions to members and groups. To learn about planning collections and permissions for collections, see this documentation.\nDetermine what permissions you want to grant to all members of the organization for all metadata resources in the catalog\nDefault for new organizations: All members have NO access to all metadata catalog resources. This allows you to control access while setting up your catalog.\nRecommended: Once your catalog is ready to be shared, we recommend you give all members View access to all metadata catalog resources.\nDetermine what permissions you want to grant to specific groups of the organization for all resources in the catalog.\nFixed permission for Admins group: Manage access to all catalog resources, datasets & projects.\nFixed permission for Authors group:  Edit access to all catalog resources.\nRecommended: Add a Catalog Admins group and grant Manage access to all catalog resources.\nDetermine what additional groups are needed to manage the content within the collections.\nMember Group: Name the user groups and set their description. If you plan to use the group for collection-based access, you will likely want to leave the Group Access roles undefined - as you will set the roles for the collection when you assign the group to one or more collections.\nRecommended: If the content is organized by domain, create an administrator group per domain. We recommend you name the user groups to reflect both the collections and role it will be assigned. For example, the Marketing Admin group is named as such because it will be assigned to the Marketing collections with the role of Manage.\nAssign groups (and permissions) to collections.\nTo learn everything about creating and managing user groups, please see this documentation.\nTo assign permissions to collections:\nGo to the Organization Profile page.\nLocate the collection for which you want to adjust the permissions.\nClick the Grant access to all members button to give all users access to the collection.\nIn the Grant access window, the organization name is selected by default and you can't change that. Set the permissions by selecting one of the following: View, Edit, Manage. Click the Grant access button.\nIf you want to give access to only selective members and user groups, click the Grant access button on the Settings tab. In the Grant access window, search for the member or group that you want to give access to the collection. Select one of the permissions: View, Edit, Manage. Click the Grant access button.",
    "url": "https://docs.data.world/en/109298-setting-permissions-for-collections.html"
  },
  {
    "title": "Bulk uploading and editing resources in collections",
    "content": "The application provides you with an option to bulk upload resources and glossary terms in a collection using spreadsheets. You can also use the same feature to download and edit your existing resources and glossary using the bulk edit capability. Note that if you want to bulk edit all the glossary terms in your organization, you must use the bulk-edit feature available on the Glossary tab of the Organization profile page. For all other resources, you have to do bulk editing per collection. It cannot be done at the organization level.\nThe bulk download and upload feature should be used only to manage and enrich metadata on resources within a collection. This feature is not intended for downloading the resources from one collection and uploading it to another collection.\nIf this is the first time you are adding resources to a collection, download the template from the application, follow the instructions in the spreadsheet and upload your resources to the application.\nIf you are working in a collection that already has resources, download the existing contents as a spreadsheet and make edits to it or add new resources. Once you are done, upload the updated spreadsheet to the application.\nOnly resource types that are allowed to be created from the UI, will be available as sheets in the spreadsheet. If a metadata attribute is marked as a source of record, then it will not show up as a column. Similarly, if a Resource type is marked as a source of record, it will also not show up as a sheet in the spreadsheet.\nThe spreadsheet has clear guidelines on how to make edits and set field values. You must follow those instructions carefully to have a smooth experience.\nIf a collection has subcollections, the exported file from the parent collection will not include contents of the subcollections. You have to go to the subcollection to perform the bulk edit action on that collection.\nYou cannot use the bulk feature to move resources across collections.\nYou cannot use the bulk feature to delete resources from collections.\nNote that only users with the\u00a0Edit\u00a0and\u00a0Manage\u00a0access to\u00a0the specific collection or\u00a0Edit\u00a0and\u00a0Manage\u00a0access to All catalog resources\u00a0at the organization level can bulk edit resources in collections. For details about permissions, see Planning user groups and their access levels.\nIn this section we walk you through the basic structure of the spreadsheet and also list the various things to avoid so that you do not run into issues while uploading the updated file in the application.\nKey things to note are:\nAll authors must work on the same downloaded spreadsheet and upload one consolidated version once all the changes are done. Also, while authors are working on the downloaded spreadsheet, you should avoid making changes to the glossary and resources from the UI.\nMake sure you always download the latest glossary and resources spreadsheet from the UI for edits.\nYou can edit the file in\u00a0Microsoft Excel\u00a0or in\u00a0Numbers. However, the application allows only uploading files in .xlsx\u00a0format. If you edit the file with\u00a0Numbers, you must export it to Excel (File > Export to > Excel) before uploading it to the application.\nThe downloaded spreadsheet file has a set structure and has all the sheets you need to add and edit the resources. Read the instructions available in the Overview tab carefully before you start editing the file.\nEach sheet in the spreadsheet is a resource type. In the sheetname there is a circle that indicates if it\u2019s empty or not (empty circle = empty sheet). Sheets are sorted in alphabetical order.\nRead the instructions on other sheets as well to follow guidelines for setting values for fields. Each sheet tells you if you can only edit the fields or if you can add new resources.\nYou should only\u00a0Edit cell values\u00a0and\u00a0Add new rows. Do not delete rows or columns. Do not add, delete, or move the header rows.\nMake sure that all the fields that are marked as required have a value set in them.\nDo not move resources from one sheet to another by using cut and paste.\nWhile adding dates to field values, follow the format: YYYY-MM-DD .\nPossible error scenarios while uploading the excel sheet\nWhen you upload a file that does not follow the guidelines, you will be notified with Error messages and Warning messages.\nAn error state indicates that the file cannot be uploaded without fixing the issue.\nA warning state means, two things can happen: if the required columns\u00a0are not filled out correctly, the specific resource with the warning is skipped. If the\u00a0non-required columns\u00a0are not filled out correctly, that\u00a0particular edit to the resource\u00a0is skipped.\nThe following table explains the possible error scenarios.\nScenario\nType\nWill import succeed?\nMaximum file size of 500 MB exceeded.\nError\nNO\nIf a file has more than 20,000 edits, we recommend splitting up the uploads. Contact data.world Support for assistance.\nError\nNO\nAll or some of the required columns [Title] are missing from the uploaded file. (for edit and add)\nError\nNO\nValues are missing for required columns [Title] (for edit and add).\nWarning\nThe entire resource will be skipped. Rest of the edits in the spreadsheet are imported.\nUser sets a value of a field that is not valid. For example, if the possible values for a field are Active and Inactive and user sets the value as Not in use.  This applies to fields that have pre-defined values available, for example, multi-select fields, fields with values available as drop down options.\nWarning\nThe particular edit to the resource\u00a0will be skipped. Rest of the edits in the spreadsheet are imported.\nDate value is not set in correct format. Dates must be in the YYYY/MM/DD format.\nWarning\nThe particular edit to the resource\u00a0will be skipped. Rest of the edits in the spreadsheet are imported.\nIf the metadata profile changes between export and import and a new field gets added or removed from the metadata profile.\nWarning\nThe column is skipped. Rest of the edits in the spreadsheet are imported.\nMake sure you always download the latest spreadsheet from the UI for edits.\nTo download resources from a collection:\nOn the Organization profile page, go to the Collections tab. Locate the collection where you want to perform the bulk action.\nOn the Settings tab, in the Edit resources in this collection section, click the Download resources button. It downloads a spreadsheet with the resources in the collection. If the collection is empty, you get a template for creating resources in the collection.\nMake sure you always download the latest spreadsheet from the UI for edits.\nTo edit the resources in Microsoft Excel:\nOpen the downloaded file in Microsoft Excel. Make edits to the different sheets.\nFollow the instructions on the Overview tab carefully in the downloaded file to avoid unintended errors.\nTo edit the resources using Numbers:\nOpen the downloaded file in Numbers.\nFollow all the guidelines on the Overview tab in the file and make edits to the content of the file.\nOnce you are done, export the file to Excel by using the File > Export to > Excel option. The export setting must be set to One per sheet. Do not set the value to One per table.\nMake sure you always download the latest spreadsheet from the UI for edits.\nTo upload the updated spreadsheet:\nOnce you are done making edits to the file, you can quickly upload to the application.\nOn the Organization profile page, go to the Collections tab. Locate the collection where you want to upload the updated spreadsheet. It should be the same collection from where you had downloaded the resources for editing.\nOn the Settings tab, in the Edit resources in this collection section, click the Upload document button.\nOn the Edit collection resources screen click the Upload from computer button or drag and drop the updated spreadsheet. The file must be in .xlsx format.\nThe application does a check on the file you are uploading. If any errors are found you are prompted with a list of issues you need to fix before you can upload the file. You cannot continue with the upload if there are errors in the file. If there are warnings, continuing will mean that the changes with warning are skipped and rest of the changes are saved in the application.\nIf there are no errors, you are shown a list of changes to review. The preview window has a tab for each resource type and the changes in these tabs are further separated into two tabs - Edit, New. The window does not list all the changes made to the resources. Only the name and description of the resources that are updated or created are displayed.\nClick the Import resources button to continue with the import. You are notified when the import is successful or if there are any errors.",
    "url": "https://docs.data.world/en/107548-bulk-uploading-and-editing-business-glossary.html"
  },
  {
    "title": "Manage custom resources",
    "content": "",
    "url": "https://docs.data.world/en/98785-custom-resources.html"
  },
  {
    "title": "About custom resources",
    "content": "The product offers support for a wide variety of resources, such as, business terms, analyses, metadata tables, and columns. However, if you want to catalog resources that don\u2019t fit under any one of these categories, you can use the Custom Resources to get such data in the system.\nThe types of custom resources that users can create in the organization are defined in the Metadata profile of the organization. Once that is defined, users can create these resources from the UI and can manage them like any other resources, that is, edit them, view the data lineage (where applicable), make change suggestions, discuss, and delete them.\nProcess for creating custom resources:\n1. Define the resources in the MDP file. Identify and configure what all resources users should be able to create from the UI. For details about doing this task, please see this documentation.\n3. Create the collections for storing the custom resources. Define permissions on these collections to control who can create resources in these collections.\n4. Create and manage the resources from Organization Profile page.",
    "url": "https://docs.data.world/en/110658-about-custom-resources.html"
  },
  {
    "title": "Accessing Custom resources",
    "content": "From the Organization Page\nLike all other resources, Custom resources are available from the Resources section of the Organization page.\nUsing Search\nUse the following search queries to get a list of the custom resources:\nTo view all the custom resources you have access to: resourcetype:catalogEntry\nTo view the Custom resources by owner: owner:\"Owner_Name\" AND resourcetype:catalogEntry",
    "url": "https://docs.data.world/en/98786-accessing-custom-resources.html"
  },
  {
    "title": "Creating custom resources",
    "content": "To create custom resources:\nOn the Organization profile page, go to the Resources tab or to the collection where you want to add the resource. Click the New resources button and select Other resources. This option appears when at least one custom resource is allowed to be created from the UI.\nYou can also create the custom resource from a specific Collection details page. Browse to the specific collection and click the New Resource button on the Collection details page. Select Other resources.\nIf you are allowed to create resources in specific collections only, this is the only place from where you can create the custom resources.\nIn the Add new resource window, set the following and click Continue.\nProvide a title for the resource.\nSelect the owner. By default, the owner is set to be the organization where you are creating the resource. If you are creating the custom resource from the\u00a0Collection Details\u00a0page, the owner is set automatically and the dropdown is disabled.\nChoose from the available types.\nChoose the collection in which you want to create the resource. If you are creating the custom resource from the\u00a0Collection Details\u00a0page, the collection is set automatically and the dropdown is disabled.\nOn the Edit resource window, set the rest of the properties of the custom resources. Click the Save changes button.",
    "url": "https://docs.data.world/en/110654-creating-custom-resources.html"
  },
  {
    "title": "Editing Custom resources",
    "content": "Users with the Manage and Edit access can make edits to the resources to constantly improve the quality of the resources. Users with View access to the resources get an option to Suggest Edits to the resources which is a great way of making improvements to the content of the resources.\nTo edit a custom resource:\nFrom the List page of the resources, click the resource you wish to edit.\nIn the Overview section, click the Edit button to make required changes from the Edit window.\nOn the Overview tab, set the title and description and add tags to the resource. You can also change the Collection of the resource. A resource can belong to multiple collections.\nOn the Status tab, set the status as Approved, Pending, Deprecated, Rejected, Warning (these options can vary for each organization). Even though the status is optional, it is a good practice to set a status as it helps users of the data to identify the reliability of the data.\nIf other users have suggested changes, you will see a note on the top of the page. Click the Review suggestion link to get to the Approvals tab where you can see the suggested change.\nOn the Approval tab, review and accept or reject the suggestion.\nIn addition to suggesting changes to a resource, users can also start discussions around a resource. Go to the Discussion tab to monitor the ongoing discussions and participate in them. The Discussion tab captures all the communication between users about the resource.\nYou get an option to Flag or Like comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nThe contents on the Discussion tab are grouped into topics. Use the default topic or start a new one. Define the discussion topic, the category (General or To dos), and the first comment for the topic. Comments in topics are written in Markdown and can contain images and links.\nComments in topics are written in\u00a0Markdown\u00a0and can contain images and links. You can mention users (using\u00a0@user_name)\u00a0to notify them about an ongoing discussion to get their attention and participation. Some important things to note:\nWhen you do this for\u00a0private resource only available in an organization, typing\u00a0@\u00a0suggests only the members of that organization. It also shows a prompt to contact an administrator of the organization to ask them to allow others to have access to the organization. If you are an administrator, you get the option to invite other people to the organization.\nFor\u00a0community resources, you get a list of suggested users from the community.\nIn both cases, only the users who have at least\u00a0read access\u00a0to the resource are notified. So make sure that when you are setting access for the users, you also give them access to the\u00a0resource\u00a0where they need to participate in the discussion.",
    "url": "https://docs.data.world/en/98787-editing-custom-resources.html"
  },
  {
    "title": "Managing access",
    "content": "Access permissions for Custom resources are inherited from the user groups you belong to or the collections in which the resources reside. You can either have no access, view access, or edit access to the resources. For details about these permissions, see Planning user groups and their access levels.\nIf you feel that no one should be using the resource, you can easily delete it from the system.\nDeleting the resource removes it from your catalog, not from the original source.\nTo delete a resource:\nFrom the List page of the resources, click the resource you wish to edit.\nIf you feel a resource is redundant and should not be used by anyone, from the Settings tab, click the Delete button to remove the resource.",
    "url": "https://docs.data.world/en/98788-managing-access.html"
  },
  {
    "title": "Eureka Explorer lineage for catalog resources",
    "content": "Eureka\u2122 Explorer is a visual map of your data and relationships powered by knowledge graph. It is a visual representation of your data catalog. Lineage combines the powerful automated metadata lineage collection with a visual tool so your data teams can quickly and visually browse the lineage relationships of your data resources. It includes the richness of the data catalog and provides one screen where lineage can be explored. With this capability, your teams will no longer spend hours and days trying to find answers to their root cause, impact, and compliance questions. They can see these relationships on one screen and answer their questions in a matter of a few clicks.\nExplorer lineage delivers both a general preview (aggregated summary of how data flows from its source to where it is consumed) and a fully interactive graph of your technical lineage (table, column, and query-level lineage).\nExplorer lineage is available to Enterprise customers on certain plans. Please contact your Customer Success specialist to find out how to enable Explorer lineage for you organization. Once Explorer Lineage is enabled for your account, the information is automatically collected and displayed in data.world when a supported collector is run. To view the list of data sources for which automated lineage collection is available, please see Currently supported data sources.\nSome questions that lineage information answers for you\nWhat data and analytics do I have? Where does it live? Can I trust it? What are my bottlenecks and hotspots?\nHow does it connect? How was it created or derived?\nWhen stuff breaks, why? What will break or be affected when I change something? Who should I talk to?\nHow does sensitive data flow? Is it being handled properly?\nBuild trust in the data you are seeing: Business analysts or a decision makers relies on a dashboard or a report or a particular table and need to know if they can rely on the data source in order to make accurate decisions. That means knowing the context and being able to look upstream to know where does it come from, how was it derived, and be able to know if you can trust how the information was derived. And when something breaks, for example, if your dashboard is not working anymore, you can look upstream and see that information is coming from Snowflake, you can reach out to the tech owner and inform them that the dashboard is broken because the data source has issues. When you look upstream, you can see what is upstream, visualize the source and the relationships of things and being able to see metadata about the different parts of the lineage graph, for example, who is the tech owner, what is the policy, that is the status (Approved, Deprecated).\nRisk and impact analysis: Data producers and data engineers need to know when they want to change something, how are they going to impact the users of the data. This also helps them troubleshoot when they accidentally broke something, who all did they impact. For example, when they are looking at the Lineage of a data source they can see that there are a bunch of people down streams in Tableau who are building dashboards using a data source that you are about to change and they need to be informed about the change. This helps data producers and data engineers be better stewards to the organization.\nYou can expect a lineage relationship between two objects from two source systems if we have the two collectors for those sources and those collectors harvest the lineage relationship. The exceptions to this statement are 1) the source metadata is lacking the relationship or 2) the source metadata is lacking enough information to positively identify both resources.\nThe system allows you to enhance the lineage pages to highlight key metadata about the resource. You can do this by adding custom fields to the sidebar of the pages. To learn everything about the out-of-the-box fields that are available in the sidebar and instructions on adding custom fields, please see this documentation.\nUsers must have View access to all resources in the lineage diagram in order to be able to view the Explorer Lineage section on the Overview tab and to open the Explorer lineage in full screen mode. To achieve this, add the users in a user group and give that group View Catalog access at the Organization level. Details about creating user groups and setting permissions, are available here.\nTo view the list of data sources for which automated lineage collection is available, please see Currently supported data sources.\nTo view the Explorer lineage:\nLocate a resource for which automated lineage metadata is available. On the resource page, scroll down to the Explorer lineage section.\nThe Explorer lineage section shows a preview of the asset that you are currently looking at and upstream resources from where this resource came from and downstream resources that are coming from this particular resource. This gives a quick overview of how the resource fits into the larger landscape.\nClick the View in fullscreen button to see the full details and to interact with the lineage information.\nUsers must have View access to all resources in the lineage diagram in order to be able to open the Explorer lineage in fullscreen mode.\nTo interact with the Explorer lineage:\nOn the resource page, in the the Explorer lineage section, click the View in fullscreen button to open the full view of the lineage.\nThe right-side shows the Lineage summary which summarizes all the resources that are part of the specific lineage. Click through to the various nodes in the summary view or use the Filter resources option to find the resource by name.\nFocused lineage view puts the main resource that you have selected into the center and shows how it fits into the overall landscape.\nFor example, we can see that the RETAIL_ORDER database table was derived using DBT from the RETAIL_ORDER_STAGE table, which was brought into the Snowflake database using FiveTran from a SQL server database. This provides a quick, at a glance view of how this information passes through the overall set of environments.\nExpand the nodes and see more information such as the columns that are a part of a table and see how information moves at that field or column level. This kind of traceability gives you the ability to see how this information moves from a technical level for root cause analysis, impact analysis, or sensitivity analysis.\nFor example, if a particular column has a warning on it, it shows to the user that there might be some problems with it and the user should use caution when using dashboards or other analysis that leverages this field.\nNote that when columns are part of the table but not included in the lineage, they are not connected to the other components of the lineage and show under the Shownmore resources. You only see this option for the resource for which you are viewing the lineage.\nTo focus on the section of the lineage that you are looking at, click the Re-center view button. Select outside and click Re-center view again, to zoom back to the full lineage view.\nYou can use the lineage diagram to learn about other related resources.\nFor example, if we expand the related embedded data source Orders and choose Days to ship Scheduled, we can learn more about this calculated field. In the right side you can view the details about the field along with the formula that is used to calculate the field information. Click View lineage of this resource to see the detailed lineage of the specific field.\nUse the following shortcuts to interact with the Explorer lineage. Access the list of shortcuts by clicking the ? button on the Explorer lineage page.\nAction\nKeyboard shortcut\nMulti-select\nClick and drag while holding down SHIFT key on the keyboard.\nExpand selection\nSelect nodes that have > in the top left and click E on the keyboard.\nCollapse selection\nSelect nodes that have V in the top left and click C on the keyboard.\nRecenter\nSpacebar\nPan around\nArrow keys\nJump through node selections\nTab\nThe system provides an option to export and download Explorer Lineage information as an excel file. Before downloading, users can preview the data and decide to pre-filter the data prior to export. The export summary provides an aggregate of what is about to be exported. As the data is filtered, the summary updates.\nTo download the Explorer lineage information:\nOn the resource page, in the the Explorer lineage section, click the View in fullscreen button to open the full view of the lineage.\nOn the Explorer lineage page, click the Download as CSV button.\nThe Download lineage of this resource window opens. It shows a summary of what all is included by default for the selected resource. Use the following filters on the page to narrow down the data you want to download. The summary information on page updates as you filter items.\nLineage: Select if you want to download only upstream or downstream information or both.\nResource types: Select from the resources available for the lineage. For example, you will see options like Database columns, Tableau Dashboards. You can select multiple items from the drop down list.\nSource: Select the source of information. This is the list of each source technology from which the resources were collected and sourced. For example, you will see options like Snowflake, dbt, Fivetran, etc. You can select multiple items from the drop down list.\nStatus: Select from statuses, such as, Approved, Pending, Warning. If you want to download a list of resources that have no status, select Resources without a status. You can select multiple items from the drop down list.\nSteward: Select from the available stewards or select Resources without a steward to download a list of resources that have no steward. You can select multiple items from the drop down list.\nClick the Preview button.\nThe next screen shows a preview of the information that will be downloaded. It includes two additional fields along with what was selected for downloading: Distance from focused - how far the related resource is from the focused resource (NOTE: distance 0 means it is the focused resource) and Parent - the name of the parent resource the listed resource is part of (if applicable).\nClick the Back to filters button to adjust the filters or click the Download button to download a copy of the information.",
    "url": "https://docs.data.world/en/105852-eureka-explorer-lineage-for-catalog-resources.html"
  },
  {
    "title": "Bulk editing metadata",
    "content": "As the number of resources you have grows, it can be difficult to make changes to them and keep them all in sync. Fortunately data.world allows you to select and edit multiple datasets, projects, or catalog resources at one time. As with editing a single resource, you can edit multiples of any resource to which you have write permissions. The edits you can make are to the metadata for the resource--you cannot edit the data itself.\nThis feature does not support:\nModifying the contributors to a dataset\nModifying relationships to and from catalog resources\nBulk editing is available for the following resources in the application:\nDatasets\nProjects\nThe following additional resources are available only for organizations with the Enterprise license:\nGlossary (for details, see Bulk uploading and editing business glossary.)\nTables\nAnalysis\nSubtypes of Glossary, Tables, and Analysis resources",
    "url": "https://docs.data.world/en/98789-bulk-editing-metadata.html"
  },
  {
    "title": "Editing multiple resources",
    "content": "To get to the resources available for bulk editing:\nFor a list of resources available for bulk editing in an organization, go to the Organization Profile Page, Resources tab. From here you can bulk-edit Datasets, Projects, Analyses, Tables.\nTo start the bulk-edit process:\nClick the Three dot menu to access the options available for editing multiple resources. The options available will differ based on where you start the bulk-edit process.\nOn the Organization's Profile page, Resources tab you can use the filters available to narrow down the list of options.\nEach editing session has three parts:\nSelect the resources\nEdit the resources\nReview the edits\nOn the Select screen, select the resources you want to update and click Edit.\nImportant things to note:\nThe Select screen only shows the resources that you are allowed to edit.\nOnly 250 resources can be selected for editing at a time.\nOn the Edit screen, select the changes you want to make. Only metadata resources with the same owner can be edited together. If you choose to edit resources with different owners, they can be edited in groups, but only groups with the same owner.\nThe simplest data that is included for editing on every resource is tags. Every resource has at least an \"Overview\" section which includes the ability to edit the Tags field.\nMany organizations, however, have more complicated metadata profiles, and allow for more complex metadata to be associated with the resources. When an organization has defined a metadata profile, then the edit interface will adapt to the profile. For a complex organization, there may be many fields divided into various sections:\nThere are a few options when editing fields.\nIn fields like Tags, users can Add to existing, Replace existing, or Clear all values:\nIn fields like Status\" you get an option to either select a status or remove status by selecting No status.\nOther fields are simple text fields. In this case, you may either elect to not change the current value, or to replace the value entirely.\nTo clear the value for such fields, select Replace existing and leave the field blank.\nSome fields have complicated hierarchies of values. Those are often separated into their own section. In this case, the user can use: Add to existing, Replace existing, or Clear all.",
    "url": "https://docs.data.world/en/98790-editing-multiple-resources.html"
  },
  {
    "title": "Reviewing changes",
    "content": "Once you have set all the values you want to change, click the Review changes button.\nYou will then be presented with a list of changes to confirm. At any point, you can go to a previous step to make changes. When you are satisfied with your changes, click the Confirm changes button.\nIf you made changes to different groups (for example, those owned by you and those owned by your organization) the changes shown are unique to each group.\nIf you want to add tags to all resources in every group, for example, you would need to make the change in each group.\nFinally, select the Yes, send notification emails option to notify the admins and content contributors of the resources you have changed. By default this option is turned off as a separate email is sent for every resource that has been modified.",
    "url": "https://docs.data.world/en/98792-reviewing-changes.html"
  },
  {
    "title": "Manage business glossary",
    "content": "",
    "url": "https://docs.data.world/en/107546-manage-business-glossary.html"
  },
  {
    "title": "About business glossary",
    "content": "data.world University!\nCheck out our What is a business glossary video!\nThe Business glossary feature is available only for organizations with an Enterprise license.\nThe business glossary of the metadata catalog brings your business terms and their definitions into the heart of working with your data. It enables you to provide standardized definitions for the terms used in your catalog to ensure that everyone using the data in the catalog has the same understanding of the terms and are using a common language about the data in your organization.\nCreate sub types for your business glossary to better organize the terms and make it easier for the users of the data to explore and use the terms. For example, you can create sub types like Metrics, Policies, Business concepts, etc. to organize and manage your business glossary.\nOnce the business glossary is established, use it by linking appropriate terms to tables, columns, datasets, projects, and other custom resources in the application. Since different business units can have their own glossary terms, each organization has its own glossary.\nNote that only users with the Edit and Manage access to Catalog resources can add and edit business glossary terms. For details about permissions, see Planning user groups and their access levels.\nTo begin with, the best and easiest way to create your business glossary is by bulk uploading the terms using Microsoft Excel.\nYou can always add new terms by using the New button in the UI.\nFor editing terms, either edit single terms from the UI, or use the UI driven bulk edit, or download the terms as an Excel sheet for bulk editing.\nMake sure you have access to Microsoft Excel or Numbers. You will need one of these tools to edit the file for bulk uploading and editing.\nMake sure you have created the different sub types for business glossary and set the various custom sections, and custom fields you want to use for the different sub types of business glossary. This is done using the Metadata Profile. If you have the Catalog toolkit deployed on your system, use this documentation.\nMake sure you have created Collections in your organization where you want to add the business glossary terms. Generally, you would create collections based on subject matter expertise and organize your terms in those collections.",
    "url": "https://docs.data.world/en/109109-about-business-glossary.html"
  },
  {
    "title": "Business glossary details page",
    "content": "When you click a business term in a glossary, the details page for that term appears.\nThe Overview tab is the default view of this page, and presents the metadata for the glossary term.\nThe metadata displayed on the Overview tab is completely configurable with our metadata cataloger. In the following example, you see the Name of the term, the status, and the resource in which it is found. You can also see a Description, Summary, and More Information.\nThe other sections available on the Overview tab are the About this Business term, Data lineage, and Technical details.\nThe Related tab displays the list of related resources to a business term. From here you can add and remove the related resources. For details, see the Associating business glossary with resources section in Using business glossary\nThe Discussion tab captures all the communication between team members about the business term. You get an option to Flag or Like comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nThe contents on the Discussion tab are grouped into topics. Use the default topic or start a new one. Define the discussion topic, the category (General or To dos), and the first comment for the topic. Comments in topics are written in Markdown and can contain images and links.\nThe Settings tab lists the permissions for accessing the business term. From here you have an option to transfer ownership to another organization. If the business term is redundant, you can delete it using the Delete Business term button.",
    "url": "https://docs.data.world/en/107770-business-glossary-details-page.html"
  },
  {
    "title": "Creating business glossary terms from the UI",
    "content": "Note that only users with the\u00a0Edit\u00a0and\u00a0Manage\u00a0access to\u00a0Catalog resources\u00a0or to specific collections can add and edit business glossary terms. For details about permissions, see Planning user groups and their access levels.\nTo add business terms from the UI:\nOn the Organization profile page, go to the Glossary tab. Click the New button.\nYou can also create the terms from a specific Collection details page. Browse to the specific collection and click the New Resource button on the Collection details page. Select Business term.\nIf you are allowed to create resources in specific collections only, this is the only place from where you can create the terms.\nIn the Add new resource window, set the following:\nTitle: Provide a name for the business glossary.\nOwner: Select the owner. By default the organization where you are creating the business glossary is selected. If you are creating the business term from the Collection Details page, the owner is set automatically and the dropdown is disabled.\nType: Select from Business term or any of the sub types created for the organization.\nChoose one or more collections: From the dropdown list, select the collections for the business glossary. At least one collection should be selected to be able to save the business glossary. If you have not already created a collection, you will not see the option to choose a collection. When you save the business term, the application automatically creates a collection with the name organization_name-collection and adds the term to it. If you are creating the business term from the Collection Details page, that collection will be selected automatically and the dropdown will be disabled.\nClick Continue.\nIn the Edit resource window, set the following:\nDescription: Provide a brief description for the term.\nSummary: Provide the definition of the term that you are adding. You can use Markdown to create rich content with links, images, etc.\nTags: Type in the tags field to select from the available tags or add new ones.\nCustom fields: Based on how your catalog is configured for your business needs, you might see some custom fields spread across various tabs. Set the fields that you wish to configure and click the Save changes button.\nThe window is closed and the user is taken to the glossary details page.\nOn the Overview tab, click the Edit button in the various sections to make changes to the properties of the term. If users do not have edit permission, they instead see a Suggest changes button.\nOn the Related resources tab, click the Add related resources button to associate the terms with the resources in the catalog. For more details, see the Associating business glossary with resources section in Using business glossary.\nOn the Discussion tab, participate in ongoing discussions for the term or start a new discussion. For more details about participating in discussions, see the Collaborating on business glossary section in Using business glossary.\nOn the Settings tab, you can delete the term or publish it to another organization that you have access to.",
    "url": "https://docs.data.world/en/107549-creating-business-glossary-terms-from-the-ui.html"
  },
  {
    "title": "Bulk uploading and editing business glossary",
    "content": "The application provides you with an option to bulk upload glossary terms in the organization using spreadsheets. You can also use the same feature to download and edit your existing glossary using the bulk edit capability.\nThe bulk download and upload feature should be used only to manage the business glossary within an organization. This feature is not intended for downloading the glossary from one organization and uploading it to another organization.\nIf this is the first time you are adding a glossary to an organization, download the template from the application, follow the instructions in the spreadsheet and upload your glossary to the application.\nIf you are working in an organization that already has a glossary, download the existing glossary as a spreadsheet and make edits to it or add new terms. Once you are done, upload the updated glossary to the application.\nThe spreadsheet has clear guidelines on how to make edits and set field values. You must follow those instructions carefully to have a smooth experience.\nAll new imported terms are added to a collection named Organization_Name-collection. For example, if your organization name is Initech, the terms are added to the collection named Initech-collection. After the upload is complete, you can browse to this collection and move the terms to the appropriate collections in the organization or use the Quick Edit feature to move the terms to different collections. Note that the next time you download and upload the business glossary for the organization, the collections of the existing terms are retained.\nYou cannot use the bulk feature to delete business terms or move terms from one collection to another.\nNote that only users with the\u00a0Edit\u00a0and\u00a0Manage\u00a0access to\u00a0Catalog resources\u00a0can bulk upload business glossary terms. For details about permissions, see Planning user groups and their access levels.\nIn this section we walk you through the basic structure of the spreadsheet and also list the various things to avoid so that you do not run into issues while uploading the updated file in the application.\nKey things to note are:\nAll authors must work on the same downloaded spreadsheet and upload one consolidated version once all the changes are done. Also, while authors are working on the downloaded spreadsheet, you should avoid making changes to the glossary from the UI.\nMake sure you always download the latest glossary spreadsheet from the UI so you can edit the most recent version of the glossary items.\nYou can edit the file in\u00a0Microsoft Excel\u00a0or in\u00a0Numbers. However, the application allows only uploading files in\u00a0.xlsx\u00a0format. If you edit the file with\u00a0Numbers, you must export it to Excel (File > Export to > Excel) before uploading it to the application.\nThe downloaded spreadsheet file has a set structure and has all the sheets you need to add and edit the business glossary. Read the instructions available in the Overview tab carefully before you start editing the file.\nRead the instructions on other sheets as well to follow guidelines for setting values for fields.\nIn the sheetname there is a circle that indicates if it\u2019s empty or not (empty circle = empty sheet). Sheets are sorted in alphabetical order.\nDo not delete rows or columns. Do not add, delete, or move the header rows.\nYou should only\u00a0Edit cell values\u00a0and\u00a0Add new rows.\nMake sure that all the fields that are marked as required have a value set in them.\nDo not move terms from one sheet to another by using cut and paste.\nWhile adding dates to field values, follow the format: YYYY-MM-DD .\nPossible error scenarios while uploading the excel sheet\nWhen you upload a file that does not follow the guidelines, you will be notified with Error messages and Warning messages.\nAn error state indicates that the file cannot be uploaded without fixing the issue.\nA warning state means, two things can happen: if the\u00a0required columns\u00a0are not filled out correctly, the specific term with the warning is skipped. If the\u00a0non-required columns\u00a0are not filled out correctly, that\u00a0particular edit to the term\u00a0is skipped.\nThe following table explains the possible error scenarios.\nScenario\nType\nWill import succeed?\nMaximum file size of 500 MB exceeded.\nError\nNO\nIf a file has more than 20,000 edits, we recommend splitting up the uploads. Contact data.world Support for assistance.\nError\nNO\nAll or some of the required columns [Title] are missing from the uploaded file. (for edit and add)\nError\nNO\nValues are missing for required columns [Title] (for edit and add).\nWarning\nThe entire term will be skipped. Rest of the edits in the spreadsheet are imported.\nUser sets a value of a field that is not valid. For example, if the possible values for a field are Active and Inactive and user sets the value as Not in use.  This applies to fields that have pre-defined values available, for example, multiselect fields, fields with values available as drop down options.\nWarning\nThe particular edit to the term\u00a0will be skipped. Rest of the edits in the spreadsheet are imported.\nDate value is not set in correct format. Dates must be in the YYYY/MM/DD format.\nWarning\nThe particular edit to the term\u00a0will be skipped. Rest of the edits in the spreadsheet are imported.\nIf the metadata profile changes between export and import and a new field gets added or removed from the metadata profile.\nWarning\nThe column is skipped. Rest of the edits in the spreadsheet are imported.\nMake sure you always download the latest glossary spreadsheet from the UI for edits.\nTo download a template in an organization that has no terms added yet:\nOn the Organization profile page, go to the Glossary tab.\nClick the Upload multiple business terms button on the page. It opens the Edit Business terms page.\nOn the Edit Business terms page, click the Download template button. It downloads a spreadsheet with a template for creating your business glossary.\nAt this point you can also continue to upload an updated spreadsheet into the application by clicking the Continue to upload button.\nTo download an existing glossary:\nOn the Organization profile page, go to the Glossary tab.\nFrom the Three dot menu select the Download all resources option.\nOn the Edit Business terms page, click the Download Business terms button. It downloads a spreadsheet with all the business terms in your organization.\nAt this point you can also continue to upload an updated spreadsheet into the application by clicking the Continue to upload button.\nMake sure you always download the latest glossary spreadsheet from the UI for edits.\nTo edit the business glossary in Microsoft Excel:\nOpen the downloaded file in Microsoft Excel. Make edits to the different sheets.\nFollow the instructions on the Overview tab carefully in the downloaded file to avoid unintended errors.\nTo edit the business glossary using Numbers:\nOpen the downloaded file in Numbers.\nFollow all the guidelines on the Overview tab in the file and make edits to the content of the file.\nOnce you are done, export the file to Excel by using the File > Export to > Excel option. The export setting must be set to One per sheet. Do not set the value to One per table.\nTo upload the updated business glossary:\nOnce you are done making edits to the file, you can quickly upload to the application.\nOn the Organization profile page, go to the Glossary tab.\nFrom the Three dot menu select the Edit via document upload option.\nOn the Edit Business terms screen click the Upload from computer button or drag and drop the updated spreadsheet. The file must be in .xlsx format.\nThe application does a check on the file you are uploading. If any errors are found, you are prompted with a list of issues you need to fix before you can upload the file. You cannot continue with the upload if there are errors in the file. If there are warnings, continuing will mean that the changes with warning are skipped and rest of the changes are saved in the application.\nIf there are no errors, you are shown a list of changes to review. The preview window has a tab for each resource type of business glossary and the changes in these tabs are further separated into two tabs - Edit, New.\nClick the Import resources button to continue with the import. You are notified when the import is successful or if there are any errors.\nAll new imported terms are added to a collection named Organization_Name-collection. For example, if your organization name is Initech, the terms are added to the collection named Initech-collection. If you want, you can now organize your terms in different collections. Browse to this collection and move the terms to the appropriate collections in the organization or use the Quick Edit feature to move the terms to different collections. Note that the next time you download and upload the business glossary for the organization, the collections of the terms are retained.",
    "url": "https://docs.data.world/en/107548-bulk-uploading-and-editing-business-glossary-107548.html"
  },
  {
    "title": "Quick editing business glossary from UI",
    "content": "Use this option when you have to apply same changes to multiple terms in the organization. For example, if you want to set the same status, tags, or same steward for multiple terms, use the wizard available to make these quick edits. Also, this comes in handy when you have to quickly move a bunch of terms from one collection to another.\nNote that only users with the\u00a0Edit\u00a0and\u00a0Manage\u00a0access to\u00a0Catalog resources\u00a0can add and edit business glossary terms. For details about permissions, see Planning user groups and their access levels.\nTo quick edit business glossary:\nOn the Organization profile page, go the the Glossary tab.\nClick the\u00a0Three dot\u00a0menu and select Quick Edit.\nIn the Quick edit wizard that opens, first select the type of business glossary item you want to edit. For example, let us select Business concept. Click Continue.\nNext, on the Edit multiple Business concepts screen on the Select tab, select the items you want to edit. Click the Edit business concepts button.\nOnly 250 terms can be selected at a time for editing.\nOn the Edit tab, make the required changes. The options available for editing will vary based on the metadata profile configured for the selected business glossary. Also, you will notice that some fields, like Tags, have options like Add to existing, Replace existing, or Clear all\u00a0values. which makes it easy to make edits to these field. You can use this Edit screen to also quickly move around the terms between collections. Once you are done with your changes, click the Review changes button.\nOn the Review tab, review your changes and click the Confirm changes button.\n",
    "url": "https://docs.data.world/en/107550-quick-editing-business-glossary-from-ui.html"
  },
  {
    "title": "Using business glossary",
    "content": "After you have established the business glossary for your organization, you can open it up to users to view it, search it, link the business glossary to different resources such as datasets, projects, tables, columns, etc. to add business context.\nUsers can also start discussions on business glossary and suggest edits to the terms created in the organization. This helps you crowd source inputs from your users and further improve the quality of the business glossary for the organization.\nOnce you have created your business glossary terms you need to associate the terms with the resources in the application so that users of the catalog can find the terms in the right context, attached to the right resource, to be able to use the term definitions appropriately while using the catalog resources.\nTo add business terms as related resources:\nOn the Business term page, click the Add a related resource button and from the dropdown menu select one of the available resources.\nThe Edit a related resource window opens the tab for the resource you selected from the dropdown menu. You can go to the different tabs to associate the business term to the different resources. Once done, click Save changes.\nOn the Related tab, navigate to the different sections to see the list of specific resources. Sort the lists by using the available sorting options. Use the\u00a0Expand\u00a0feature to view the resources in full screen mode.\nAll the fields show hover text that include complete details about the field when the text cannot be displayed in the column view. For fields like Created and Updated hover over the field values to see the actual date and time information.\nWhen you are in full screen mode, you can navigate to the different related resource from that view. Click the Collapse button to back to the normal view.\nTo access the business glossary as top concept:\nBusiness glossary terms get surfaced as top concepts on search pages. This concept card highlights the best matching glossary term. It presents at-a-glance the custom metadata configured by the catalog stewards including descriptions, technical metadata, related people, and resources. For details, see Understanding search results.\nAs a user of the business glossary, you will not always have edit access to the glossary terms. However, you can use the discussion feature to interact with the stewards of the glossary to ask them questions, point errors, or make suggestions. If you have specific content change suggestions, you can always use the suggest changes feature to do that.\nThe Discussion tab captures all the communication between team members about the term. Either join an ongoing discussion or start a new one to collaborate with your team members on the project. You can also use this space to provide feedback.\nTo collaborate on a business glossary term:\nGo to the Discussions tab and start a new discussion or participate in an ongoing one. You get an option to Flag or Like comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nWhile adding comments, you can use Markdown and add images and links. You can mention users (using @user_name) to notify them about an ongoing discussion to get their attention and participation. Some important things to note:\nWhen you do this for private resources only available in an organization, typing @ suggests only the members of that organization. It also shows a prompt to contact an administrator of the organization to ask them to allow others to have access to the organization. If you are an administrator, you get the option to invite other people to the organization.\nFor community resources, you get a list of suggested users from the community.\nIn both cases, only the users who have at least read access to the resource are notified. So make sure that when you are setting access for the users, you also give them access to the resource where they need to participate in the discussion.\nAs a user of the business glossary, you will not always have edit access to the glossary terms. But, if you find errors in terms or want to enhance the content, you can either start a discussion on the term, or better, suggest specific changes to the terms using the suggestions feature.\nTo suggest changes to business glossary:\nBrowse to the term you want to suggest edits for.\nIn the appropriate section, click the Suggest changes link.\nIn the Suggest changes window, make the suggestions by editing the fields. Click the Review changes button.\nOn the next screen, see the summary of your suggestions and click the Save changes button.\nThe users who can review the changes are notified through emails and in-app notifications about your suggestions.\nA new Suggestions tab is added to the glossary term page where you can view all your suggestions. If you want, you can also remove your suggestion from here.",
    "url": "https://docs.data.world/en/107551-using-business-glossary.html"
  },
  {
    "title": "Managing suggestions for business glossary",
    "content": "As users start using the terms in the business glossary, they might have suggestions to improve the content of the term, suggest better tagging, or other such things. Users will use the Suggest edit feature available for each term to submit their suggestions.\nTo manage suggestions:\nWhen a suggestion is made to a glossary term, the users who can manage the business glossary are sent an in-app notification and email notification.\nWhen you access the term on which suggestions are made, you see an Approvals tab.\nFrom the Approvals tab, review the suggested changes and accept or reject the changes. Once you accept the change, it is reflected to all the users using the business glossary.",
    "url": "https://docs.data.world/en/107755-managing-suggestions-for-business-glossary.html"
  },
  {
    "title": "Managing governance automations",
    "content": "",
    "url": "https://docs.data.world/en/159712-managing-governance-automations.html"
  },
  {
    "title": "About the automations",
    "content": "Data governance teams govern large volumes of diverse data with hundreds, sometimes thousands of stakeholders. Much of that work is manual and time consuming, such as metadata enrichment and managing data access requests. The Data Governance application enables governance teams to more effectively manage tasks and engage stakeholders.\nThe data.world Data Governance application enables automations and automation-driven workflows. As part of the data.world Data Governance application, automations called Eureka Bots reduce the amount of manual effort needed for data governance tasks, helping to increase productivity, engagement, and trust.\ndata.world Data Governance Powered By Eureka Bots combines automations with data.world\u2019s Knowledge Graph architecture to create an application that is simple, flexible, and interactive. This new application \u2013 part of data.world\u2019s Data Catalog Platform \u2013 enables data governance teams to be more productive, customize workflow, and engage stakeholders in data governance\u00a0processes.\nFor more insights from our product experts, watch How Automation Unlocks Productive, Engaging Data\u00a0Governance.\nEach organization in data.world comes with a set of automations that you can use for metadata enrichment, metadata completeness, metadata freshness, ownership assignment, and query based actions.\nInherited Value\nAssign Default Value\nMetadata Freshness Refresh\nMetadata Freshness Review\nSensitive Data Discovery\nMetadata Completeness\nMetadata Query-based Actions (for advanced users only)\nCustomers can purchase the following additional automations to set up comprehensive workflows for access requests for resources.\nApproval: Access Request\nThe Data Governance application comes in two tiers.\nData Governance Core tier is included with the Standard version of the data.world Platform and includes single-step automations.\nData Governance Premium tier is an add-on to the Platform that augments the Core automations with multi-step workflows and task management capabilities.\nAutomations and automation-driven workflows are both available in the Platform\u2019s Automation gallery. Tasks can be claimed and managed from a dedicated UI. The application also includes built-in reporting to enable data governance teams to manage progress and prioritize efforts.\nConfigure automations and automation-driven workflows using the automation templates.\nOnly Organization administrators can view, configure, and run automations.\nClaim and manage data governance tasks in one place.\ndata.world University!\nCheck out our Data Governance Application course.",
    "url": "https://docs.data.world/en/159920-about-the-automations.html"
  },
  {
    "title": "Approval - Access request Automation",
    "content": "",
    "url": "https://docs.data.world/en/159925-approval---access-request-automation.html"
  },
  {
    "title": "About the Approve - access request automation",
    "content": "This automation is available only for customers that have purchased the Data Governance Premium tier.\nUse this automation to set workflows for catalog users to request access to selected resources. Define the types of resources members can request access to and assign specific user groups in the catalog to process these access requests. Once the requests are approved in the catalog, the information can be optionally sent to a third-party ticketing system where where governance teams and others (for example IT) can then give access to the requested resources.\nResource types: Identify the Resource Types for which you are setting up the automation, For example, Power BI Dashboards, Tableau Dashboard, etc. There can be only one active access request automation per resource.\nThis automation is not available for datasets and projects.\nLevels of approvers: Decide the levels of approval you want and identify the user group that will approve the requests at each level. You can set a maximum of 5 approval levels. Atleast one user at each level has to approve the request before the users at the next level can process the request. The request is not sent to the next levels if it is rejected by the users at the preceding level.\nDecide third-party system: Decide if you want the approved requests to go to a third-party system (such as, ServiceNow) for final action, which is granting access to the requested resources. If you do not integrate with a third-party system, you will have to setup an alternative mechanism to inform authorized users to grant the access to the requested resources.\nA user configured connection to ServiceNow. (optional)\nA user configured automation for the organization.\nA user configured Request access button (name configurable) on resources pages for requesting access.\nSystem provided Tasks page for approvers to process the requests.\nSystem provided three collections for organizing all access requests.\nNew Access Requests\nPending Access Requests\nCompleted Access Requests\nTasks for automation admins\nSet up the ServiceNow connection from Connection Manager. This is required only if you want to send the approved requests to ServiceNow for final action, that is, granting access to the actual resource.\nGet the list of resources for which you want to apply the automation.\nGet the list of groups that will approve the request.\nCreate the automation using the information from step 1-3.\nEnable the automation.\nTasks for users using the catalog\nFrom the resource page, click the Request access button (name is configurable) added for requesting access to resources.\nCreate your request. If you wish, you can add more resources to the same request. Once ready, submit the request for approval.\nYou can check the following three collections to see the status of your request:\nNew Access Requests\nPending Access Requests\nCompleted Access Requests\nOnce your request is approved or rejected, you are notified through an email notification. In the event of approval, you might get additional instructions through a separate email from a third-party system on how you will access the requested resource.\nTasks for approvers of access requests\nYou are notified about new approval requests through in-app and email notifications.\nClick the request to come to the Tasks list in the data.world application.\nTo claim the task, either click the Claim button on Tasks page or browse to the specific task to see its details and then claim the task.\nAfter reviewing the task, either accept or reject it with a reason.\nAs tasks keep going through the various levels, the approvers of the next stage are notified. On final approval, the submitter of the task is notified about the final decision.\nYou can check the following three collections to see the status of the various request that you are authorized to work on:\nNew Access Requests\nPending Access Requests\nCompleted Access Requests",
    "url": "https://docs.data.world/en/159921-about-the-approve---access-request-automation.html"
  },
  {
    "title": "Setting up ServiceNow Connection",
    "content": "If you plan to use ServiceNow for processing the access requests once they are approved, set up the connection to your ServiceNow instance. This configuration should be done before setting up the automation.\nTo set up the connection:\nOn the Organization profile page, go to the Settings tab.\nIn the Connection manager section, click the Add connection button.\nIn the Add an organization-level connection window, select ServiceNow.\nIn the Add a new ServiceNow connection window, set the following:\nDisplay name: Provide a name for the connection.\nServer: Provide the Service now URL. For example, 8Bank.service-now.com\nConnection username: Provide the username for the connection.\nConnection password: Provide the password for the username you are using to connect to your ServiceNow instance.\nClient ID: Provide the client ID for your account.\nClient Secret: Provide the client secret.\nOnce done, click the Configure button.",
    "url": "https://docs.data.world/en/159788-setting-up-servicenow-connection.html"
  },
  {
    "title": "Creating access approval request automation",
    "content": "Only Organization administrators can view, configure, and run automations.\nTo create the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Approval: Access Request template. Click the Configure button.\nIn the Configure Approval: Access Request window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Approval: Access Request window, on the next screen set the following and click Next.\nResource Types: Select the types of resources the automation applies to.\nThis automation is not available for datasets and projects.\nAutomation action label: Provide the name of the button.\nIn the Configure Approval: Access Request window, on the next screen set the following and click Next.\nApproval group: Specify the group of users that can approve the request. You can specify 5 levels of approvers and each level can have one designated user group. Any user from that group can approve the request.\nIn the Configure Approval: Access Request window, on the last screen set the third-party integration as Service Now and click Save. This is where the requests are sent after final approval so that the request can be processed by the person who is authorized to grant access. If you do not wish to integrate with ServiceNow, don't select anything.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/159713-creating-access-approval-request-automation.html"
  },
  {
    "title": "Requesting access",
    "content": "To request access to resources:\nGo the resource for which the Access Request workflow is configured.\nOn the resource page click the button available for requesting access. The button name is configured by the administrator and can vary.\nIn the Add new Access Request window, provide a title for the request and click Continue.\nOn the next screen, on the Request Details tab, provide the name of the person submitting the request and click Save.\nNext, the access request page with the details of the request opens. If you want to use the same access request for more resources, go to the Related tab and more resources to the same request.\nOnce you are ready to submit the request, click the Submit Request button.\nAfter clicking the\u00a0Submit Request\u00a0button, you will no longer be able to make changes to the Access Request.\n",
    "url": "https://docs.data.world/en/159714-requesting-access.html"
  },
  {
    "title": "Approving access requests",
    "content": "Once a access request is submitted, the users who are authorized to approve the request are notified about it via an email and in-app notification.\nTo review and approve an access request:\nFrom the email in your inbox, click the View task button to navigate to the access request.\nIf you are already in the data.world application, click the Tasks icon on the Left sidebar to go to the list of access requests. You will see a list of all tasks for different access requests in the system. Locate a task that is not claimed by anyone. Click the Claim button.\nAlternatively, click the link of the request in the Task List to view the task details before claiming it. From the Related tab, you can view the list of resources for which the request is submitted. Click the Claim task button to start processing the request.\nOnce you are ready to approve or reject the request, click the Complete task button.\nIn the Complete access request task window, click the Approve or Reject button and add the optional Description. Click Submit.\nOnce you click the Submit button, notifications are sent out to the requester and other data approvers, if there are multiple levels of approvals.",
    "url": "https://docs.data.world/en/159716-approving-access-requests.html"
  },
  {
    "title": "Metadata completeness Automation",
    "content": "Use this automation to check the metadata completeness of resources in a collection. This gives you automated insights into the completeness of metadata across your resources, enabling better data tracking and quality control.\nDefine the parameters of the selected resources that the automation will check for completeness. At the end of the run the automation will generate a report providing a list of all the resources that don't meet the defined completeness standards.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Metadata Completeness template. Click the Configure button.\nIn the Configure Metadata Completeness window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Metadata Completeness window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check.\nMetadata Fields: Select the metadata fields to be checked for completeness.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the automation details page.\nClick the Run button. You also get an option to set a schedule for running the automation.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/159922-metadata-completeness-automation.html"
  },
  {
    "title": "Assign Default Value Automation",
    "content": "Use this automation to assign default values to resource fields that do not have any value set. This automation ensures consistent and accurate data entry and helps scale your catalog curation.\nDefine the field of the selected resources the automation will check for values. If the field has no value set for it, the value defined in the automation will be set for it. At the end of the run the automation will generate a report informing users about the outcomes.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Assign Default Value template. Click the Configure button.\nIn the Configure Assign Default Value window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Assign Default Value window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check.\nMetadata Field: Select the metadata field for which you want to set the default value.\nDefault value: Provide the default value to be assigned to the field.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the automation details page.\nClick the Run button.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/159923-assign-default-value-automation.html"
  },
  {
    "title": "Inherited Value Automation",
    "content": "Use this automation to assign an inherited value to resource fields that do not have any value set. This allows you to easily inherit metadata values from related resources, saving time and reducing redundancy.\nDefine the fields of the selected resources the automation will check for values. If the field has no value set for it, the field will inherit the value of the field set for the parent resource. This works if the Is Part of relation is set between the two resources and both resources have the exact same metadata field defined for them.\nFor example, you can set the automation to check if Columns have the Owner email set. If they don't, they will inherit the value of the Owner email field set for the Table it is part of.\nAt the end of the run the automation will generate a report informing users about the outcomes.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Inherited Value template. Click the Configure button.\nIn the Configure Inherited Value window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Inherited Value window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check.\nMetadata Field: Select the metadata field from which the value should be inherited.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the automation details page.\nClick the Run button. You also get an option to set a schedule for running the automation.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/160044-inherited-value-automation.html"
  },
  {
    "title": "Metadata Freshness Refresh Automation",
    "content": "Use this automation to assign a freshness value to specific resource types in a collection. This automation is designed to update the Refreshed On date property for each resource in a specified collection. The property is set to the current time whenever the automation is triggered. The automation automatically updates the Refreshed On property for each resource, removing any previous values and ensuring a fresh start.\nAt the end of the run the automation will generate a report informing users about the outcomes.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Metadata Freshness Refresh template. Click the Configure button.\nIn the Configure Metadata Freshness Refresh window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Metadata Freshness Refresh window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check. Only resources of these types will be given a freshness value.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the Automation details page.\nClick the Run button.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/160132-metadata-freshness-refresh-automation.html"
  },
  {
    "title": "Metadata Freshness Review Automation",
    "content": "You must run the Metadata Freshness Refresh automation before running this automation.\nUse this automation to review the metadata freshness of resources in collections. This automation empowers Data Stewards to set up regular cadences for metadata review. It takes the guesswork out of the process and ensures data remains current.\nDefine the fields of the selected resources the automation should check for freshness and the shelf life of the field is days. If the field value has not changed in the defined days, it will mark the resource as Not Fresh. At the end of the run the automation will generate a report informing users about the outcomes.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Metadata Freshness Review template. Click the Configure button.\nIn the Configure Metadata Freshness Review window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Metadata Freshness Review window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check.\nMetadata Fields: Select the metadata field Refreshed On that represents the last time the resource was refreshed. This field is created by the Metadata Freshness Refresh automation.\nShelf Life (Days): Set the number of days that a resource should be considered fresh.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the Automation details page.\nClick the Run button.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/160043-metadata-freshness-review-automation.html"
  },
  {
    "title": "Metadata Query-based Actions Automation",
    "content": "Use this automation to write and run advanced SPARQL queries.\nThis automation is only for advanced users who have been trained on using SPARQL queries for automation. You must submit a request with your Customer Success Director to get this automation enabled for your organizations.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Query-based Actions template. Click the Configure button.\nIn the Configure Query-based Actions window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Query-based Actions window, on the next screen add the SPARQL Query and click Next.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the Automation details page.\nClick the Run button. You also get an option to set a schedule for running the automation.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/160079-metadata-query-based-actions-automation.html"
  },
  {
    "title": "Sensitive Data Discovery Automation",
    "content": "Sensitive Data Discovery automation scans through your resources, identifies the resources which have sensitive data classifications identified via the Sensitive Data Discovery tool, and enumerates resources in a report. It simplifies the process of locating confidential information and managing its security.\nActivate this automation when you want an efficient way to manage the classifications of sensitive data obtained from your sensitive data discovery tool.\nIf you don't have Sensitive Data Discovery this automation is not applicable to you and you should not use it.\nOnly Organization administrators can view, configure, and run automations.\nTo configure the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation templates section, locate the Sensitive Data Discovery template. Click the Configure button.\nIn the Configure Sensitive Data Discovery window, on the first screen set the following and click Next.\nAutomation name: A name for the automation.\nAutomation description: A brief description.\nIn the Configure Sensitive Data Discovery window, on the next screen set the following and click Next.\nCollection: Select the collection for the automation.\nResource types: Select the resource types the automation should check. Only resources of these types will be monitored for sensitive data.\nOn the last screen, select the Trigger Daily option if you want schedule the automation to run daily. Click Save.\nOnce you have setup the automation, enable it so that the access requests can start going through the process.\nTo enable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to enable and open it by clicking it.\nClick the Enable automation link. This option is available only for automations that have never been enabled. If you enable an automation and then disable it, it cannot be enabled again.\nTo run the automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to run. Open the Automation details page.\nClick the Run button.\nAfter the automation is done running, a report is generated and made available in the Project created for the automation.\nTo view the automation run report:\nOn the Organization profile page, go to the Resources tab.\nFrom the Resources tab of the Organization profile page, locate the Project that contains the report for the automation. View the details captured by the automation. Each time the automation runs, it adds a new report to the same project.\nOnce you disable an automation it cannot be enabled again. Any requests that are in progress will not be available for any further action.\nTo disable an automation:\nOn the Organization profile page, go to the Automations tab.\nIn the Automation section, locate the automation you want to disable and open it.\nClick the Disable automation link.",
    "url": "https://docs.data.world/en/160133-sensitive-data-discovery-automation.html"
  },
  {
    "title": "Find resources with search",
    "content": "",
    "url": "https://docs.data.world/en/98794-find-resources-with-search.html"
  },
  {
    "title": "How to find data resources",
    "content": "The product comes with a powerful search engine that you can use to quickly and easily find the resources you are looking for.\nUse the search bar and start with a basic text search or use the Advanced Search Builder to run an advanced search.\nAlternatively, type in a complex search query to quickly get to what you are looking for. You can find some useful search queries here.\nWhen you get to the Search Results page, use the filters available on the search page to narrow down what you see. Each refinement of the results set tells you how many resources meet the combined criteria, and it is easy to drill down through the myriad results to find just the resource you are looking for. The Search Results page also offers a Advanced Search Builder for creating Advanced searches.",
    "url": "https://docs.data.world/en/98795-how-to-find-data-resources.html"
  },
  {
    "title": "Using the search bar",
    "content": "The Search bar is where you can go to find a variety of resources across the application. When you click in the Search bar, you see a list of last eight resources accessed by you.\nAs you type in the field, a list of auto-suggested results is presented and an option is given to go to the list of search results. You can click one of the options you see or just press Enter to get to the list of search results. Click the Create advanced search for Search term option in the dropdown to launch the window for creating an advanced search.\nYou can also use the search bar to search for multiple search terms. When you enter more than one text string into the search bar you will get results that contain all the strings in them. You can search for an exact match on a text string by enclosing the string in double quotes. For example, \"sales analysis\".\nYou can combine operators to broaden your search results. For example, if you wanted everything that had either sales or analysis in it you could simply search for sales OR analysis\nFinally, you can use powerful search queries such as owner:@us-usda-gov to look for everything owned by a specific organization or user.",
    "url": "https://docs.data.world/en/98796-using-the-search-bar.html"
  },
  {
    "title": "Running a basic text search",
    "content": "A basic search is a simple text search without the use of any operators, such as sales.\nIn a basic text search the search engine looks for a match to the search terms in the following places:\nTitle, description, and summary of all resources such as, datasets, projects, queries, business terms, file names, tables, columns, etc.\nInsights\nUser and organization names\nCustom metadata\nSearch supports partial title searches allowing users to search for resources by entering just a portion of the title (3+ characters), making it easier to find the right data. Searches also support camel case,\u00a0making it easier to find resources that have complex names that combine\u00a0uppercase and lowercase letters (for example, accountID).\nIf you have a search string with more than one word in it, you can structure your search so that the search engine looks for an exact match of the string or looks for individual words and searches for items that have all of the words in any order.\nTo search for the entire string, put double quotes around that string (\" \"). The search engine then looks for instances of that exact string and only returns items which contain it. Spaces, hyphens or underscores in search strings are not tokenized if there are double-quotes around the string.\nFor example, \"Medicare hospital spending\" returns only data resources with the exact phrase Medicare hospital spending in them.\nSearches with multiple words in them that are not bounded by double quotes return items that have all of the words in the search string.\nFor example, Medicare hospital spending returns results that include Medicare, hospital, and spending, but they don't have to be in that order.\nSearches for words with characters like : (colon) will return matching results. For example, if you have resources with names like company:product:marketing and company:product:sales, search for company:product:marketing to find resources that have this exact term. Search for company:product will return both the resources - company:product:marketing and company:product:sales, and search for company:produ will not return any results.",
    "url": "https://docs.data.world/en/98797-basic-text-search.html"
  },
  {
    "title": "Creating advanced searches",
    "content": "To create an advanced search:\nClick the Create advanced search button in the Search bar. It opens the Create advanced search window.\nIn the Create advanced search window, set the following:\nOrganizations: From the list of Organizations, select a specific organization you want to search. By default All my organizations is selected.\nInclude results from the community: Select the option to include community results.\nCollection Scope: The drop down in the field enables when you select an organization from the list of Organization. The drop down gives you the list of collections, including the Collection hierarchy that you have access to and you can use it narrow down the scope of your search.\nSearch: Specify a search term.\nFilters: Use the search filters to build complex searches. The filter options change based on many criteria. For example, if you have selected a specific organization for search, the filter options will be limited to the organization. When you are creating advanced searches from the Organization profile page - Resources, Collections, Glossary tabs, you will see only the filter options that are relevant to the specific tab you are on.\nUse the None of the following option to exclude the results you don't want to see. For example, you could use this option to search for all the datasets in your organization that are not in Approved status.\nUse the All of the following options to include results which meet all of the specified criteria. For example, you could use this option to search for resources that are tagged as Agriculture and Atmosphere.\nFor example, if you want to find all resources in your organization with the term sales training that do not have any status and do not belong to specific collections, create the following search query.\nThe search results will look like:",
    "url": "https://docs.data.world/en/98808-creating-advanced-searches.html"
  },
  {
    "title": "Filtering search results",
    "content": "When you run a search you might get just the result you are looking for on the top of the results list. More often you might need to sort through it a bit to get your nugget of gold.\nOne option to narrow down the list of search results that you are seeing is to go the tabs (All, Resources, Organization and people, Comments, Columns) on the Search Results page.\nAlternatively, use the filters available to narrow down what you want to see. These options are dynamic and change as you start filtering your search results. They also change based on which tab of the Search Results page you are on. Some of the options available are:\nInclude community results: This option is available only if you belong to an organization. if your organization allows access to data in the public domain, use this option to look for things in the publicly accessible resources on data.world. If you do not belong to an organization, you always see the community results.\nResource type: Select from the list of resources. In some cases, selecting a resource takes you to a different tab of the Search Results page. This is the best option to start with to narrow down your search results.\nOwner: The organization or user who owns the resource.\nIf you know other characteristics about the resources you are looking for, such as, Tags, Collections, etc. use those filters to narrow the list of results further.\nYou may see other options available for filtering, such as Status, Popularity, Steward, Tech Owner, etc. This can vary based on your organization is configured.",
    "url": "https://docs.data.world/en/98800-filtering-search-results.html"
  },
  {
    "title": "Understanding search results",
    "content": "After you run a search you are presented with a search results page divided into tabs. The All tab shows the top three search results for each Resource type and gives you a high level overview of the types of resources you can find in the application. Hover over the Info icon to see the Information Card that gives brief information about the resource. You can use this information to then decide if you need to navigate to the search result to explore it further.\nOrganizations with the Enterprise License and with a Glossary, see a Top concept  (Eureka Answer\u2122) on top of the search results on the All tab. This concept card highlights the best matching glossary term. It presents at-a-glance the custom metadata configured by the catalog stewards including descriptions, technical metadata, related people, and resources. The term in the concept card could change as you use filters on the results page to narrow down the search results. Click the concept card to go the specific search result. Click on links in the concept cards to go to specific resources. When you click the people in the Related people section, a search is done for the person in context of where the user name is clicked. For example, if you click the user name that is a Steward for the current term a search is done for all resources where the user is the Steward (metadata:\"Steward:sarah smart\").\nIf you want, you can use filters on the results page to narrow down the list of results. Click the See allrecource_type link in each section to go specific tabs to see all the results returned for that resource.\nOnce you get to the tabs, you can see additional information about the resources, such as, when was the last time the resource was updated, how many times it has been used, etc. The additional information on the search results can be useful for determining whether a resource is a good fit for your project. Format, source or provenance, recency, and frequency of citation or popularity are all useful metrics for evaluating data. See evaluating search results for details.",
    "url": "https://docs.data.world/en/98798-understanding-search-results.html"
  },
  {
    "title": "Finding related data",
    "content": "You can also find information about other related datasets from the Overview page of a dataset including:\nOther resources with the same creator (right sidebar)\nOther resources with the same owner (top left)\nResources with the same tags\nRelated projects\nClicking through to any one of them will take you to another set of data that might be relevant to your current project or that you might be interested in looking at later. Finally at the bottom of the overview page there is a set of curated suggestions of related datasets:",
    "url": "https://docs.data.world/en/98799-finding-related-data.html"
  },
  {
    "title": "Evaluating data from search results",
    "content": "If you're looking for search syntax and terminology see our article on advanced search.\nThis article is about qualifying the results you get from a search to see if they are appropriate for use in your project. To find the answers to general search questions, or to learn how to use our search bar, see the article on finding data resources. To learn how to restrict your search results using filters or facets see the article on filtering search results.\nFinding data is only the first step to using it. With all the information available it can be difficult to know what's good data and what's not. Some of the criteria you might use to qualify the data you find as appropriate for your purpose are its:\nCertification (status)\nFormat\nProvenance (source)\nRecency\nFor datasets: Popularity (frequency of citation), and content\nAs was discussed in the article How to find data resources, every resource card shows the format of the resource in the upper left corner of the card. However the dataset and project resource cards also both show the provenance, recency and popularity of the data, and the certification of the data if it exists. For a detailed explanation of the data on a resource card see the article on finding data resources.\nIf you are in an organization that certifies its data by assigning a status you will see information on the resource card indicating the status of the resource. Default statuses are Pending, Approved, Deprecated, Rejected, and Warning and are flagged with an icon on the search result:\nFurther information on the certification is provided in the summary section of the data resource overview:\nCertifications are also configurable.\nUsing just a list of data resource names to determine appropriate resources is not very useful when there are multiple screens of results to sift through. When you are looking for tabular data or datasets, for example, it is handy to be able to both see at a glance the format of the data resource and also to be able to restrict your search so you only see certain types of resources.\nSee the article on filtering search results for more information about restricting your search results. If you do not recognize the format of a data resource by its icon, there is a guide to all of the icon types used by data.world.A guide to icons\nIf a document doesn't have a certification flag, another way of determining its quality is to look at the provenance. If the source is an official agency or person/organization known to be credible, it's a good bet the data is sound. There are two components to provenance in data.world: The creator of the resource and the owner. Both are indicated on the results card:\nFor data from a continually changing source (e.g., a company's sales records in a dataset), knowing when the data was last updated can also help you decide between different representations of the same data. In this example both datasets are from California Health and Human Services, both are concerned with tobacco sold to minors, but the first one was updated 24 days ago and the other was last updated three years ago:\nFor resource types which are datasets there are two additional indicators of the potential value of the resource: Frequency of use and content. Frequency tells you how many projects use that dataset, and content shows you the number of files and tables in the dataset:",
    "url": "https://docs.data.world/en/98801-evaluating-data-from-search-results.html"
  },
  {
    "title": "Creating advanced searches manually",
    "content": "This article is an advanced look at the search operators used in the search bar on data.world. For an introduction to all search capabilities including filtering search results and finding similar data start with the article on finding data.\nThere is a lot of information on data.world, and finding just the data resource that you're looking for can be a daunting task. Fortunately the robust search options available with the data.world search engine enable you to craft just the right search string to find what you're looking for. From the search bar (located at the top of your homepage and on many other data.world pages), you can search for an entire phrase or for matches on single words. Additionally, you can qualify your search with various operators or perform complex searches combining operators.\nTwo ways for running advanced searches:\nUse the Advanced Search tool. This friendly form helps you construct more complex searches with multiple filters, logical operators, categories, and custom metadata fields.\nIf you are an expert user and know the advanced queries, you can type them directly in the Search bar. See the following table to get you started on the advanced search queries. Replace the resource types and search terms to adapt the queries for your needs.\nAll the links in the following examples open search results in the data.world open data community.\nIf you want to\nUse\nFind all projects owned by \"siyeh\".\nowner:siyeh AND resourcetype:projects\nFinds all insights owned by \"siyeh\".\nowner:siyeh AND resourcetype:insight\nFinds all datasets that are tagged \"health\" and have the word 'shelter' somewhere in them.\ntype:dataset AND tag:health AND shelter\nFinds all datasets tagged as \"economics\" and created between the specified dates.\ntype:dataset AND tag:\"economics\" AND created:{2021-01-01 TO 2021-07-03}\nFind all datasets owned by \"democorp\" that have the status \"deprecated\" .\ntype:dataset and owner:democorp AND status:deprecated\nFind all datasets that have a specific tech owner.\nNote: This shows how to use custom metadata to form complex searches.\ntype:dataset AND metadata:\"Tech Owner:brenda griffith\"\nFind all datasets verified by a specific user.\nNote: This shows how to use custom metadata to form complex searches.\ntype:dataset AND metadata:\"verified by:Sarah Smart\"\nFind all columns available in files in\u00a0datasets and projects with the term \"county\" in them.\ncounty AND type:\"dataset column\"\nFind all table columns in the catalog with the term \"sales_agent\" in the column name or description.\ntype:column AND \"sales_agent\"\nFind all business terms with the word \"bill\" in the name or description.\ntype:\"business term\" AND bill\nFind everything verified by a specific user.\nmetadata:\"verified by:Sarah Smart\"\nFind everything that has a value set for a specific metadata.\nhasMetadata:\"Steward\"\nFind everything that does not have a value set for a specific metadata.\nNOT hasMetadata:\"Steward\"",
    "url": "https://docs.data.world/en/98807-advanced-search.html"
  },
  {
    "title": "Logical operators AND, OR, and NOT",
    "content": "The logical operators AND, OR, and NOT can all be used to restrict results returned from a search.\nUsing the operator AND is not necessary while searching for multiple terms because AND is the default operator for searches on data.world.\nThe search string colony collapse returns anything with the words colony and collapse somewhere in the search fields.\nThe OR operator returns results that have either one string or the other. The OR operator can be used multiple times in a search string.\nSample syntax for OR operator:\nAll the links in the following examples open search results in the data.world open data community.\nFor single term searches use:\nsales or order\nsales or order or analysis\nFor multiple terms searches use:\n\"sales analysis\" or \"sales_order\"\n\"sales analysis\" or \"sales_order\" or \"order\"\nUse the NOT operator to eliminate items from your search results.\nAll the links in the following examples open search results in the data.world open data community.\nFor example:\nwildlife NOT refuge: Returns results that match \"wildlife\" and do not contain \"refuge\".\nwildlife NOT refuge NOT \"us-doi-gov\": Returns results that match \"wildlife\" and do not contain \"refuge\" or \"us-doi-gov\".\nNOT, like OR, cannot be used in complex searches (combined with AND or OR) without specifying the grouping of the search terms.",
    "url": "https://docs.data.world/en/98809-logical-operators-and,-or,-and-not.html"
  },
  {
    "title": "Operators and keywords",
    "content": "Keywords can also be used with a set of data.world-specific operators to further qualify your searches. There is a common set of rules that govern the use of these operators and it is consistent across all of them. For all keyword operators:\nThe syntax of an operator search string is operator:keyword where operator is the name of the operator and keyword is the string you want to match.\nThere is no space after the colon (operator:keyword, not operator: keyword).\nIf there are underscores, hyphens, or spaces in the search string you need to use double quotes to match the entire string. (operator:\"key word\")\nCreated and Updated are two operators which can be used to find datasets, projects, insights, users, and organizations based on the date they are added or last updated.\nTimestamps are set in UTC, not your local time, so you might get results that are a day off of your local time depending on where you are.\nAll the links in the following examples open search results in the data.world open data community.\nCreated after date: created:>2022-01-22\nUpdated on or after date: updated:>=2021-07-01\nCreated before date:  created:<2021-07-01\nUpdated on or before date: updated:<=2021-07-01\nCreated between dates (not including): created:{2021-07-01 TO 2021-07-03}\nSearching with the extension operator returns all datasets and projects which include files with the specified extension.\nThe searches are exact-match only and the '.' is optional.\nHere are some sample queries:\nAll the links in the following examples open search results in the data.world open data community.\nSearch for all resources with .jpg files: extension:jpg, extension:\"jpg\", or extension:.jpg\nSearch for all projects with .jpeg files: extension:jpeg and resourcetype:project, extension:.jpeg and resourcetype:project, or extension:\"jpeg\" and resourcetype:project,\nUse the file operator to search for datasets and projects with specific files, or to search for a list of specific files.\nAll the links in the following examples open search results in the data.world open data community.\nSearch for a list of projects and datasets with a specific word in file name: file:damage\nSearch for files with specific word in file name: type:file and damage\nThe owner of a resource is the person or entity who was designated as such when the resource was created. If a person was selected as the owner, that person will also be the creator. If an organization was selected as the owner, the creator will still be the person who created the resource. The owner operator returns all the resources owned by either a person or an organization. The creator operator returns all the resources created by a user. contributor works the same as owner, but instead of matching on resources the user owns, it matches resources where the user has been added as a contributor. They all follow the same patterns as user and org.\nAll the links in the following examples open search results in the data.world open data community.\nTo search for\nUse\nResources owned by any user or organization with 'dave' in either the display name or ID.\nowner:dave\nResources owned by any user or organization with the exact display name or ID 'dave'.\nowner:\"dave\"\nowner:@dave\nowner:\"@dave\"\nResources owned by the user whose display name is Dave Griffith.\nowner:\"dave griffith\"\nResources owned by the organization data-ny-gov (created by individuals in the organization).\nowner:\"data-ny-gov\"\nResources created by the user with the login stateofny\ncreator:@stateofny\nAn exact match of everything contributed to by the user dave.\ncontributor:dave\ncontributor:\"dave\"\nThe resourcetypeoperator allows you to search for the following types of resources:\ndataset\nproject\ninsight\nfile\ntable\nquery\ncatalogTable\ncatalog\nterm\ndatatype\nanalysis\ncatalogEntry\ncollection\nFor example, to search for projects, use: resourcetype:project\nIt is best to use the resourcetype  operator in conjunction with another search string. For example, to search for all projects with the word sales in the name, use: resourcetype:project and sales. For more details, see complex search.\nUse the operator Status to build advanced search queries to filter results on status. For example:\nSearch for all resources in Approved status by using a simple query: status:approved\nSearch for all datasets in Approved status using the following complex query: status:approved AND type:dataset\nSearch for all datasets owned by democorp in Deprecated status using the following complex query: status:deprecated and type:dataset and owner:democorp\nThe table operator is used to find datasets and projects with specific tables in them. It looks for tablular data either in table or as a sheet in a spreadsheet.\nAll the links in the following examples open search results in the data.world open data community.\nTo search for\nUse\nAll datasets and projects with tables with \"income\"in the name.\ntable:income\nTables with the word \"income\" in them. This returns a list of tables.\nresourcetype:table and income\nAll datasets and projects with the exact table name \"austin_animal_center_outcomes\" in them.\ntable:\"austin_animal_center_outcomes\"\nTables with the exact name \"austin_animal_center_outcomes\". This returns a list of tables.\nresourcetype:table and \"austin_animal_center_outcomes\"\nThe tag operator specifically searches against the tags associated with datasets or projects and returns a list of only the datasets and projects that have that tag. Partial and exact matches are allowed.\ntag:property: Any dataset or project with the word 'property' in its tag (for example, property, property tax).\ntag:\"property tax\": Only datasets and projects with the exact tag 'property tax'.\ntag:\"property\" not tag:\"land\": Any datasets and projects that are tagged \"property\" and exclude results that have both the tags \"property\" and \"land\"\nIn addition to alphanumeric characters, tags also support ampersands, apostrophes, and periods.\nThe user and org operators search for a string found in the display name or the ID of a Useror Organizationrespectively. The character \"@\" restricts the search to an exact match of the ID.\n@denver or@dave:To look for exact user or organization.\norg:denver: Any organization with \"denver\" in the display name or the organization ID.\nuser:\"dave\": All users with the exact login name 'dave' or display name 'dave'.\nuser:\"dave griffith\": Only the user whose display name is Dave Griffith.\nThe visibility operator is mainly useful to verify permissions on your data:\nvisibility:private: Returns all private resources owned by you or an organization you are in.\nvisibility:open: Returns all public resources on data.world.",
    "url": "https://docs.data.world/en/98813-operators-and-keywords.html"
  },
  {
    "title": "Tokenization",
    "content": "Hyphen and underscore (- _) characters are tokenized in some searches and are not read by the search engine as hyphens and underscores except in exact match searches:\nanimal_center and animal-center: Return the same results.\nanimal center: Returns a different set of results.\n\"animal-center\", \"animal_center\" and \"animal center\": All search for the exact strings in the quotes and return different results.\ntable:\"bee_colony_census_data_by_county\": This is the format required to get an exact search match.",
    "url": "https://docs.data.world/en/98824-tokenization.html"
  },
  {
    "title": "Complex searches",
    "content": "Combining search operators is a powerful way to restrict search results and really drill down through the data to find what you want. Here are some examples of complex searches created by combining operators:\nAll the links in the following examples open search results in the data.world open data community.\nowner:siyeh AND resourcetype:insight: Finds all insights written by a specific person.\nresourcetype:dataset AND owner:dave: Finds all datasets owned by anyone with 'dave' in their ID or display name.\nextension:jpeg and resourcetype:project: Find all projects which include files with the .jpeg extension.\nGrouping of search terms\nIt is also possible to combine different operators in a complex search, but you need to clearly group the parts of the search string that go with each operator or the search engine will not process your request correctly.\nFor example the search string bee AND pesticide OR colony AND collapse could be parsed in a few of different ways including:\n(bee AND pesticide) OR (colony AND collapse) - all results that either have bee and pesticide or have colony and collapse.\nbee AND (pesticide OR colony) AND collapse - all results that have bee and either pesticide or colony and also have collapse.\nbee AND (pesticide OR (colony AND collapse)) - all results that have bee and either pesticide or both colony and collapse.\nThe search string bee AND pesticide OR colony AND collapse will not return predictable results.\nSearching for exact matches\nSearching for exact matches in complex searches also requires careful construction of the search string to get the desired results. For example if you wanted to search for everything that had to do with either a university degree or a high school diploma the following search strings would give you completely different results:\nuniversity degree OR high school diploma - will not return desired results because of the lack of grouping.\n(university degree) OR (high school diploma) - all results have either the terms university and degree (together or separate in any order or location), or the terms high, school, and diploma (also together or separate in any order or location)\n\"university degree\" OR \"high school diploma\" - all results have either the string \"university degree\" or the string \"high school diploma\" somewhere in them.",
    "url": "https://docs.data.world/en/98825-complex-searches.html"
  },
  {
    "title": "Using Archie Bots",
    "content": "Archie Bots make it easier for users to use data.world to discover data and unlock organizational knowledge \u2013 regardless of expertise level. Archie Bots integrate the power and flexibility of data.world\u2019s knowledge graph-architecture with large language models (LLM).\nThe key capabilities that Archie Bots deliver are:\nDiscover data using AI-assisted search\nAuto-enrich data assets for greater productivity and understanding\nGuided ideation for deeper exploration\nGenerate SQL with natural language to tap into deeper knowledge\nThis is a\u00a0BETA feature\u00a0and you can opt-in to try it out only if:\nYour organization has read and signed the AI Addendum and requested access via data.world support (access is granted at the organization-level).\nYou select the\u00a0Enable experimental features\u00a0option in your account settings.\ndata.world University!\nCheck out our Archie Bots course.",
    "url": "https://docs.data.world/en/160498-using-archie-bots.html"
  },
  {
    "title": "Enriching catalog resources with Archie Bot",
    "content": "Use Archie Bot to enrich your catalog resources by generating automatic descriptions for key resources such as Tables, Columns, Glossary terms, Collections, and the metadata resources like Views, dbt Test, dbt Models, Snowflake Masking Policies, Snowflake Access Policies, etc.\nTo enrich the catalog resource descriptions using Archie Bot:\nBrowse to a resource that allows generating descriptions using Archie bot. For example, collections.\nOn the Overview tab, in the About section, click the Edit button.\nIn the Description field, click the Click to ask Archie Bot for a suggestion button.\nThe system processes the request and fills the Description field with automatically generated content. The content is pre-fixed with [Archie Bot suggestion] to annotate that it is generated by Archie Bot. If you want to generate a different description, click the Click to ask Archie Bot for a suggestion button again. That generates a new description. You can edit the content to tailor it your needs.\nUse the Did Archie Bot suggest a relevant description? to give feedback on the auto-generated content.\nClick the Save button to save your changes.",
    "url": "https://docs.data.world/en/160499-enriching-catalog-resources-with-archie-bot.html"
  },
  {
    "title": "Empowering search with Archie Bot",
    "content": "Use Archie Bot to discover data with a chat-like interface to quickly search, get suggested filters, and refine your results.\nTo use Archie Bot for search:\nGo to the Discover page.\nClick the Turn on Archie Bot link and select any organization for which you have enabled the Archie Bot features.\nIn the Search with Archie Bot box, type what you are looking for using natural language. For example, search using terms like All approved Tableau dashboards. This will search for all Tableau dashboards that have an Approved status.\nOnce the search results are displayed, use the Refine your search box by either using the suggested filters or by typing more text in the search box.\nTo help train Archie Bot, give feedback using the Did Archie Bot interpret your search correctly? option.\nIf you want to turn off the Archie Bot and use the regular search, click the Turn off Archie Bot link.",
    "url": "https://docs.data.world/en/160500-empowering-search-with-archie-bot.html"
  },
  {
    "title": "Generating questions for a collection",
    "content": "Use Archie Bot to suggest research questions and analytic hypotheses for database tables in collections.\nThey help data analysts, data scientists, and other data explorers get a sense of what kinds of questions the data might be able to answer by generating research questions within the context of a collection, using the metadata to start diving into the data to get answers. Not only does this help non-technical users who may not work in SQL every day but when a question is saved as a query to a project, this helps all future users who might have a similar question because they can find the query later via search!\nTo generate a list of useful questions for a collection:\nBrowse to a collection that has tables in it.\nGo to the Contains tab and from the Tables section, click the Three dot menu and select Generate questions.\nThe system processes the tables that are in the collection and then suggests a list of questions that the collection may be able to answer. Select any or all of the questions and save them to a project or dataset where you can generate relevant queries. Click the Add questions button.\nThe dataset or project where you are saving these queries must have the actual database tables stored in them. Only then you will be able to convert these questions into queries and get meaningful insights on the data in the tables.\nYou are taken to the Project or Dataset where you saved the questions. Launch the Workspace and you will see each question saved as a template for the query.\n\n",
    "url": "https://docs.data.world/en/160501-generating-questions-for-a-collection.html"
  },
  {
    "title": "Using Archie Bot with SQL queries",
    "content": "Use Archie Bot to automatically convert a natural language question into a well structured SQL query with a plain English description to aid in human understanding.\nData analysts and business users can jump into the data and do exploratory analysis quicker because they can now use natural language to generate SQL. When these generated queries and summaries are saved, this helps all future users who might have a similar question because saved descriptions and queries are searchable!\nTo generate an explanation of a SQL query:\nBrowse to a project where you want to work with the SQL query.\nBrowse to an already saved query in the project.\nIn the Archie Bot section, click the Summarize SQL button.\nThis automatically generates an explanation of the query in simple English.\nTo generate a SQL query from simple text:\nBrowse to a project where you want to work with the SQL query.\nOpen the resource you want to query.\nIn the query box type in simple English what you want the query to do.\nIn the Archie Bot section, click the Generate SQL button.\nThis automatically generates the query and adds it to the query box. Review the query to make sure it is correct. Alternatively, click the Run Query button to see if it runs properly or there are any errors in it. Fix the errors and run the query again.",
    "url": "https://docs.data.world/en/160502-using-archie-bot-with-sql-queries.html"
  },
  {
    "title": "Store data: datasets",
    "content": "",
    "url": "https://docs.data.world/en/98826-store-data--datasets.html"
  },
  {
    "title": "What is a dataset?",
    "content": "A dataset is the basic repository for data files and associated metadata, documentation, scripts, and any other supporting resources that should be stored alongside the data. Datasets are where all data is stored and documented for later sharing and use in projects. They are the building blocks for projects. They contain data and metadata related to a topic. The files and tabular data in a dataset are meant be used--queried and analyzed--in one or more projects. Datasets are meant to be reusable resources. They can be combined with other datasets in projects, or they can be a single source for querying and analysis in a project.\nDatasets can be owned by an individual or an organization, and a dataset provides an additional layer of access permissions to the data in a project. Because permissions are assigned at both the dataset and the project level an individual can create a project available to the public, but if the individual adds any datasets owned by an organization to the project, they won't be visible to the public--only to the other people in the organization. Not only is the dataset not visible, but any queries in the project written against that dataset are also not visible except to members of the organization.\nBecause datasets are linked to projects, any changes to the data or the metadata in the dataset show up automatically in the linked project. Linking data to a project instead of copying it into the project means that everything is kept up to date throughout your organization.",
    "url": "https://docs.data.world/en/99027-what-is-a-dataset-.html"
  },
  {
    "title": "When to use a dataset and when to use a project",
    "content": "Generally if you are putting up data to share or data that is private but which you might conceivably want to reuse in other projects, it's better to add the data to a dataset. If the data is in a dataset, all of its metadata will automatically show up in your project because the dataset is linked instead of copied. All changes to the original dataset--including automatic updates from the source and manual updates by the dataset owner to the metadata--will also be conveyed.\nThe table below summarizes the differences between adding data files to a dataset vs. to a project:\nDataset vs. Project\ndataset\nproject\nCan run and save queries against\nX\nX\nCan have charts/visualizations\nX\nCan incorporate different file types\nX\nX\nCan contain multiple files\nX\nX\nCan be shared/have contributors\nX\nX\nCan have a discussion thread\nX\nX\nCan include insights\nX\nCan use existing data.world datasets without having to download and reimport them and having to recreate the associated meta-data\nX\nCan be included in a project\nX\nCan be shared for others to use in their own datasets and projects\nX",
    "url": "https://docs.data.world/en/99028-when-to-use-a-dataset-and-when-to-use-a-project.html"
  },
  {
    "title": "Column histogram information",
    "content": "When you are previewing a table on the dataset overview page, if you click Switch to column overview in the lower right corner of the table, you'll see information on the format and values in that column. Selecting one the lines brings up the information for that particular column. When a histogram is displayed you can hover over any point on the histogram for information about that value. By default columns are sorted by frequency but they can also can be sorted by value. Other information about the column captured in the overview includes:\nnumber of distinct values\nnumber of empty and non-empty cells\nminimum, maximum and mean length of values\nthe name of the table the column is in\nthe unique column identifier\nCheck out the video below for a run-through of the column overview:",
    "url": "https://docs.data.world/en/99392-column-histogram-information.html"
  },
  {
    "title": "Create a dataset",
    "content": "When you create a dataset it might be because you have a database or other tabular data that you want to analyze and share. But data from a database isn't the only kind of data you can put in a dataset. Any file type can be saved there. Check out our article on supported file types for Information about various file types and the ways they are handled.\nThere are several ways datasets can be created:\nManually - we'll walk through that here\nVia our API - instructions available in our API docs\nThrough super connectors like Stitch, KNIME, Knots, and Singer - instructions can be found in our integration documentation under super connectors",
    "url": "https://docs.data.world/en/99029-create-a-dataset.html"
  },
  {
    "title": "+New dataset",
    "content": "While logged in to data.world click on +New in the upper right corner of your window to create a new dataset and you'll be prompted to choose either a dataset or a project:\nChoose  Create new dataset and you'll be prompted to name the dataset, and set the ownership and, accessibility. If you are in one or more organizations, by default the owner field will contain the name of one of the organizations you are in. You can also set the owner to be yourself or any of the other organizations you are in by selecting the dropdown on the owner filed:",
    "url": "https://docs.data.world/en/99030--new-dataset.html"
  },
  {
    "title": "Dataset owner and permissions",
    "content": "If the dataset is intended to be used in the organization, it should typically be created with the organization as the owner. In this way the dataset benefits from the organization's service tier, permissions can be easily set based on the members of the organization, and datasets remain available within the organization even as individuals and permissioning changes. Permissions on a dataset owned by an organization can either be set to No one or to everyone in the organization:\nIf you are not in any data.world organizations, you will automatically be set as the owner of the dataset, and you can choose to keep the dataset private or to share it with the data.world community:\nThe number of private datasets you are allowed is determined by your user license--you can create as many public datasets as you would like. More information on account types and pricing are found on our pricing page. There are several factors to consider when deciding whether to make your dataset public or private:\nWhen you make a dataset public you allow others to use that dataset in their own projects and build from it. They can't change your dataset in any way or even save queries to it, but they can use and share it.\nData that is public on data.world can be downloaded from data.world and used externally. If your data is proprietary or sensitive, it shouldn't be shared.\nPublicly shared datasets add to the amount of information that is available to everyone for analyzing, visualizing and learning from\nMore information on permissions can be found in the article Understanding permissions.Understanding permissions\nWhatever the permissions are set at for the dataset will also pass through to any projects that use the dataset. So if the dataset is shared with no one then only you will be able to use it in a project, and if the project in which you include it is open to everyone, no one else will be able to see that dataset. Permissions can always be edited at a later time. After you create your dataset you can document your objective for it, add data to it, or continue on to the overview.",
    "url": "https://docs.data.world/en/99031-dataset-owner-and-permissions.html"
  },
  {
    "title": "Crowdsourced datasets",
    "content": "In data.world datasets and projects can be owned by individuals or organizations. They can be private, shared with an organization, or shared with the public. With the crowdsourcing feature individuals can even set the ownership of a dataset or project to an organization that they don't belong to as long as that organization is configured to accept ownership proposals. In this article we'll cover:\nConfiguring an organization you administer to accept dataset proposals\nSetting ownership of a resource to an organization you don't belong to\nWhat happens after an ownership proposal is made\nOn an organization's page in data.world there is a settings tab where administrators of the organization can set preferences for membership in the organization and whether it accepts datasets proposed by individuals outside the organization:\nIf the organization is configured so that it accepts dataset proposals the projects and datasets can be created for this organization by any community member and will be subject to admin approval. Proposed resources will count towards the organization\u2019s resource limit. Once the organization hits the limit, users will not be able to submit proposals and will see a message saying, \u201cThis organization is not accepting datasets or projects right now.\u201d\nWhen you create a new dataset or project, one of the ownership options you can choose is any organization that is accepting proposals. On the Create a new dataset dialog there is an option to see what organizations are accepting proposals:\nWhen you follow the link you will be able to search for an organization by name or select from a list:\nAfter you select the owner it's a good idea to enter a description and upload or link data so that the organization admin\nWhen you propose a dataset or project to an organization it is only visible to you and the org admin(s) until it has been approved by the admin(s) and shared. If the admin(s) choose not to share it, it will remain visible only to you and them--no one else in the organization. You will also no longer be able to change the access privileges (make it discoverable, or publicly available) or invite anyone else to contribute to it.",
    "url": "https://docs.data.world/en/99032-crowdsourced-datasets.html"
  },
  {
    "title": "How to get your data into data.world",
    "content": "There are several ways to get your data or metadata into data.world and there is no one right choice. There are benefits to each method, and which you choose will depend on several factors:\nWhat format is your data in?\nWhere is your data currently located?\nWhat is the size of your data?\nHow often does it update?\nWhat are you going to do with it?",
    "url": "https://docs.data.world/en/99033-how-to-get-your-data-into-data-world.html"
  },
  {
    "title": "Databases or data warehouses",
    "content": "If you work with a JDBC-compatible database or warehouse, you can use our API to directly sync your data into a dataset. Below is a view of the databases supported by our API, and a current list can be found on our database integrations page.\nIf you wish to leave your data at rest in your existing database or data warehouse, whether it\u2019s on-premises or in the cloud, our Enterprise Tier supports virtualized access to that data source.",
    "url": "https://docs.data.world/en/99034-databases-or-data-warehouses.html"
  },
  {
    "title": "Local files",
    "content": "The best way to get files from your computer onto data.world is to upload them directly into a dataset. Files uploaded from a computer cannot be automatically synced or updated, but you can manually push new versions up to your dataset, replacing the previous version, as needed. When a new version is uploaded, the older version is still available for auditability and versioning. More about uploading data from local files and versioning can be found in our article Adding data files.",
    "url": "https://docs.data.world/en/99035-local-files.html"
  },
  {
    "title": "Cloud-based storage",
    "content": "Documents that are stored in cloud-based storage services (e.g., in Google Drive, Box, Dropbox or Amazon S3) can be easily added to data.world with one of our integrations and set to sync so that they update automatically:\nAs with manual updating, versions of files that are automatically updated are also kept for reference. More information about adding cloud-based files can also be found in the article Adding data files.",
    "url": "https://docs.data.world/en/99036-cloud-based-storage.html"
  },
  {
    "title": "Excel spreadsheets",
    "content": "For Excel spreadsheets, data.world has created a specific add-in that's available on the AppSource or from within Excel. The add-on allows you to work with your data in Excel while at the same time sharing it in a dataset with others who may not have or use Excel:\nSee our Excel integration page for more information. Of course if you so choose you can always either upload your Excel spreadsheet into a dataset like you would any other file type, or you could put in a cloud service like Google Drive, Box or DropBox and add it to the dataset there so it can automatically sync between the two. Versions of Excel files that are uploaded or synced are also kept for future reference.",
    "url": "https://docs.data.world/en/99037-excel-spreadsheets.html"
  },
  {
    "title": "Data from real-time sources via streaming",
    "content": "You might have data that updates in real-time that you would like to put on data.world. This data could be something like log files, test metrics or tracking data. The best way to integrate this data into a dataset is to use data.world's streaming API. Unlike the methods previously mentioned which pull data from the source on a regularly scheduled basis, data brought in through the streaming API can be pushed into a dataset based on a change to the original data. Because it's triggered by data events and not random time intervals, using the streaming API is the best way to manage real-time data. You can read more about streaming in our API Quickstart guide.\nFor those less comfortable with working directly with an API, data.world also integrates with several superconnectors like IFFTTT, KNOTS, Singer or Stitch. While easier to use, they are less flexible and versatile than our own streaming API. You can see a full list of our superconnector integrations on our superconnector integrations page.",
    "url": "https://docs.data.world/en/99038-data-from-real-time-sources-via-streaming.html"
  },
  {
    "title": "Data via a URL or RESTful API",
    "content": "Another common source of data is from a URL or RESTful API available on the internet. If you have a Google Sheets doc, e.g., you can add it to a data.world dataset. As long as the data is on a site that's publicly accessible, you can sync it to data.world--even if it's on a password-protected site with data.world's option to add from a URL. Detailed instructions for adding and syncing data from a url can be found in the article Adding files from a URL. If you do not own the data from the web that you'd like to bring into data.world, you can find out more about licensing and data in the article Licensing and data you found.\nIf you have data that is behind an API that you'd like to put on data.world--e.g., data from Salesforce, Facebook Ads, Google Ads, etc.--the best way to get it into a dataset is to use one of the superconnectors shown above. More information about our sales and marketing app integrations can be found here.",
    "url": "https://docs.data.world/en/99039-data-via-a-url-or-restful-api.html"
  },
  {
    "title": "On-premise data",
    "content": "In addition to data that is available to data.world via cloud sources or APIs, some data that you might want to make accessible on data.world might only be available on your corporate network or behind a firewall. For customers with a need to catalog data behind a firewall, we make our Virtual Data Connector available as an appliance that can be hosted at your site and communicates with data.world via a secure bridge protocol. This option is available to our enterprise tier customers. If you have this need, please contact our sales team at sales@data.world and they will help you with your options.",
    "url": "https://docs.data.world/en/99040-on-premise-data.html"
  },
  {
    "title": "Adding files to a dataset",
    "content": "Some of the sources for data you would like to put in a dataset include:\nYour local drive\nThe cloud\nLive data connections and data extracts\nURLs\nThe easiest way to add files from a local drive is to drag and drop them onto the add data box. Drag and drop allows you to add multiple files to a dataset at once:\nYou can also add files by selecting the Add data button or clicking anywhere in the Add data box. The Add data button opens the add data window which lists all the ways you can add data to a dataset. Note that in addition to the Upload from computer option you still have the option to drag and drop files into your dataset. Upload from computer is similar to drag and drop, but you can only select one file at a time for upload. Dragging and dropping is much more efficient for adding multiple files from a local drive.\nIf you want to add data files from the cloud you will need to configure your cloud service account to allow access by data.world. To configure a service, go to https://data.world/integrations/categories/import or click on the Add data button from the dataset overview and following the link to the integration page:\nOnce you have configured a cloud drive, it remains connected for further use. There is no limit to the number of cloud drives you can have configured.\nYou can select and add multiple cloud-based files at a time, and files sourced from the cloud can also be set to sync regularly--ensuring you'll always have the most up-to-date version of the file in your dataset.\nSync options can be changed at any time from the overview page of the dataset:\nIf you manually update a file (delete and re-upload it) or if a file updates automatically from a sync, all the previous versions of the file are preserved in data.world and can be downloaded at any time. In this way your data is preserved for auditing, accountability and versioning. To access past versions of your data go to the Activity tab on the dataset and select Versions, then click on the three-dot menu to the right of the version you wish to recover:\nIf you'd like to know how to link data through a URL see the article Adding files from a URL.\nWhen you select Add data on a dataset you'll be taken to a screen where you can choose a variety of options including your virtual connection listed under MY DATA SOURCES:\nWhen you select your connection name you'll have the choice of creating the dataset with a live connection or a data extract:\nData extracts are not available for all data sources. If this option is not available for your data source it will be greyed out--as shown above.\nThe main differences between a live table and and a data extract are as follows.\nOn a live table :\nData continues to live at its source and will not be ingested into data.world.\nAny queries executed against this dataset will be translated and executed in the data source.\nUsers may select tables to pull into the dataset, but cannot specify a SQL query.\nWith a data extract:\nData will be pulled into data.world and processed into our internal representation.\nYou can set it to update at specific intervals from the source.\nUsers can select tables to pull into the dataset or specify a SQL query whose results should be pulled into the dataset.\nAfter choosing live or extract you might be prompted to select a database, and then a schema, followed by tables--or you might just be presented with a list of tables. Your options are determined by the data source.\nWhen you get to the table selection you have the option of adding one or many at the same time. If you want to use all of the tables in your dataset, select Name at the top of the list:\nSelect Import ... tables and when the tables have been linked you will receive a confirmation and a reminder of whether this data is from a live connection or brought in with a data extract:\nFinally you will get a confirmation that your dataset has been created and when you close that window you'll be taken to your dataset overview page where you can document the dataset and edit the metadata:\nHave data on another site that you'd like to import to data.world for easy sharing, collaboration, and querying? No problem! As long as you have a direct URL to one of our supported file types and permission to access it, data.world will be able to import it easily. This is a great solution for importing data from the web, data portals, cloud storage apps, GitHub, and API endpoints! Even better, if the files change at the source, data.world can automatically update it.\nTo add data from a URL click the Add data button on the dataset overview page and select the Sync from URL option:\nIf the URL does not require authentication to access it, all you need to do to add it to your dataset is enter the shareable link (provided by the data source) into the source URL field and select Continue:\nFor sources where you need permission to access the data, first paste the URL, then choose authentication and select the appropriate option in the dropdown menu (OAuth, Token, or Username and password aka Basic):\nHeaders and POST body are used to make API calls to sites that support REST API. See the support docs on those sites for required values. Though it is possible to include authentication information in headers, we strongly recommend using the Authentication setting as collaborators on the dataset or project can see information in headers (including logins and passwords), but not Authentication values.\nWhen you have finished entering the required information from your URL, click Continue andyou'll be prompted to name the data file as it will be seen on data.world. Choose carefully, as this name cannot be edited later. Changing the name would require deleting the connection and creating a new one with the new name.\nIf data.world encounters an issue with the source URL, we will display an error requesting you to verify the link and the settings. Hovering over the ? next to the error message will show you the exact error returned:\nClicking Edit will bring up the same dialog you used to enter the initial parameters so you can make changes:\nNote that sometimes the error returned will say 404 Not Found instead of 403 Forbidden if it's an authentication failure even though the URL is correct. This result is a security feature of the API.\nOnce your files are added, configure them to update regularly through the automated sync settings.",
    "url": "https://docs.data.world/en/99041-adding-files-to-a-dataset.html"
  },
  {
    "title": "Finding shareable download URLs",
    "content": "A sharable download link, or pre-signed URL, is necessary for data.world to pull in files from external sources. This is the URL that will trigger the file download for anyone with the link.\nDepending on the source, these can sometimes be tricky to find, so we'll do our best to keep these how-to's accurate. Please let us know if you see something missing, or have other sources you'd like us to add!\nWe offer a secure, direct integration with Google Drive that will allow you to connect files to data.world without leaving our platform. For more information on that, see the Google Drive integration page.\nIf you would prefer not setting up that integration, you can add files one at a time by generating a public URL for each file.\nYou can find that URL by doing the following:\nWith the file open within Google Drive, click File > Publish to the web.\nSelect the file and format from the dropdown menus under Link.\nClick the Publish button.\nCopy the resulting link. This is the URL to use when adding files via URL to your data.world dataset.\nGitHub can be a great source to find interesting data - but how can we get that data into data.world?\nFirst, we need to find the data file in GitHub. That's simply a matter of navigating through the directory tree in a repository until we're able to open up a preview of the contents. Then, right click (or control-click) on the Raw button on the top right, and save the link address.\nIn the cases where a data file is too large to preview, then there won't be a Raw button - instead, right click on the Download button and save the link address.\nNow that you've copied the address of the data source, follow the instructions for adding files via URL to import the data into a project or dataset.\nTo upload files you have stored on S3, you can either make the file public or, if that's not an option, you can create a presigned URL.\nTo make your file public on S3, navigate to the file, right-click and select Make Public. After doing so, go to the Properties for the file, and the Link value can be used to upload to data.world.\nIf your file is private on S3, you still have the option to generate a presigned URL for sharing or uploading to data.world. First, you will need to install and configure the AWS CLI. Once that is in place, use the following command to generate the URL for each file:\nWhere S3 URI is in the format s3://mybucket/myfile.",
    "url": "https://docs.data.world/en/99046-finding-shareable-download-urls.html"
  },
  {
    "title": "Syncing your data",
    "content": "Data that is added from a live source (virtual connection), a URL, or one of our integrations can be automatically updated to keep it current with its source. The options for autosync are:\nDo not sync\nSync hourly\nSync daily\nSync weekly\nThe setting you choose affects all the files able to be synced in your dataset. The current autosync setting for your dataset is shown, and can be modified, on the overview screen above your dataset files. It can also be modified under the Settings and Automatic sync options.\nFiles can also be synced manually at anytime by clicking Sync now link from the file preview as below, or via the Sync now button from the file view within the workspace:\nIf a scheduled sync fails for any reason it will be attempted again on the next scheduled interval and an error message will be indicated in the file preview.",
    "url": "https://docs.data.world/en/99050-syncing-your-data.html"
  },
  {
    "title": "Match and extend your data",
    "content": "After uploading data to data.world, if your data contains a common, general type of information that data.world recognizes, we will suggest related information that you can use to enhance your data and research. As data.world processes your data, we will analyze each field's data type and format in an attempt to match it to any known data types within the data.world system. When a potential match is discovered, it will be indicated by an interactive green triangle displayed against that field:\nClicking the corner will present a menu which indicates that potential matches exist for this column of data.\nClicking on the highlighted menu item will presents a list of potential matches for this column. In many cases, this will only be a single item, although it is possible to have multiple matches as shown below.\nAdding a matched column will sometimes cause additional column choices to become available. In the above case, by selecting to include the \"us_state\" matched column, the user will be presented with a number of other \"Related\" columns. Related columns can be added to your table as well to provide additional levels of aggregation. Related columns generally represent containing entities. So in the case of States, Census Divisions and Regions contain the state:\nClicking \"Add related column\" will add this information to your table. Also note that columns can be renamed after adding them to your table using the pencil icon.\nUpon clicking \"Done\" the file will be reprocessed and new columns will be inserted. This could take a few moments depending on the size of the table, although it should generally be pretty quick.\nThe reprocessed table now contains the selected columns along with all of the original data columns. Note that if a new version of the original file is uploaded (with additional rows of data, or additional columns), the file will automatically be matched using the same algorithm.\nNow that we have our match columns, we can gain some additional context about these rows. Clicking on the blue \"bubbles\" will being up a small popup with some additional information.\nWhile these pop-ups only provide some basic information today, we will be working to flesh these out over time. Our ultimate goal is for these to contain complete descriptions, maps, flags, population counts, etc.\nMatch columns may be removed at any time from the column header menu:\nBy including the containing information in this table, we now have additional data that we can aggregate across. Let's jump in and see how many states our data has in each of the various census districts. First, choose to \"Query this file\"\nA simple aggregation sql query across this column tells us that we have two states in the Pacific Census Division.\nIf you have two tables, both of which have been matched to the same class, then you can easily join those tables using SQL. In the following example, two tables which have been matched to states will join on the entity by default.\nMatches are discovered based on the content of the column, not the name of the column.\nExample: If a column contains values such as 78703, 78731, 00501, and 24151, then it will be recognized as a Zip Code, even if the column is not named zipcode or postalcode.\nYou can find the full up-to-date list of matches in the ddw/ontology-v0 dataset. Specifically, this query gives a full accounting of available matches.",
    "url": "https://docs.data.world/en/99051-match-and-extend-your-data.html"
  },
  {
    "title": "Verifying your data with data inspectors",
    "content": "When you ingest a tabular data file on data.world it is run through a series of inspections to validate both the structure and content of the data in the file. If issues are found, the file is flagged and users can take action to fix them.\nThe warning flag can be found on the Dataset details page under the name of the file:\nYou can also see the indicator in the About this file section as Inspections on the dataset or project workspace for the file:\nTypes of indicators\nIssues are indicated using two indicators:\nWarnings indicated by yellow triangle : By far the most common, yellow triangles are there to alert you to potential problems with the data that might affect your ability to query it, or warn you that sensitive data (social security numbers, phone numbers, email addresses, etc.,) was detected.\nSever errors indicated by red circle: Very occasionally you will get a red flag which indicates that there was an error on ingest and data from the original file was lost. Possible reasons for the loss of data include:\nThe original file is corrupt.\nThere was a data type mismatch between the data type identified for the column and the data stored in it.\nData that you choose to connect to a specified linked data class had values that didn't match the linked data.\nFor a list of warnings and errors, see Data Inspections\nReviewing warnings and errors\nWhen you click the warning or error indicator, a window opens that lists all the issues captured by the data inspector. Review the issues and decide if you need to fix them or click Dismiss if you do not wish to be notified about them.\nOnce a set of warnings are dismissed, they will not show up in the file again even if you delete and reimport the file or update it. The ONLY way to get a list of all the warnings back is to ingest it again with a different name.\nIf you wish to correct the issues with files that were originally added to data.world by a direct add, you can download the file and make the corrections and re-upload the file.\nFor files that are synchronized from external services (such as cloud storage services), you will need to update the file in the source system and click the Sync now button to sync the changes.\nSometimes changes made to the data dictionary can cause error warnings in the data. For example, after ingesting a file, if you accidentally convert a String column to Integer, it will immediately cause a red flag on the inspections as some of the data would be left out on re-ingest due to a datatype mismatch:",
    "url": "https://docs.data.world/en/99060-verifying-your-data-with-data-inspectors.html"
  },
  {
    "title": "Document your data",
    "content": "Once you have created a dataset or project and added your files to it, you can make it easier to find and more useful to others by describing, or documenting it. Documenting consists of creating the metadata for your dataset or project and helps others to trust your data and work. Searches on data.world also look at titles, descriptions, summary, and tags to match search strings so the more completely you describe your data the more chance it has of being found.\nThe starting point for describing your data is the dataset or project overview page. From here you can edit the description, create the summary, assign tags, set the licensing, and complete the data dictionary:",
    "url": "https://docs.data.world/en/99061-document-your-data.html"
  },
  {
    "title": "Description",
    "content": "Datasets, projects, all the files in each, and all the columns in any structured data files have description fields associated with them. Descriptions are very short and serve as a quick reference for the item they describe. To edit the description for a dataset you can select Edit next to the description, Edit next to About this dataset, or navigate to the Settings tab:",
    "url": "https://docs.data.world/en/99062-description.html"
  },
  {
    "title": "Summary",
    "content": "The summary is one of two documents created with a dataset or project. The summary is where all of the information about the origin of the data, why you created the dataset, further documentation of your work, etc. is found. Use the Summary section to tell your data's story. For example:\nWhere did the data come from? Cite and link to your sources or include your details for a 'citation request'. Not only does this give credit where credit is due, but it helps other people evaluate the data's suitability for their needs.\nIf you think a particular piece of context will be useful to others, add it.\nThe best summaries cover the \"who, what, where, when, why, and how\" of the data.\nWhat's the data telling you? What would others be interested to know about it? What have others found using this data?\nIf the data has associated data dictionaries or other documentation, upload it and then link to it from your Summary.\nSummaries are created and edited in either the data.world Simple Editor or in Markdown.",
    "url": "https://docs.data.world/en/99063-summary.html"
  },
  {
    "title": "Organizing a dataset with file labels",
    "content": "When a dataset contains many files, determining the purpose of each file can be difficult without examining it more thoroughly. By adding file labels, you can see each file's category at a glance. The following file labels are available:\nraw data\nclean data\ndocumentation\nscript\nvisualization\nreport\nYou can add file labels from a dataset overview page by clicking the three dots icon on the right side of the file preview and choosing Edit file metadata:\nThat will bring up a new window where you can add file labels and edit other file metadata:\nTo update those labels later, just follow the same steps and add or remove them as needed.",
    "url": "https://docs.data.world/en/99064-organizing-a-dataset-with-file-labels.html"
  },
  {
    "title": "Tagging",
    "content": "Tags are a powerful feature that you can use in a variety of ways to facilitate access to your data. For example, tags can be used to organize and group your dataset or project by topic, category, source, department, or team. They can be searched for explicitly with the tag search operator, and can also help to filter down more generic search results.\nYou can add or remove tags from a dataset or project's Overview page with either the Edit or Add tags links on the right side:\nThere is no limit to the number of tags you can use for a dataset, and there is an autofill feature on the tag field. If the dataset is owned by an organization, the tags displayed for autofill are chosen from all the tags used by the organization. If the dataset is not owned by an organization, the autofill suggestions are from a generic list of tags as well as from tags you have recently created.",
    "url": "https://docs.data.world/en/99065-tagging.html"
  },
  {
    "title": "Data dictionary",
    "content": "The data dictionary contains all the metadata (data about the data) for the files, tables and columns in a dataset. For all files it contains:\nThe names of all the files in the dataset\nA place to add descriptions for each file\nThe labels for each file\nand for tabular files it has:\nThe column names\nThe format of the data in each column\nA place to add a description for each column\nYou can get to the data dictionary either from the Overview tab (right below the Summary) or from the Documents section in the left pane of the workspace:\nData dictionary entries for each file are edited separately by selecting the Edit link next to the filename in the data dictionary document. Every file--no matter what type--has a data dictionary entry which contains the file metadata for the file:\nTabular files also have optional advanced settings and csv settings additional options in their file metadata:\nThe Authentication setting allows you to specify password, token, or OAuth parameters if the source URL requires authentication. The Headers setting is to specify options to modify the response from the URL, e.g., to specify a file content type. The Post body setting enables you to switch the request method from GET to POST if the source URL requires it.\nThe CSV settings section manages how your comma separated value format files are handled. To access it, select Show to the right of the section:\nTabular files also have a tab for columnar metadata in their data dictionary where you can rename the columns, change their format, and add descriptions for them:\nChanging column names and adding a description is a great way to avoid the ambiguity that comes from having multiple columns with the same name. It also renders obscure column names understandable.\nChanges to column names, descriptions, and data types propagate throughout data.world to every project that references the dataset, and the changes remain even if the data is updated from an external source.",
    "url": "https://docs.data.world/en/99066-data-dictionary.html"
  },
  {
    "title": "Setting a license type",
    "content": "Setting a license type for a dataset is important to explicitly define how others may use the data. Licensing is determined by two factors:\nThe licensing of the source documents in the dataset\nThe wishes of the dataset owner\nThe general rule of thumb is that the most restricted license for the source material is the least restricted license that can be used for the dataset. However--existing source licensing again being taken into account--the owner of the dataset can choose even more stringent licensing for others who wish to use the dataset. To set the license type:\nGo to the dataset overview page\nClick Edit next to About this dataset on the right side of the screen\nChoose the appropriate license from the Public license dropdown menu and save\nWhen creating a dataset on data.world, use the following articles to help determine the license type you should use:\nLicensing and data you found\nLicensing and data you own\nCommon license types for datasetsCommon license types for datasets",
    "url": "https://docs.data.world/en/99067-setting-a-license-type.html"
  },
  {
    "title": "What does it mean for a dataset to be discoverable?",
    "content": "When a dataset is set to be discoverable, it is listed in public search results and its metadata (description, summary, creator, and license), contributors, and discussion can be viewed by anyone on data.world. For those customers with a closed network, it is visible to everyone on that customer installation. None of the individual files or tables in the dataset are visible, however. Only files that are explicitly marked as \u2018Preview\u2019 files are viewable by end-users until the end-user is granted read or write access to the dataset. The reason for the discoverable flag is to expose the existence of a dataset to others who might have a use for it, while at the same time maintaining control over who has can access the data in it. It is a useful tool for making users in other groups aware of the dataset so they can be granted permissions to it on an individual or group basis.\nOnly a dataset that is owned by an organization (as opposed to an individual user) and which has its permissions set to private can be made discoverable. When a dataset is owned by an organization but set to private, it is not automatically shared with anyone else in the organization except the admins. Sharing a dataset or making it discoverable can be changed from the Settings tab on the dataset. Making a dataset discoverable is done after the dataset has been created from the Settings tab:\nWhen a dataset is discoverable this is what it looks like to everyone who doesn't have explicit permissions to it. Notice the indication under the summary that there are files in the dataset to which the viewer doesn't have permission:\nWhen someone wants to see the rest of the dataset, they select the Request access button in the upper right of any tab on the dataset. The creator receives notice of the request in email and can then either approve or disapprove it:\nWith discoverable datasets we introduced the ability to make a the existence of a dataset available without exposing any of the data in it. In this article we'll discuss how to take this feature and extend it even further by adding sample data files that can also be viewed. Users can view the samples to determine if they want to request access to the full dataset. A preview of the sample file is visible on the dataset overview page. If the user evaluating the data would like to see more than the preview, the file can be downloaded and viewed.\nThere are different ways to create sample preview files. A sample may have all of the columns as the original file, but not all the rows. Or it may have only some of the rows and some of the columns--columns with sensitive data having been removed.\nThe quickest way to create an extract from a file to use as a preview in a discoverable dataset is to:\nQuery the original table and limit the results\nSave the query results (the extract from the table) as a new file in the dataset\nIf there is sensitive data in the file that you would like to mask you can either modify your query to exclude or mask the data, or use a custom_types.ttl file to set the column to a masked data type. See the linked articles for more information.\nTo query the original table, select the Explore dataset button on the top right of the dataset overview page. Then select the table you would like to create the extract from in the list in the left sidebar and click Query:\nA sample query will be presented at the top of the window and all you need to do is change the LIMIT clause to however many rows you would like to be available in the preview. Note that only five results will be previewed on the dataset Overview tab, but the rest are available for download):\nHit the Run query button, then Download and Save to dataset or project:\nYou will be given the choice of using a live view or using a data extract.\nIf you want to be able to apply custom data types to the columns in the table, choose use data extract as custom types cannot be applied to live data.\nName your dataset, and the current dataset name will automatically be populated in the Dataset/Project field:\nIt's a good idea to name the resulting table so that it's easily identified as a sample of a data file, not the file itself.\nEnabling file on a discoverable dataset to be previewed is a quick and easy process:\nOn the dataset overview tab, scroll down to the sample file and select the three dots on the right to edit its metadata\nCheck the Preview box to make a random 5-line sample of the file visible from the overview page of the dataset and save\nFiles that have been made previewable are flagged with the Preview label on the overview tab in the creator's view:\nEven though the file cannot be accessed directly in data.world until permission has been granted by the creator, any file visible on the overview page can be downloaded by anyone that can see it.",
    "url": "https://docs.data.world/en/99068-what-does-it-mean-for-a-dataset-to-be-discoverable-.html"
  },
  {
    "title": "Masking data for discoverable datasets",
    "content": "Data masking enables you to either hide or change the values of a column containing sensitive data in a data.world dataset. One common use case for masking data is creating a preview, or sample, table for discoverable datasets. If the sensitive data is masked, anyone can see that it exists in the data source without being able to see the values until and unless they are granted access. data.world has two different ways of masking data:\nMasking the data returned from a query - Can be used with both live and extracted table data\nUsing a .ttl file of custom data types - Can only be used with extracted table data",
    "url": "https://docs.data.world/en/99074-masking-data-for-discoverable-datasets.html"
  },
  {
    "title": "Managing sensitive data from a query",
    "content": "There are three easy ways to mask one or more columns returned by a query so that you can share either the query or a data extract from the query without sharing any sensitive data:\nSelect only the columns without sensitive data:\nSubstitute an anonymizing text string for the data in the column(s)\nHash the sensitive data in the results\nThe results all meet the goal of hiding sensitive data, in this case customer phone numbers, from a discoverable data extract:\nExclude results\nMask results\nHash results\nEach of these is good for a quick share of one data resource. But if you want to be able to apply a regular masking strategy to data across your organization, creating a ttl file with defined custom data types is a better long-term solution. You can find details about how to create the sample preview file from the query here.",
    "url": "https://docs.data.world/en/99075-managing-sensitive-data-from-a-query.html"
  },
  {
    "title": "The custom_types.ttl file",
    "content": "If you want a masking solution that is reusable then using a custom data type defined in a .ttl file is a good solution. Custom data types are stored in a .ttl file in the dataset that is designated to house the organization's ontology files. You can get a copy of our sample custom_types.ttl from support and either use it as-is or extend it with your own custom maskings.\nThe requirements to use a custom data type defined in a .ttl file are:\nExtracted tables only - The column you want to apply the data type to must be in an extracted table located on data.world. Custom data types do not work on live tables yet.\nData types must match - The data type of the column must match the data type defined in the data.world ontology. For example, if the original column is of type String, the custom type chosen to apply to the column must also be of type String.\nData format must follow expected pattern - The format of the data in the column must match the format expected by the custom type. If the custom type is formatted to render US phone numbers with the format XXX-XXX-XXXX, and one of the cells has XXXXXXXXXX, the custom type for the mask will not be applied.",
    "url": "https://docs.data.world/en/99076-the-custom_types-ttl-file.html"
  },
  {
    "title": "Using a custom_types.ttl file",
    "content": "This article contains the minimal steps to prepare the custom_types.ttl file from data.world for use as-is (includes the custom data types of social security numbers, masked social security numbers, US phone numbers, and masked US phone numbers). If you would like to create your own custom data type, you can find instructions here.\nTo install and configure the custom_types.ttl file:\nRequest a copy of the file from data.world support.\nCreate the dataset ddw-ontologies in your organization., and give everyone in the organization read access to it.\nOpen the custom_types.ttl\u00a0file and change change the line @prefix\u00a0hg: <https://ddw-doccorp.data.world/d/ddw-ontologies/> . to prefix\u00a0hg:\u00a0https//your-org-name.data.world/d/ddw-ontologies, replacing your-org-name with the agentid of your organization:\nYou can find your organization's agentid in the url of your org home page:\nUpload the custom_types.ttl to the ddw-ontologies dataset.",
    "url": "https://docs.data.world/en/99077-using-a-custom_types-ttl-file.html"
  },
  {
    "title": "Adding your own custom data types",
    "content": "This article covers how to extend the masked data types available in your custom_types.ttl file. If you do not already have a custom_types.ttl file, follow the steps for getting one found at the bottom of Masking data with custom data types. If you are already using custom data types, download a copy of the custom_types.ttl file from your organization's ddw-ontologies dataset.\nThe easiest way to create a new masked data type in your custom_types.ttl file is to copy an existing data type and paste it at the bottom of the file to use as the basis for your new type:\nIf you are unfamiliar with the nomenclature and structure used for .ttl files, see our overview of the custom_types.ttl file\nThe triples (or lines of the file) you need to change are:\nThe name of the data type in the ontology - hg:masked_us_phone_number a csvw:Datatype;\nThe title for the menu - dc:title \"Masked US Phone Number\" ;\nThe description - dc:description \"US phone number with area code\" ; - just\nHow you want it rewritten - dwo:mask [ a dwo:RegexMask; dwo:matchPattern \"([0-9]{3})-([0-9]{3})-([0-9]{4})\"; dwo:rewritePattern \"XXX-XXX-XXXX\"]. )\nFor this example you do not need to change:\nThe expected input format - csvw:format \"[0-9]{3}-[0-9]{3}-[0-9]{4}\";\nDo not change:\nThe location of the definition- dwo:isDefinedIn hg: ;\nThe data type of the output string - csvw:base dwo:maskedString;\nIt is a highly recommended to test your new data type on a test table before applying it to production data. If you inadvertently create an invalid data type, you will need to recreate any table you applied it to.",
    "url": "https://docs.data.world/en/99078-adding-your-own-custom-data-types.html"
  },
  {
    "title": "Applying custom data types to sample table extracts",
    "content": "Once you have a sample preview table for a discoverable dataset created and a copy of the custom_types.ttl file in your organization's ontologies dataset, you can apply the custom data types to the columns in the sample preview table:\nGo to the file you want to preview on your discoverable dataset and select Edit column info from the dropdown menu to the right of the column you want mask:\nThe column details window will open at the location of the column you chose. Pick a new custom data type from the Select custom type menu and click Save:\nNow when you look at the cust_phone column on the preview table, the data will be in the masked format:\nThough the masked data cannot be viewed in data.world, the viewer can still download the preview file so if you need absolutely lock down access to the data, you should create your preview file from a query and anonymize the data with the query before creating the the preview file from it.\nIf you change your mind and want to revert to the original format, you can. The underlying data in the columns is still there, it is just masked in the view. To revert to the original format\nSelect Edit column info from the dropdown menu on the right of the column name while viewing the table.\nChoose the dropdown next to the custom type on the column, and click Revert to original type.",
    "url": "https://docs.data.world/en/99079-applying-custom-data-types-to-sample-table-extracts.html"
  },
  {
    "title": "Overview of the custom_types.ttl file",
    "content": "In this article we introduce the composition of the custom_types.ttl file and the vocabulary used to describe it. Then we go on to discuss the format of the file and indicate what should not be changed, what must be changed, what can be changed, and how. If you do not currently have a ddw-ontologies dataset with a custom_types.ttl file in it, you can request one from support. Your custom types file can be called anything you'd like and stored in any of your organization's datasets that you would like.\nThough you can name your .ttl file anything you would like and put it in any dataset you would like, we recommend using ddw-ontologies as the name for your dataset and storing the .ttl file there.\nA .ttl file is divided into two parts: the prologue at the top followed by the body. The prologue is where short forms of the prefixes for the iris (addresses or references for resources) are defined. These short forms are used in the body so that you don't have to type out the long form of the prefix each time you describe a resource.\nMost of the prefixes in the custom_types.ttl prologue refer to standard ontologies and should be left alone. The one exception is the hg prefix which should be changed to reference your organization's ddw-ontologies dataset:\nBecause you are defining the prefixes in the prologue, you can rename them, for example changing hg to something else. However, if you do, you will also need to change all the references to that prefix elsewhere in the file:\nThe body is made up of information about resources. This information is in a form called a triple . A triple has a subject-predicate-object format, which is\u00a0a standardized way of describing something. For example, one triple might be, \"Data science is fun for everyone\" \"Data science\" is the subject, \"is fun for\" is the predicate, and \"everyone\" is the object.\nTriples for the same resource can be grouped together in a stanza. The benefit of grouping the triples for a resource is that\nThe subject in the triple does not have to be rewritten for every triple in the stanza\nYou can see all the triples for the same resource at once.\nThe first stanza of the body of the data.world custom_types.ttl file describes the data.world ontology. It needs to remain unchanged for custom types to run.\nThe second stanza contains an example of a custom data type defined by data.world:\nThe following table shows the triples in the stanza separated into subjects, predicates and objects. The elements that can be modified are shown in bold. Notice that only the subject and the objects that are text strings can be changed, and the prefix for the subject should only be changed if it was changed in the prologue:\nLine\nSubject\nPredicate\nObject\nline 1\nhg:ssn\na\ncsvw:Datatype\nline 2\n\ndc:title\n\"SSN\"\nline 3\n\ndc:description\n\"Social Security Number\"\nline 4\n\ndwo:isDefinedIn\nhg:\nline 5\n\ncsvw:base\nxsd:string\nline 5\n\ncsvw:format\n\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\"\nline 7\n\ndwo:contraintsOptional\ntrue\nThe third stanza shows a masked custom data type:\nAs in the previous example, the elements that can be modified are in bold:\nLine\nSubject\nPredicate\nObject\nline1\nhg:masked_ssn\na\ncsvw:Datatype\nline 2\n\ndc:title\n\"Masked SSN\"\nline 3\n\ndc:description\n\"A Social Security number (SSN) is a ...\"\nline 4\n\ndwo:isDefinedIn\nhg:\nline 5\n\ncsvw:base\ndwo:maskedString\nline 6\n\ncsvw:format\n\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\nline 7\n\ndwo:mask\n[ a dwo:RegexMask; dwo:matchPattern \"([0-9]{3})-([0-9]{2})-([0-9]{4})\"; dwo:rewritePattern \"XXX-XX-$3\"]\nThe example is complicated a bit by additional predicates and objects nested in the object of the triple on the seventh line. However, as before, only the subject and objects that are text strings can be modified. Everything else should stay the same. For more information about Regex and how to use it, visit the Oracle Java documentation.",
    "url": "https://docs.data.world/en/99080-overview-of-the-custom_types-ttl-file.html"
  },
  {
    "title": "Dataset best practices",
    "content": "Create a dataset to house data, metadata, and supporting documentation when the data could be used in many different analysis projects. Create a project to house all of the work that goes into an analysis project, and link in the datasets to support your work rather than duplicating (see the article on datasets vs. projects for more information).\nWhen uploading tabular data, we recommend a CSV file format over an Excel format (xlsx) as we can support larger file sizes for querying.\nRemove any headers, footers, or notes outside of a single row of column headers from the data file. Include any removed content in the dataset summary or upload as a separate notes file within the dataset. Keeping the data file basic (machine readable over human readable) ensures data.world will import and analyze the data with the best accuracy.\nTag and document your data so that others will better understand and use the data.\nUse the data inspector to verify your data has imported correctly and for a view into the data\u2019s quality.\nEnsure your dataset is within the data.world size limitations. You can also upload a zip file, and then attempt to extract it after upload if presented that option in the application. If you need data.world to support a larger dataset size for queries, please contact us and we'll do our best to accommodate.\nFiles within a dataset are displayed alphabetically, so if the files in your dataset should be displayed in a particular order, name them accordingly (01_*.xls, 02_*.pdf, etc.) or use the summary to take others through your data and analysis.\nSearch first to see if the same dataset has already been uploaded, and if so, consider collaborating or linking directly to that dataset rather than uploading a duplicate. There\u2019s nothing wrong with uploading your own copy, but sharing through collaboration or direct linking will keep that data\u2019s \u2018story\u2019 in one place.",
    "url": "https://docs.data.world/en/99081-dataset-best-practices.html"
  },
  {
    "title": "Work with data: Projects",
    "content": "",
    "url": "https://docs.data.world/en/94893-work-with-data--projects.html"
  },
  {
    "title": "About projects",
    "content": "Projects bring datasets together with documentation and analysis. This is where work and collaboration happen. A project, as the name implies, likely has a beginning and an end. Data in it is shared and analyzed, and insights are derived from the analysis and written up in the project.\nProjects are where all querying, analysis and discussion of data takes place. Data in different datasets can be used for many different projects, but each project contains all and only the data that is relevant for that project. The information in a project can come from datasets, files attached directly to the project, insights written by the project's team members about the data and the project, and discussions about the project.\nThe biggest difference between a dataset and a project is that datasets can be linked to and included in projects, but projects cannot be linked to or included in other projects or datasets--nor can the files that are added directly to a project. With a project you can run queries against the data, analyze it, share it and create charts and visualizations from it.\nAlthough you get an option to add files directly in a project, as a best practice, it is recommended that you create datasets and add files in them and not directly in a project. The datasets can then be linked to the projects.\nThrough hundreds of interviews with people who work with data, we have found that most work stems from a question rather than a particular set of data. It is the question that drives the search for relevant data, generating insights, and presenting reproducible findings, yet there hasn't been a great way to keep all your work in one place--not to mention collaborate easily with others on the project.\nProjects help you to capture and share the most important aspects of your work as the project unfolds, even across multiple datasets, from question to conclusion.\nWith data projects you can:\nKeep all project data in a single place by linking datasets or adding data directly in the project\nAdd people as contributors or viewers to your project and see the project activity stream.\nUse the workspace to document, explore, query and chart any of the local or linked data and files.\nShare and discuss insights\nCheck out some of the following sample projects:\nAre dog size and intelligence linked?\nWhat does Census data tell us about politics?\nEaten by Raptors?",
    "url": "https://docs.data.world/en/95433-about-projects.html"
  },
  {
    "title": "Project details page",
    "content": "Browse to the projects from the application Home page, an Organization Profile page, or the search results page.\nWhen you click on a project and open it, you will see the Project Details page. The header at the top of the page has several valuable pieces of information and actions that you can take in regards to the project.\nOn the left of the header are:\nAn icon for the project that includes indicator for the status of the dataset. For example, Approved, Deprecated. Needs Review.\nThe name of the person or organization that owns the project. Click the owner link to get to their profile page.\nThe name of the project.\nOn the right of the header are options to:\nShare the project\nComment on the project (available if you don't have access to the project.)\nBookmark it (and see how many others have bookmarked it)\nAccess the Launch workspace menu. From the Workspace menu you can also open the project in a third-party application (integrated with data.world).\nThree dot menu to request access to the project and options to subscribe or unsubscribe from email notifications for the project (available if you don't have access to the project.)\nWhen you open a project, you land on the Overview tab. The Overview section includes the Status of the project, if it is set, a short Description of the project, and the longer Summary information.\nMuch of the information on this tab is configurable by your organization. In the following example, the More Information section contains an example of completely configured metadata.\nNext, you get an option to:\nAdd data\nWrite a query\nCreate a new insight\nIf there are any insights associated with the project, they are shown on the Overview tab. From here, you can also bookmark the insights or comment on them.\nThe Activity tab of a project contains information about all the events for the project, and it also provides access to previous versions of the project. There are two sub-tabs, the All Activity and Versions.\nThe All Activity sub-tab is a list of each event that has occurred with this project. You can see who made changes and when, and the type of change.\nClick Versions to view the sub-tab containing version information for the project. Click the Three dot menu to download a previous version.\nThe Insights tab on the Project page contains all the insights created from the project. The insights are previewed on the main page and there is a thumbnail gallery on the right. You can comment on an insight and bookmark it.\nThe Discussion tab captures all the communication between team members about the project. You get an option to Flag or Like comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nThe Settings tab is for administrators to manage the project. There are three sub-tabs, General, Access and ownership, and Webhooks.\nThe General sub-tab contains administrative settings, which include:\nAutomatic syncing options.\nAdditional notification recipients.\nArchive options for files.\nA Delete Project button.\nThe Access and ownership sub-tab allows you to manage access to the project and make projects visible to community members.\nThe Webhooks sub-tab is for configuring and managing webhooks to the project.",
    "url": "https://docs.data.world/en/94895-project-details-page.html"
  },
  {
    "title": "Project workspace",
    "content": "Click the Launch workspace button to get into the detailed view of the project files.\nThe project workspace has five main parts:\nProject directory: View the project summary and data dictionary and list of contents of the project that is, project files, connected datasets, queries, and insights. Use the Add button to add new resources to the project.\nTabs: The contents of the project open as tabs in the workspace.\nObject viewer: Each tab has an object viewer where you can interact with the object that is open.\nAbout section: Where available, the About section shows details about the object that is open in the Object viewer.\nProject schema: For some objects, such as queries, the project schema is available for reference.",
    "url": "https://docs.data.world/en/94896-project-workspace.html"
  },
  {
    "title": "Creating projects",
    "content": "Creating a project includes the following steps:\nCreate the basic structure of your project.\nAdd data to your project. The best way of adding data to your projects is by linking datasets to it.\nOnce the data is added, create queries for your data and create query templates to make them accessible to non-technical users.\nDocument your project by adding summary, tags, file labels, etc. The more documentation you add to your data, the more it becomes usable.\nPost insights to your project and make it a richer experience for the users.\nCollaborate with others on the project",
    "url": "https://docs.data.world/en/95127-creating-projects.html"
  },
  {
    "title": "Starting a data project",
    "content": "To create a project:\nClick the + New button in the Main header toolbar of the application. In the Create a new window, click Create new project.\nOther places from where you can start a project:\nIf you are on the Organization profile page, on the Overview or Resources tab, select the Project option from the New resource button.\nIf you are working on a dataset, click the arrow next to Explore this dataset button to start creating a new project for the dataset. Alternatively, add the dataset to an existing project.\nOn the Create a new project page, set the following:\nOwner: Select the owner of the project. By default, if you are in an organization your organization is set as the owner of the project. You can choose another organization from the dropdown or you can switch to personal account if you wish to be the owner as well as the creator of the project.\nProject name: Provide a project name.\nShare with: Specify who can see the project. Permission options to the project are either no one, other members of your team (if the team is set to be the owner), or public to the data.world community.\nIf you started creating the project from a dataset page, you will see the dataset that is getting added to the project.\nClick Create project.\nOnce the project is created the main page is refreshed to show the workspace of the project. Go to the Project Summary section and add a detailed summary for the project. Use markdown to add rich content to the summary. Click Done to save the changes.\nClick the name of the project in the title bar to go back to the Project details page.\nOn the Overview tab, in the Overview section, click the Edit button to add a description for the project and to add Tags to the project. Tagging projects help search projects later. In the Tags field, start typing the tag. If it already exists, it will show in as a suggestion with the number of times it is used. Click on the tag to add it or press Enter to create a new tag and add to the project.\nNext, go to the Settings tab and set the following options for the project in the General section and click Save.\nAutomatic sync options: For details see Configuring sync options for projects.\nCaching live tables and live views: Select the Allow previews and data to be cached for one day  option to enable caching of live tables and live views to make browsing of data more efficient. By default caching is disabled.\nAdditional notification recipient: Various notifications for projects are sent to users contributing to projects. If you want an additional email address to get notifications for the activities on a project, provide those email addresses in this field. Only one email address can be provided here.\nArchive options: Select the Expand archives by default to automatically expand all files in an archive file (for example, zip files) added to the project. Select the Prefix file name with archive name option to add the name of the archive file to the file names extracted from the archive file. This helps easily identify the source of the file.\nFor example, when these settings are enabled and a user adds a file latest_orders.zip with two files in it (california_orders.xlsx and austin_orders.xlsx), the system automatically extracts them and add the files as - latest_orders/austin_orders.xlsx and latest_orders/california_orders.xlsx to the project.\nOnce you are done setting the basics of a project, add files and connect the project to data sources. For information about adding data to your project see Connect data to your project.\nNote that as you edit your project, a record is created of the changes being made and versions of projects are created for download.",
    "url": "https://docs.data.world/en/95585-starting-a-data-project.html"
  },
  {
    "title": "Configuring webhooks for projects",
    "content": "Before adding the webhook URL for the project in the application, configure the webhook. Note down the URL for the webhook.\nTo configure web hooks for projects:\nBrowse to the project.\nIn the Settings tab, go to the Webhooks section and set the following:\nProvide a brief description of the webhook.\nProvide the URL for the webhook.\nClick Add.",
    "url": "https://docs.data.world/en/94899-configuring-webhooks-for-projects.html"
  },
  {
    "title": "Configuring sync options for projects",
    "content": "For files that are added to projects through virtual connections can be set to sync automatically on regular intervals. This ensures that the users of data in the application have access to updated content.\nThe sync options available are:\nNever\nEvery hour\nEvery day\nEvery week\nNote that when auto sync is enabled for a project, only the files added directly in a project are synced. Any files in the dataset are not synced. to sync those files, enable auto sync for datasets.\nTo configure auto-sync for projects:\nOn the Project page, browse to the Settings tab.\nIn the Automatic sync options, for the Files setting, select from Every hour, Every day, or Every week. Click Save.\nIf you are using streams to add data to projects, for the Streams setting, select from Every hour, Every day, or Every week. Click Save.\nIf your project is already linked to virtual data sources, you get the option to enable these sync options from the Overview tab of the Project page.\n\nWhen the sync options are changed from the Overview tab, the setting for both Files and Streams are changed.\nIf available, you can always sync files manually by clicking the Sync now option next to the files. Clicking the button next to any file, syncs all files in the project.",
    "url": "https://docs.data.world/en/94898-configuring-sync-options-for-projects.html"
  },
  {
    "title": "Sharing projects",
    "content": "Once you have created a project share it with other users so that they can use the project or contribute to it.\nWhen the project is shared with the community, users can launch the workspace of the project to see its contents, can request access to contribute to the project, can suggest changes to the name, description and tags of the project, and can start a discussion on the project.\nMaking a project available to community users:\nCreate a project. While creating the project set the Share with option to No one.\nOnce you are ready to share the project with community users, go to the Settings tab of the project. In the Access and ownership section, in the Visibility settings, click the Share with community.\nA message appears asking for confirmation to make the project visible to the community. Click the Share with community button.\nYou can create a project in an organization and not share it with any members of the project till it is all set and ready for use. Once you are ready that members of the project share it with all of the organization or selective members.\nTo share a project with organizations and organization members:\nCreate a project and do not share with anyone.\nWhen you are ready for the rest of the org to use the project, go to the Settings tab of the project. In the Access and Ownership section, do one of the following:\nIn the Manage access settings, click the Grant access to all members button to invite everyone in an organization to access the project. Sharing the project with all of the organization ensures that as new members get added to the organization, they automatically get access to the project.\nClick the Grant access button, to give access to only selective members of the organization. or to give special permissions to certain members of the organization. For example, you could set all of the organization to be able to view the project, but you may want to give only handful of members the ability to edit the project. You can also invite members of the community to access your project.\nIn the Grant access window, do the following:\nIf you are granting access to an organization, the name is selected by default and you can't edit it. If you are giving access to selective members, you get the option to type the username, full name, or email . To search for users in the community, click the Search all data.world members link. If you do not find someone you are looking for, type in their email address to invite the person to view your project. If they are not a user of data.world, they are sent an email with an invitation to join data.world to view your project.\nSpecify if the members of the organization can Discover, View, Edit, or Manage the project.\nClick the Add a custom message link to open the text box to add an optional message to the invite.\nClick the Grant access button.\nYou can create projects in your personal space and share with an organization when you are ready. You can only suggest projects to the organizations that are setup to accept suggestions.\nTo share your personal project with an organization:\nCreate a project in your personal space.\nWhen you are ready to share the project with an organization, go to the Settings tab of the project. In the Access and Ownership section, click the Grant access button.\nIn the Grant access window that opens, search for the organization where you want to suggest the project. Only the organizations that have the Allow community members to propose new resources setting enabled show in the list. The organization administrators are notified about the invitation and they can choose to accept the invitation and contribute to the project.\nWhen you make a project discoverable, users can view certain aspects of the project and can request access to contribute to the project. All such access requests are available in the Notification page and on the Settings tab of the Project Details page.\nTo manage access requests:\nFrom the Notifications page, approve the access request or browse to the project for which access has been requested.\nFrom the Settings tab, go to the Access and Ownership section. Here you can review the request and accept or reject and adjust the permission level for the user requesting access.",
    "url": "https://docs.data.world/en/95379-sharing-projects.html"
  },
  {
    "title": "Connecting data to projects",
    "content": "After you have created your project, you can add data to a project in the following ways:\nLink datasets to projects\nAdd files directly to a project\nSync from URLs\nUse integrations to add data to projects\n",
    "url": "https://docs.data.world/en/94900-connecting-data-to-projects.html"
  },
  {
    "title": "Linking datasets to projects",
    "content": "Linking datasets to projects is the best way of adding data to projects.\nTo link datasets to projects:\nBrowse to the project where you want to add a dataset.\nIn the Project main page, click the Add data button and select the data.world dataset option. Or, in the Project workspace page, click the Add button and select the Dataset option.\nIn the Connect a dataset to this project window, explore the list of datasets available in the following tabs. Use the search bar to narrow down the list of available resources.\nYour resources: A list of datasets you have created or are part of the organizations you have access to.\nYour bookmarks: Shows the list of datasets bookmarked by you.\ndata.world community: Shows the list of open datasets available in the data.world community.\nClick the View button to see the details of the datasets and when you are ready to connect a dataset to a project, click the Connect button.\nClick Done to close the window.\nThe datasets linked to the project are available from the Data sources section on the Project main page and from the Connected datasets section in the Projects workspace.",
    "url": "https://docs.data.world/en/95445-linking-datasets-to-projects.html"
  },
  {
    "title": "Uploading files directly to the project",
    "content": "Uploading files and data directly to your data project is great for materials specific to the project such as images, documentation, and code. You can also add data this way, but because you cannot link projects to other projects, data is generally best placed within datasets to make reuse easier.\nTo add files to projects:\nBrowse to the project where you want to add the files.\nDo one of the following:\nOn the Project details page, click the Add data button and select New file.\nIn the Project workspace, click the Add button and select Project file.\nIn the Add data from anywhere window that opens, either drag and drop the file on the Upload from computer option or click it to add a file to the project.\nThe added file is displayed in the Project files list.",
    "url": "https://docs.data.world/en/95557-uploading-files-directly-to-the-project.html"
  },
  {
    "title": "Syncing files from URL",
    "content": "Have data on another site that you would like to import to data.world for easy sharing, collaboration, and querying? No problem! As long as you have a direct URL to one of our supported file types and permission to access it, data.world will be able to import it easily. This is a great solution for importing data from the web, data portals, cloud storage apps, GitHub, and API endpoints! Even better, if the files change at the source, data.world can automatically update it.\nTo add files from a URL:\nOn the Projects Overview page, click the Add data button and click New file.\nIn the Add data from anywhere window, click the Sync from URL option.\nIn the Add file from URL window, provide the URL of the data source and any authentication required to access the data. If the URL does not require authentication to access it, enter the shareable link provided by the data source into the source URL field and click Continue. The authentication options are:\nUsername and password (HTTP): Provide the username and password required to access the source file.\nOAuth: This is the modern and more secure authentication used by many sites. Click one of the options next to the field to set the OAuth connection with your data source. Once the connection is established, you can select it from the dropdown added under the Authentication field.\nToken: This authentication type is for sources that require token based authentication.\nSet the Headers and POST body to make API calls to sites that support REST API.\nClick Continue.\nOn the next screen provide the File name. Choose carefully, as this name cannot be edited later. Changing the name would require deleting the connection and creating a new one with the new name. Click Done.\nIf the application encounters an issue with the source URL, it displays an error requesting you to verify the link and the settings. Click Edit to go back to the file window to fix the errors.\nOnce your files are added, configure them to update regularly using the automated sync settings.",
    "url": "https://docs.data.world/en/94903-syncing-files-from-url.html"
  },
  {
    "title": "Querying data",
    "content": "After you have added data to your project, create queries to extract information from the data sources and make it consumable for users. All data files are normalized so they are immediately queryable and joinable, whether they are similar formats or not. This lets you jump right into analyzing and querying to perform calculations, produce summaries, and manipulate data across many different formats and locations.\ndata.world supports SQL and SPARQL query languages. While data.world is built on semantic web technologies that are best queried using SPARQL, we recognize that most people are more familiar with SQL. For that reason, we have developed our own version of SQL to easily query data.world datasets.\nSQL: If you are new to SQL, we have documentation that will walk you through everything from What is a database to How do I join all these different tables together so I can query them all at the same time. There is also a tutorial based on the exercises in the documentation.\nSPARQL: SPARQL (pronounced \"Sparkle\") is a powerful query language to retrieve, modify and make the best use out of linked data. It is recognized as one of the key technologies of the semantic web due to its flexibility as well as ease of joining complex data structures and detecting intricate patterns in data. Its also the query language upon which the data.world platform is based. If you are new to SPARQL and would like to learn more, see our Getting Started with SPARQL tutorial to jump into the basics.\nWhenever possible, data.world translates your query into the proper syntax to run on the target system. There are cases, however, where a query cannot be fully translated. When that happens, part of the query will run natively on the target system, and part will run on data.world as an emulation. As a result, the total query processing time will frequently be greater than if the entire query ran on the target system.\nThe most up-to-date information about the support for aggregations and functions is in our dataset Function Support Matrix.\nIf you are comfortable with SQL you can write your own query to search for support for aggregations in a specific system:\nTables for the aggregation and function support for each system Is in the ReferenceAggregation and function support DEPREC section.",
    "url": "https://docs.data.world/en/95558-querying-data.html"
  },
  {
    "title": "Creating SQL queries",
    "content": "Best practices for optimizing your queries\nThe federation capabilities and the query optimizer available in the application make it possible to join tables from disparate data sources without having to worry about the source data formats. Use the following guidelines for joins to make your queries more efficient.\nBe careful when creating joins to ensure that all applicable join conditions are specified.\nUse inner joins rather than outer joins, where possible.\nBe wary of joins on calculated conditions, including especially joins which require casting.\nWhen possible, group together joined tables that are virtualized from the same sources.\nTo create a SQL query:\nStart creating a query from one of the following places in the application:\nOn the Project details page, on the Overview tab, click the New Query button.\nFrom the Project workspace, click the Add button and select SQL Query.\nOn the Datasets details page, on the Overview tab, click the Three dot menu next to a dataset file that allows query creation and click the Query option.\nThe query editor opens with the Project schema available on the right side for reference. If the project schema is not visible, click the Show sidebar button to open it. Use the Project schema available on the page to refer to more details about the available columns. Click the More info icon to view the details and click the column name or table name to copy it and paste in the Query editor.\nStart typing the query in the Query editor. As you type your query, the editor suggests SQL terms--operators, aggregations, and functions--for you. Press Enter to auto-complete the highlighted suggestion in the list and use the Up and Down arrows to choose another option.\nAs you type the Function names, you can hover over them to view the available help text for the SQL functions and aggregations.\nOnce you have written a query, it is best practice to format the query to make it more readable. Use the keyboard shortcut to auto-format the query - on Mac - command + option + L , on Windows - ctrl + alt + L).\nTo access the list of all available shortcuts, use the following shortcut - on Mac -cmd + Option + /, on Windows - ctrl + alt + /\nClick the\u00a0 Run query button to run the query. If the query runs successfully, the results are displayed. If an error occurs, the details of the error are displayed. For details, see troubleshooting query errors.\nClick the Save button to save the query. In the Save a copy of this SQL query window, set the following and click the Save button.\nName: Provide a name of the query.\nDescription: Provide a brief description.\nWhere will this query be saved: If you started creating a query from a project page, it shows the name of the project. If you started from a dataset page, you get to either save the query to a new project or to the dataset itself.\nSave queries to datasets if you want to clean up the data, join tables and preserve the lineage of the original tables for reference, or just use the query in multiple projects without having to rewrite it. Note that queries saved to a dataset do not show up in the queries list of any project the dataset is used in. Instead they display under the connected dataset when the dataset is opened in the project workspace.\nWho can see this query: Specify if the query is available to everyone who has access to the project or only the user creating the project. This option is only available while saving queries to projects.",
    "url": "https://docs.data.world/en/95559-creating-sql-queries.html"
  },
  {
    "title": "Troubleshooting query errors",
    "content": "To troubleshoot query errors:\nIf an error occurs while running a query, an error message is shown in context that points the issue and can help you solve the issue. For example, in the following query the error is in line 4. The value won is expected to be in double quotes.\nEvery time you run a query, query logs are also generated which capture all the stages the query execution goes through and any errors that are encountered. Click the View log link to browse to the query logs. If you need to email data.world support for help with the query, click the Contact support link in the window. A copy of the details of the query execution are automatically added to the email.",
    "url": "https://docs.data.world/en/95560-troubleshooting-query-errors.html"
  },
  {
    "title": "Working with query results",
    "content": "Once you have created a query, run it successfully, and you get the query results, you can do many things with the results to use them efficiently.\nDownload as CSV or XLSX files\nOpen with third-party apps\nSave to a dataset or project\nShare as a URL or embeddable code\nTo download a copy of the query results as CSV or XLSX files:\nIn the query results area, click the Download button and click one of the following options: Download as CSV or Download as XLSX. A copy of the results are saved on your machine.\nTo open results in third-party apps:\nIn the query results area, click the Open in app button.\nIf you want to use the built-in Chart Builder app, click the Open with Chart Builder option. When prompted, authorize the Chart Builder app to connect using your account. The next screen shows the Chart Builder. Use it to create charts from the query.\nIf you want to enable another integration, click the Add more integrations option. From the list of presented integrations, select the one you want to enable.\nWhen you access the query results area after this, the Open in app button now shows the list of enabled integrations, with an option to add more integrations.\nAnother option for accessing query results is through URLs and embedable code for markdown:\nDownload URL:  A link to download the latest results for the query in CSV format. This URL can be used in the application or can be shared outside the application as download link.\nEmbeddable markdown code: Likewise, you can generate code for embedding the results of the query on a page that supports markdown. For example, use this code to highlight the results in a summary of a project in the application. You can also use this embeddable code in any third-party apps that support markdown.\nNote: The link and markdown code can be configured to use the query as it existed at the time at which the link or code was generated. Or, they can be set to respect changes to the query over time and always use the latest version of the saved query.\nTo get the URL or embeddable code for query results:\nIn the Query results area, click the Download button and click the Copy URL or embed code option.\nIn the Share or embed this query window, do the following:\nEnable the Allow changes to the original query to update option if you want the updates to the query to automatically update the results for the shared URL or embeddable code. Note that the URLs in the next two fields change when you toggle this option.\nNext, copy the URLs to the download link or the embeddable code for markdown.\nClick Done to close the window.\nThe application gives you an option to save a copy of your results as tables to datasets and projects. While saying the tables you get to define how the data is updated in the saved tables. The main reasons you might want to save your query as a new data file are security and ease of analysis.\nTo save query results as tables to datasets and projects:\nIn the query results area, click the Download button and click the Save to dataset or project option. The Choose how data updates window opens.\nIn the Choose how data updates window, do the following if you want a live link for the query results:\nClick the Live view  option to ensure each query or preview is up to date with source data at all times. Data continues to live at its source. Any queries executed against this data will be translated and executed against the source dataset.\nIn the next window, provide a name for the file.\nSelect from the list of datasets and projects. You can also select the option to save to a dataset, which saves the query results to the new dataset and automatically links the new dataset to the project.\nIn the Choose how data updates window, do the following if you want a data extract of the query results:\nClick the Data extract  option if want the data to be pulled into data.world and processed into the internal representation. It can be set to update at specific intervals. Select this option when you want to cache results to a file for further analysis.\nIn the next window, enable the Maintain link to saved query  option if you want the modifications to the original query text will be reflected on next sync.\nProvide a name for the file.\nSelect from the list of datasets and projects. You can also select the option to save to a dataset, which saves the query results to the new dataset and automatically links the new dataset to the project.",
    "url": "https://docs.data.world/en/94913-working-with-query-results.html"
  },
  {
    "title": "Using query templates",
    "content": "A query template creates a graphical interface that simplifies running a query with different parameters. This allows an end-user to easily change the values that a query uses without needing to access or understand the underlying SQL or SPARQL code.\nAs an example, let us take a look at the Data Professional Salary Survey and examine the most popular databases from that survey in 2018. To do that, you could use the following SQL query.\nFor those with a bit of SQL knowledge, modifying this query to get the results for the year 2017 is trivial. One would just need to change the year value from 2018 to 2017. But when a query involves JOINs, UNIONs, nested queries, and aliases, the number of lines in the query could go from 5 lines to 50 or more and finding the desired value buried in the query becomes far more challenging.\nCreating a query template makes it easy for end users to access the information without worrying about editing a SQL or SPARQL query.",
    "url": "https://docs.data.world/en/94907-using-query-templates.html"
  },
  {
    "title": "Creating SQL query templates",
    "content": "The steps in this section are explained using a sample dataset and query. This example only uses one variable but you can extend your query to use multiple variables making your query templates more roburst.\nSteps include: Create a query, declare a variable, use the variable in the query, add comments to give instructions on using the template, and save the template.\nTo create a query template:\nCreate a new SQL query.\nClick the New template button in the query editor. It opens the Create a query template window with sample code. Click the Add a sample statement button to add the sample code in the query.\nModify the variable for use in the query or create your own variable. In this example, year is the variable name, INTEGER is the data type, and 2018 is the optional default value. If no default value is specified and the query is run without entering a value manually, the query will return an error. Allowed data types are: STRING, INTEGER, DECIMAL, BOOLEAN, DATE, TIME, and DATETIME.\nUse the variable in the query. Choose where the variable is to be used in the query. This will often be in the WHERE clause of the query , although it can also be used in ORDER BY, LIMIT, or other clauses. In our example, we'll use it in the WHERE clause.\nThis example is a bit complicated because the survey_year has the data type YEAR in the database, but we cannot declare a variable with that type. Instead, we declare our variable as a INTEGER and then used the CAST function to convert the value of survey_year from YEAR to INTEGER to make the comparison on Line 5.\nAdd comments above the Declare statement to provide instructions and present valid input values. Denote comments by adding the # or --  characters before the text. Comments added before the DECLARE statement display in the final query template form.\nClick the Preview query template button to view and use your query template.\nClick Save to save the query template in a project.\nAny time you want to make changes to the query or the template, click the Edit query link.",
    "url": "https://docs.data.world/en/95561-creating-sql-query-templates.html"
  },
  {
    "title": "Creating SPARQL query templates",
    "content": "Steps include: Create a query, declare a variable, use the variable in the query, add comments to give instructions on using the template, and save the template.\nTo create a template for SPARQL query:\nCreate a SPARQL query. Check out the SPARQL tutorial to learn about how to write SPARQL queries.\nDeclare a variable in the query. The SPARQL query contains atleast one variable by default. To turn that into a parameter that can be used in a query template, change the character in front of the variable name from\u00a0?\u00a0to\u00a0$. Make sure to perform this change for every occurrence of the variable within the query. Alternatively, declare a new variable by starting its name with with\u00a0 the $\u00a0character.\nIn our example, we will create a new variable\u00a0$year_of_interest\u00a0and replacing the value\u00a02018\u00a0in the\u00a0FILTER\u00a0function.\nTo add a default value for a variable, add a conditional\u00a0BIND\u00a0statement on the first line of the WHERE clause.\nThe general format of that statement is:\nBIND(IF(BOUND($variable), $variable, DEFAULT_VALUE) AS $variable)\nIn our example, add a new line on line 7:\nBIND(IF(BOUND($year_of_interest), $year_of_interest, 2018) AS $year_of_interest)\nAlthough this sets the default value, that value is not shown in the textbox in the query template form. Even though the value is not visible, the query will now use 2018 as the default year until another value is entered.\nSince the query template does not show any default values, it is recommended you add this information in the comment for the query so that users using the template are aware of it.\nAdd comments to the query by using the # character. The query template shows all comments before the first SPARQL statement (typically a PREFIX statement).\nClick the\u00a0Preview query template\u00a0link above to create the template.\nNote that the Preview query template link appears only when the SPARQL query is created properly with the variables required to create a template.\nClick Save to save the query template in a project.\nAny time you want to make changes to the query or the template, click the Edit query link.",
    "url": "https://docs.data.world/en/95562-creating-sparql-query-templates.html"
  },
  {
    "title": "API",
    "content": "Parameterized queries can also be made through the data.world public API. Note that non-parameterized queries can be executed with a GET request to the same endpoint, but in order to specify the parameters, a POST request must be made with the parameters specified in the body of the request.\nExample curl command:\ncurl -X POST \\\nhttps://api.data.world/v0/queries/${DW_QUERY_ID}/results \\\n-H 'Accept: application/json' \\\n-H 'Authorization: Bearer ${DW_API_TOKEN}' \\\n-H 'Content-Type: application/json' \\\n-d '{\n\"parameters\" : {\n\"parameterName1\" : \"parameter value 1\",\n\"parameterName2\" : \"parameter value 2\" }\n}'\nSee the API docs for more information.",
    "url": "https://docs.data.world/en/94911-api.html"
  },
  {
    "title": "Documenting your project",
    "content": "Once you have created a project and added your files to it, you can make it easier to find and more useful to others by describing, or documenting it. Documenting consists of creating the metadata for your dataset or project and helps others to trust your data and work.\u00a0 Searches on data.world also look at titles, descriptions, summary, and tags to match search strings so the more completely you describe your data the more chance it has of being found.\nTo document your project:\nWhile creating a project: add summary, tags, description to projects and queries\nAdd insights to projects\nSet the data dictionary. This includes:\nAdding labels to files and descriptions\nAdding details of columns",
    "url": "https://docs.data.world/en/95563-documenting-your-project.html"
  },
  {
    "title": "Adding file labels and descriptions",
    "content": "After you have added files to your project make sure you add descriptions and labels to the files. This makes the files more usable for the people using the projects.\nTo add labels and descriptions to files:\nOpen the Project workspace and browse to the file in the project.\nIn the About this file section, click the Edit button.\nIn the window that opens, on the File metadata tab, add a description and labels to the file.\nClick Save.",
    "url": "https://docs.data.world/en/95564-adding-file-labels-and-descriptions.html"
  },
  {
    "title": "Updating table columns",
    "content": "To update table columns information:\nOpen the project workspace and browse to the file in the project.\nIn the About this file section, click the Edit button.\nIn the window that opens, on the Column details tab, set the following:\nApply schema: If you know of a table that is already in the application that has similar columns, type the name in the field. A search is automatically run and shows the name of the matching table, if one is found. Click Save to save the column descriptions automatically added from the table.\nColumn name: Click the Rename column button next to the column name to change the name.\nType: From the dropdown field select the type of field.\nDescription: Add a description for the column.\nClick Save to save the changes.\nIf your data contains a common, general type of information that the application recognizes, the application suggests related information that you can use to enhance your data and research. As data is processed, the application analyze the data type and format of each field in an attempt to match it to any known data types in the system. When a potential match is discovered, it is indicated on the user interface and users can take action to add matched columns and related columns.\nTo add matched columns to a table:\nIn the Project workspace, browse to a table in your project.\nLook at the column headers in the table to locate a Green Triangle  indicator.\nClick the arrow in the field and click the Match this column option.\nIn the window that opens, you see a list of possible matching columns with an option to browse to the source of the column. Click the Add matched column button to add the column to the table. Once you do that, you are presented with a list of possible Related columns for the matched column. Click the Add related column button to add any related columns.\nNote that if a new version of the original file is uploaded (with additional rows of data, or additional columns), the file will automatically be matched using the same algorithm.\nThe matched and related columns are shown in the main table. Click the values of fields in these matched and related columns to view additional information about the fields.\nTo remove matched and related columns from the table, click the arrow in the field and click the Remove from table option.",
    "url": "https://docs.data.world/en/95565-updating-table-columns.html"
  },
  {
    "title": "Posting insights",
    "content": "When you have findings, conclusions, or interesting points for discussion about your project, the insights section is the place to do it. Insights allow you to capture the conclusions from your work, packaging them up in a way that quickly communicates a nugget of information, while giving the viewer the tools they need to dig down into your methods and sources. Insights balance efficiency of communication with reproducibility--two concepts that are often at odds in this phase of data work. You can use insights to capture the results and analysis of your work and synthesize them so they are understandable and accessible to stakeholders at all levels in the project. Insights can be created by the project owner and any contributors, and are the first thing that displays on the main overview page. They also have their own tab in the project.\nWays of adding insights (insights added from builder also get added as project files. Do not delete the file as it impacts the insight. When you delete the insight, the associated file does not get deleted.)\nThere are several places from where you can add an insight to your project:\nFrom the product workspace - Overview tab and Insights tab\nFrom Chart Builder, built-in visualization tool.\nTo add an insight:\nClick the Add insight button.\nThe New insight tab opens in the project workspace.\nUse the markdown or simple text editor mode to add the insight.\nClick Done.\nIn the Name your insight to save window, provide a name and description for the insight. Click the Save insight button.\nThe insight gets added in the project workspace. Click the Edit button to make changes to the insight. If you want to rename the insight, click the insight name or click the Rename option from the Three dot menu.\nTo start another insight from an existing insight, go the Version tab on the right. Click the Three dot menu for the version and you want to copy and click Make a copy.\nTo add an insight from the chart builder:\nCreate a chart in the Chart Builder.\nOnce you are ready to publish, click the Share button and select Insight.\nIn the Save as insight window, select the project where you want to save the insight, provide a Title and Description. Click Save.\nA confirmation message appears on the screen. Click the Open in new tab link to go to the Insight in the project where it is added. Click Close to close the window and go back to the chart builder.\nTo edit the insight from chart builder, click the link that is automatically added to the content. Make changes to the chart in the Chart Builder. Once done, repeat stet 2 and 3 to save the insight to the same project or a different project.\nTo delete an insight:\nBrowse to the insight you want to delete.\nClick the Three dot menu and select Delete.\nA message appears confirming the deletion. Click OK to complete the deletion process. The deleted insight is automatically removed from the bookmarks of users.\nUse the comments feature for insights to collaborate with users of your project. Even users who have View access to projects can use comments to provide inputs on insights without having the ability to make any changes to projects.\nTo comment on insights:\nBrowse to the insight you want to comment on.\nUse the comments box in the footer of the insight to start a discussion on the insight. Use markdown and mentions of users to enrich the experience of interacting.\nAs a person who added the comment, you can edit and delete your comments. Click the Three dot menu next to the comment to Edit or Delete a comment.\nViewers of the comments can report inappropriate comments. Click the Three dot menu next to the comment to Flag a comment.\nBookmark the insights that you find useful and would like to come back to later. Bookmarked insights can be managed from the Bookmarks space of the application.\nTo bookmark an insight:\nBrowse to the insight you want to bookmark.\nIn the footer of the Insight, click the Bookmark button. If the Insight is bookmarked by other users, it shows the bookmark icon with a number. If the Insight is bookmarked by you, click the button to unbookmark the insight.",
    "url": "https://docs.data.world/en/95485-posting-insights.html"
  },
  {
    "title": "Collaborating on projects",
    "content": "The\u00a0Discussion\u00a0tab captures all the communication between team members about the project. Either join an ongoing discussion or start a new one to collabrate with your team members on the project. You can also use this space to provide feedback on projects.\nTo collaborate on a project:\nBrowse to a project.\nGo to the Discussions tab and start a new discussion or participate in an ongoing one. You get an option to\u00a0Flag\u00a0or\u00a0Like\u00a0comments in a discussion. Flagging notifies a moderator of potentially inappropriate content in the post.\nWhile adding comments, you can use Markdown and add images and links. You can mention users (using\u00a0@user_name) to notify them about an ongoing discussion to get their attention and participation. Some important things to note:\nWhen you do this for\u00a0private resources\u00a0only available in an organization, typing\u00a0@\u00a0suggests only the members of that organization. It also shows a prompt to contact an administrator of the organization to ask them to allow others to have access to the organization. If you are an administrator, you get the option to invite other people to the organization.\nFor\u00a0community\u00a0resources, you get a list of suggested users from the community.\nIn both cases, only the users who have at least read access to the resource are notified. So make sure that when you are setting access for the users, you also give them access to the resource where they need to participate in the discussion.\nUse the request access feature to make contributions to projects that you can only view or discover.\nTo request access to projects:\nBrowse to a project you want to be able to contribute to.\nRequest access to it.\nFill in the details in the Request access window and press Submit.",
    "url": "https://docs.data.world/en/94917-collaborating-on-projects.html"
  },
  {
    "title": "Managing versions of projects",
    "content": "Every time a change is made to a project the application creates a record of it. For changes where files are added to a project, the system creates a downloadable version. This is most useful when users want to go back to old versions of files that were used in the projects.\nTo view project activity and download versions:\nOn the Project page, browse to the Activity tab.\nOn the All activities sub tab, you can see the stream of all activities for a project. For changes to objects like licences, tags, descriptions, etc. you can click the Show changes link to see the changes right there.\nFor changes that led to additions of files to projects, you also get an option to Download version from the Three dot menu against the change. If you just want to see the list of all versions that have a file available for downloading, go the Versions sub tab.",
    "url": "https://docs.data.world/en/94919-managing-versions-of-projects.html"
  },
  {
    "title": "Deleting projects",
    "content": "You need to have Manage permissions to be able to delete a project. Users with Admin permissions on an organization can also delete projects in an organization.\nImportant things to note:\nAll the files added in the project directly get deleted when you delete a project.\nAll the queries and query templates added to the project get deleted.\nThe datasets linked to the project are not deleted.\nTo delete a project:\nBrowse to the project you want to delete.\nOn the Settings tab, in the General section, click the Delete project button.\nA confirmation message appears. Click the Delete button to confirm the deletion.\nThe project is removed from the application and from all lists such as bookmarks, recent activity on the home page etc.",
    "url": "https://docs.data.world/en/94918-deleting-projects.html"
  },
  {
    "title": "Metrics and Auditing",
    "content": "",
    "url": "https://docs.data.world/en/114571-metrics-and-auditing.html"
  },
  {
    "title": "Metrics for the catalog",
    "content": "",
    "url": "https://docs.data.world/en/142580-metrics-for-the-catalog.html"
  },
  {
    "title": "About metrics",
    "content": "Metrics provide visibility and quantifiable measures to help you monitor the adoption of data.world, demonstrate return on investment, and track growing impact.\nMetrics help you answer questions such as:\nHow is the roll-out going?\nIs your organization getting what you intended out of data.world?\nHow are users joining?\nWhat organizations are they a part of?\nHow many user \u201cReal Active Days\u201d (RADs) per time period? (Note: RAD = 11 or more actions by a user in the platform in a given days.\nHow do I get Metrics for my organization? Check the\u00a0steps here\u00a0to get your metrics and use it.\nType\nDescription\nMost useful metrics\nEvents\nThese metrics capture event level details for interactions of users with different parts of the application. For example, you can find list of events that were generated when users created datasets and projects in the organization.\nEvents - Authorization Requests\nEvents - Dataset Activity - By Day\nMembership\nThese metrics provide details about members of your organization. You can find information like when a member joined the organization, when did they last login, and how many queries, datasets, projects they have created in the application.\nMembership - Current\nMembership - All Time List\nResources\nThese metrics provide insights for resource such as, organizations, datasets, projects. For example, you can find a list of queries created in the organization, and a list of datasets that are synced with external data sources.\nResources - Queries\nTops\nThese metrics capture information like the most searched terms, most viewed resources, and the most engaged uses with the application.\nTops - Most Searched Terms\nTops - Most Viewed Resources\nTops - Engagement\nVisits\nThese metrics capture details about how often the tool is used and by whom.\nVisits - Daily Adoption\nVisits - New Users By Month\nVisits - Request Rates By Month\nAll dates and timestamps are based on UTC time zone.\nA record in the data corresponding to any action executed by an application or integration that uses data.world's API.\ndata.world integrations\ndata.world API\nThe technical definition of a RAD (Real Active Day):\nmeasured at the level of individual users\nmust be a return day (first day on platform is excluded)\nui_events + api_events > 10\nWe use RADs as a measure of engagement. As compared with a regular user day, a RAD requires a slightly higher level of engagement (more than 10 distinct events). RADs purport to measure real active days by excluding user days with very low volume of interactions with the platform.\nA record in the data corresponding to any action executed by a human directly in the data.world app.\nUI Events occur when a person clicks buttons in data.world.\nSupport and solutions accounts refer to those owned by data.world team members that have been granted access to resources for purposes of support or solutions engineering.\nYou can identify such accounts with sufix@data.world email address. For example, jane.smith@data.world.\nMembership and Visits tables do not contain data.world support and solutions accounts.\ndata.world support and solutions accounts may appear in Events, Resources, and Tops tables.\nData in Events and Visits tables is within 24 hours of up-to-date.\nData in Membership, Resources and Tops is usually within 24 hours of up-to-date, but may be between 24 and 48 hours behind.\nOccasionally, table may fail to sync. A red dot on the table's icon will indicate that the most recent sync failed.\nAttempt to sync the table manually by opening the table and then clicking \"sync now\" where the table's metadata is displayed in the sidebar on the right.\nIf this does not resolve the problem, then please report in a support ticket.",
    "url": "https://docs.data.world/en/114572-metrics-and-auditing.html"
  },
  {
    "title": "Events",
    "content": "A record of instances when requests for authorization were submitted.\nColumn\nDescription\ndate\nThe date event occurred.\nrequester\nThe unique account ID for the user.\nrequester_email\nThe email address of the requester.\nrequester_displayname\nThe display name of the requester.\napprover\nThe agentid of the user who approved the request.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier. Together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource\nresourcetype\nThe type of approved resource.\nvisibility\nThe visibility of the resource. It can be PUBLIC or PRIVATE.\nlevel\nThe level of the granted authorization. It can be READ, WRITE or ADMIN.\nFind out the number of times an agent requested access to resources in the organization.\nSample query\nFind out the number of times requests are made to access a resource.\nSample query\nA record of bookmarks created.\nColumn\nDescription\ndate\nDate when the event occurred.\nagentid\nThe unique account ID for the user.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier. Together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nemail\nEmail address associated with the data.world account of the user.\nA fact table that captures the catalog pages activity such as views, edits, suggestions, and deletes done by users aggregated by UTC-based calendar day.\nThis table captures only the actions taken on the catalog resources from the application user interface.\nColumn\nDescription\ndate\nDate when the event occurred.\nowner\nThe owner of the resource.\nagentid\nThe unique account ID for the user who took an action on the resource, such as, viewed, edited, deleted, etc.\nresourcename\nThe name of the catalog resource.\nresource\nThe resource identifier. Together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourcetype\nThe type of resource. Example values are: Table, Collection, Business term.\nviews\nNumber of views for the resource by the unique user on the specific date.\nedits\nNumber of edits made to the resource by the unique user on the specific date.\ncreates\nThis field will show a value of 1 against the agentid that created the resource.\nsuggestions_submitted\nNumber of suggestions submitted on the resource by the unique user on the specific date.\ndeletes\nThis field will show a value of 1 against the agentid that deleted the resource.\nSee the activity for a particular type of resource. The following sample query is for resource type business terms.\nSample query\nSee total edits, views, and suggestions that were submitted by day for a specific resource. The following sample query is for resource name order\nSample query\nSee total activity summary for all catalog resource pages by date and owner.\nSample query\nFind the list of unique users logged in to different organizations.\nSample query\nA fact table that captures the details of approvals and denials on suggestions submitted by users on catalog resources.\nThis table captures only the actions taken on the catalog resources from the application user interface.\nColumn\nDescription\ndate\nThe date when the event occurred.\nowner\nThe owner of the created resource.\nresponder\nThe agentid of the user who denied or approved the submitted suggestion.\nrequester\nThe agentid of the user who submitted the suggestion.\nresource\nThe unique ID of the catalog resource. This represents the unique IRI value of the resource.\nresourcename\nThe name of the resource on which the suggestion was made.\nresourcetype\nThe type of catalog resource. Example values are: Table, Collection, Business term.\nsuggestions_approved\nThe value of 1 in this field means the suggestion is approved.\nsuggestions_denied\nThe value of 1 in this field means the suggestion is denied.\nA fact table that captures information about the suggestions on catalog resources that have not yet been approved or denied.\nThis table captures only the actions taken on the catalog resources from the application user interface.\nColumn\nDescription\ndate\nDate when the event occurred.\nowner\nThe owner of the resource.\nagentid\nThe agentid of the user who submitted the suggestion.\nresourcetype\nThe type of catalog resource. Example values are: Table, Collection, Business term.\nresource\nThe unique ID of the catalog resource. This represents the unique IRI value of the resource.\nresourcename\nThe name of the resource on which the suggestion was made.\nA record of events representing the creation of a dataset or project.\nColumn\nDescription\ndate\nDate when the event occurred.\nowner\nThe owner of the created resource. Owner can be either an individual or an organization. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nagentid\nThe user who created the dataset or project.\nvisibility\nThe visibility level assigned to the resource. Possible value are PRIVATE, OPEN, or DISCOVERABLE\ntype\nThe type. It can be DATASET or PROJECT.\nemail\nEmail address associated with the data.world account of the user.\nA fact table containing dataset activity measurements, aggregated by UTC-based calendar day.\nColumn\nDescription\ndate\nUTC-based calendar date.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nqueries_run\nCount of run query events executed within the dataset.\nqueries_saved\nCount of save query events executed within the dataset.\ndownloads\nCount of download events executed within the dataset.\npageviews\nCount of page views for the dataset homepage.\nbookmarks\nCount of bookmarks added for the dataset.\nauth_requests\nCount of authorization requests submitted for the dataset.\nA record of events representing the download of a dataset, file, or query result.\nColumn\nDescription\ndate\nThe UTC-based calendar date when download occurred.\nagentid\nThe unique account ID for the user.\nemail\nEmail address associated with the data.world account of the user.\ndisplayname\nThe name of the user.\ntype\nThe type of download: dataset, file, query-result, or error-linter (download error).\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nfilename\nName of the downloaded file or query-result.\nfilelabels\nArray containing any labels assigned to the file at the time of download event. Note that the most recent day's filelabels records may not be completely up-to-date if changes or additions were made to a file's labels a short time prior to the download event.\nA fact table containing metadata assets activity measurements aggregated by UTC-based calendar day.\nThis metrics is not available in Private Instance installations. You should instead look at the Events - Catalog resources pages activity - by day metrics to track activities such as views, edits, and suggestions submitted on metadata pages that are part of your data catalog.\nFor Single Tenant and Multi tenant customers, this metric will only have rows through September 2021. Starting October 2021, you can monitor activities on your metadata pages that are a part your catalog in the Events - Catalog resources pages activity - by day metric.\nColumn\nDescription\ndate\nUTC-based calendar date.\nowner\nThe organization namespace where metadata asset resides.\nresourcetype\nMetadata asset type.\nresource\nThe unique ID of the metadata asset.\nviews\nCount of pageviews for the metadata asset.\nedits\nCount of save edit events for the metadata asset.\nA record of pageview events, including viewer and resource viewed.\nColumn\nDescription\ndate\nDate when pageview occurred\nts\nTimestamp when pageview occurred.\nagentid\nThe unique account ID for the viewer.\ncustomer\nThe customer to which the viewer belongs.\nemail\nThe email address of the viewer.\naction\nPageview.\nurlpath\nThe URL path of the viewed resource. The path is the part of the URL that comes after \"https://data.world\"\nreferrer\nThe location (URL if available) from which a page view originated.\nA detailed log of query events occurring over the past 30 days.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nts\nTimestamp (UTC timezone).\nenvironment\nInternal use only.\nqueryrunagentid\nThe unique account ID for the user that executed query.\nqueryruntoken\nUnique query run ID.\naction\nThe status of the query. Possible values are:\nstart: Present when the querytype is SQL or SPARQL. Note that all SQL or SPARQL queries have an event where action = \u201cstart\u201d\nend: Present when the querytype is SQL or SPARQL. Queries that were terminated prior to completion by \u201ccancel\u201d or \u201cerror\u201d will not have an event where action = \u201cend\u201d\nexecute.live.[end, error, firstrow, start]: Events associated with virtualized data sources (live database connections).\nNULL: For internal queries or backed processing steps.\nresourceid\nThe unique resource identifier. It is concatenation of (owner/resource) and also used for the web URL to view the resource.\norigination\nThe part of the platform (api, query ui, preview, etc.) that triggered the query.\napi: Means it was kicked off by the public api.\nquery: Means it came from a query in the UI.\nping: Means it was part of a health check to make sure the service is working.\nnull: For internal queries or backed processing steps or related to virtualized tables.\nquerytype\nThe type of query. Possible values are:\nSPARQL: A query executed in the SPARQL query engine.\nSQL: A query executed in the SQL query engine.\nexternal: A query executed from a virtualized data source (live database connection).\nNULL: For internal queries or backed processing steps.\nquerytext\nOriginal text of the query. Should only be included on start, prepare.end, & end events.\ntargetquerytext\nContains the SQL that was sent to the downstream system after compilation from dwSQL (Snowflake, Athena, etc.)\ntotalquerytime\nTime, in milliseconds, for the query to execute (only available on end events).\ntotalresultcount\nTotal result count for the query (only available on end events).\neventid\nUnique identifier for the log event itself.\nA log of search events during past 90 days.\nColumn\nDescription\ndate\nCalendar data when event occurred.\nts\nDatetime of event in standard ISO 8601 format.\nagentid\nUser who executed search event.\naction\nType of search event.\nSearch Bar Result Click is when someone clicks an auto-suggestion that appears as someone is typing in the search bar in the top header.\nSearch Bar Submit is typing in the search bar and hitting enter in the header available on every page.\nSearch View Submit is typing in the search bar on the Search page itself (not in the header) and hitting enter.\nSearch View Search Results is being displayed a set of results on the search view.\nBoth Search Bar Submit and Search View Submit result in Search View Search Results being triggered. However there are other ways in our app someone can be sent to search results and trigger the Search View Search Results event, so the numbers won't line up exactly.\nsearch_value\nSearch terms the user entered. If a tag or filter was applied, this will show like tag:tag_name. For example, tag:orders.\nnum_results\nThe number of search results returned, if applicable, to the event action type.\nemail\nEmail address associated with the data.world account of the user.\nFind out how many times a keyword (in this example, Sales) was searched in the last 90 days.\nSample query\nFind the top search terms trends over time.\nSample query\nRanked list of number of dataset or project views by sub-org.\nColumn\nDescription\norg\nsub-org.\nviews\nCount of dataset or project views.",
    "url": "https://docs.data.world/en/114573-events.html"
  },
  {
    "title": "Memberships",
    "content": "A simple roster of all current and past members (includes organizations).\nColumn\nDescription\nagentid\nThe unique account ID for the visitor user.\nemail\nEmail address of the user. Value is shown as DELETED, if user no longer exists on data.world.\nonboard_date\nCalendar date when user completed the enrollment.\ndisplayname\nDisplay name of the user. Value is shown as DELETED, if user no longer exists on data.world.\ncurrent_member\nUser membership status.\nTRUE: The user account is currently and actively on record\nFALSE: The user account has been removed\nlast_date_active\nThe most recent date that the user was active on data.world.\nA granular listing of membership by date.\nColumn\nDescription\ncustomer\nUnique client ID.\ndate\nCalendar date of membership.\nagentid\nThe unique account ID for the user.\nemail\nEmail address associated with the data.world account of the user.\ndisplayname\nThe name of the user.\nA detailed roster of all current members, enriched with engagement summary data\nColumn\nDescription\ndisplayname\nThe name of the user.\nagentid\nThe ID of the user.\nemail\nEmail address associated with the data.world account of the user.\nstartdate\nCalendar date when the user first visited the platform.\nlastseen\nCalendar date of the most recent visit of the user.\nonboard_date\nCalendar date when user completed the enrollment.\napi_events\nNumber of API events executed by the user.\nui_events\nNumber of UI events executed by the user.\npageviews\nNumber of resources viewed by the user.\nqueries_created\nNumber of queries created by the user.\ndatasets_created\nNumber of datasets created by the user.\nprojects_created\nNumber of projects created by the user.\ndownloads\nNumber of times user downloaded a file or dataset.\nA granular listing of membership, broken down by organization.\nColumn\nDescription\nagentid\nThe unique account ID for the member.\ndisplayname\nThe name of the member.\nemail\nEmail address associated with the data.world account of the user.\norg\nThe organization that the user is a member of.\nauth_level\nThe org-level authorization granted to member (READ, WRITE or ADMIN)\nvisibility\nThe member's org-level visibility setting (PRIVATE or PUBLIC).\ndate_auth_updated\nThe date of most recent update to member's org-level authorization settings.\nstartdate\nCalendar date when the member first visited the platform.\nlastseen\nCalendar date of the member's most recent visit.\nDaily count of total number of members in customer organization.\nColumn\nDescription\ndate\nThe calendar date.\nmembers\nNumber of members in customer organization.\nA granular listing of membership, broken down by organization.\nThis table is only available in organizations in multi-tenant installations.\nColumn\nDescription\ndate\nThe calendar date.\ncustomer\nThe unique customer ID.\norg\nThe org that the count is grouped by.\nstartdate\nThe calendar date user first visited platform.\nlastseen\nThe calendar date of user's most recent visit.",
    "url": "https://docs.data.world/en/114574-memberships.html"
  },
  {
    "title": "Resources",
    "content": "A listing of contributor authorizations.\nColumn\nDescription\ndate\nThe calendar date when authorization was granted.\nagentid\nUnique ID of user who was granted authorization.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nresourcetype\nThe type of the resource.\nlevel\nThe level of authorization. Value options are ADMIN, READ, or WRITE\nvisibility\nThe visibility level applied to the resource.\napproverparty\nThe user account that approved the authorization.\nA log of all comment events that took place in customer organization.\nColumn\nDescription\ndate\nThe calendar date when the comment was posted.\nagentid\nThe unique account ID for the user that posted the comment.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nresourcetype\nType of resource in which comment was added (dataset, table, or datasetinsight).\nA detailed listing of all (currently existing) datasets.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nagentid\nThe unique account ID of the user that created the resource.\ndescription\nThe description associated with the dataset.\nsummary\nThe contents of the summary markdown field.\ncreatedby_agentid\nThe unique ID of the user that created the dataset.\ncreatedby_displayname\nThe name of user that created the dataset.\ncreatedby_email\nThe email address of user that created the dataset.\ncreateby_client\nThe client context in which the dataset was created. It can be used to differentiate datasets created via the web UI or API.\ncreated\nThe calendar date when the dataset was created.\nupdated\nThe calendar date when the dataset was last updated.\nvisibility\nThe access level the dataset/project is set to: PRIVATE, DISCOVERABLE, or OPEN.\ntags\nCurrent list of tags added to the dataset.\nnotificationsEmail\nEmail addresses associated with the notifications for the dataset.\nA detailed listing of all (currently existing) files residing in datasets\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\ndataset_name\nName of the dataset.\nfile_name\nName of the file.\nfile_created_time\nThe UTC date:time when the file was created.\nfile_created_date\nThe UTC date when the file was created.\nfile_createdby_client\nThe client used to create the dataset. It is useful for distinguishing API- from UI-based events.\nfile_materialized_or_virtualized\nTile type (MATERIALIZED or VIRTUALIZED).\nis_file_discoverable\nIs the mark as preview setting enabled for this file. Possible values are True or False.\nfile_size_in_bytes\nThe size of the file (in bytes).\nA listing of all existing database connections.\nColumn\nDescription\nowner\nThe organization that owns the database connection.\ndatabase_connection_label\nName given to the connection by creator of the connection.\ndatabase_connection_type\nType of database connection. (For example, Snowflake, PostgresSQL, etc.)\nconnection_agentid\nID of the user that created the connection.\nconnection_agent_displayname\nThe name of the user.\nconnection_agent_email\nEmail address associated with the data.world account of the user.\nconnection_created\nCalendar date when the connection was created\nconnection_createdby\nClient and agentid associated with connection.\nconnection_updated\nCalendar date when connection was last updated.\nconnection_updatedby\nUser that updated the connection.\nA summary table of dataset metrics by owner and datasetid.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nqueries_run\nCount of query runs.\nqueries_saved\nCount of queries saved.\ndownloads\nCount of downloads.\npageviews\nCount of number of times the dataset/project was viewed.\nbookmarks\nCount of number of times dataset/project was bookmarked.\nauth_requests\nCount of authorization requests placed on dataset/project.\nA list of followers of users or organizations.\nColumn\nDescription\nfollowing\nID of individual or organization that is followed.\nagentid\nID of individual follower.\ncreated\nCalendar date when the follow event occurred.\ndisplayname\nName of individual follower.\nemail\nEmail address associated with the data.world account of the user.\ncompany\nFollower's company as represented in the follower's profile.\nbio\nFollower's bio as represented in the follower's profile.\nA listing of all (currently existing) project insights.\nColumn\nDescription\ncreatedby\nAgentid of the user that created the insight.\nowner\nThe resource owner and the namespace where the resource resides -- together with the resource (owner/resource) this becomes a unique resourceID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\ncreated_date\nCalendar date when the insight was created.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nA long form series of counts of metadata assets created by date.\nColumn\nDescription\ndate\nUTC-based calendar day.\nasset\nType of metadata asset.\nn_created\nNumber of metadata assets created.\nFind the total count of actively available catalog metadata assets per asset type.\nSample query\nA listing of followers, broken down by organization.\nThis table is available only in organizations in mult-tenant installations.\nColumn\nDescription\nfollowing\nThe followed organization.\nagentid\nThe unique ID of the follower.\ndate_followed\nCalendar date of follow event.\nemail\nEmail address associated with the data.world account of the user.\ndisplayname\nThe name of the follower.\nA detailed list of all orgs owned by customer.\nColumn\nDescription\norgname\nID of the organization.\ndisplayname\nName of the organization.\ncreated\nCalendar date when the organization was created.\ncreatedby\nClient and agentid of the user that created the organization.\nupdated\nCalendar date when the organization was last updated.\nupdatedby\nuser that last updated the organization\nwebsite\nWebsite associated with the organization.\nbio\nBio associated with the organization\nallowedroles\nRoles authorized to the organization.\nA detailed list of all currently existing projects.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nthe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nagentid\nThe unique account ID for the user that created the resource.\ndescription\nThe description associated with the project.\nsummary\nThe contents of the summary field.\ncreatedby_agentid\nThe unique ID of the user that created the project.\ncreatedby_displayname\nThe name of user that created the project.\ncreatedby_email\nThe email address of user that created the project.\ncreateby_client\nThe client context in which the project was created \u2014 can be used to differentiate projects created via the web UI or API.\ncreated\nCalendar date when the project was created.\nupdated\nCalendar date when the project was last updated.\nvisibility\nThe access level of the project: PRIVATE, DISCOVERABLE or OPEN.\ntags\nCurrent list of tags added to the project.\nnotificationsEmail\nEmail addresses associated with the notifications for the project.\nA detailed list of currently existing queries.\nColumn\nDescription\ncreated\nThe calendar date when query was created.\nagentid\nUnique ID of user that created the query.\nresourcetype\nType of resource where query was created.\nquerytype\nType of query: SQL or SPARQL\nquery_name\nName given to the query.\nowner\nThe resource owner and the namespace where the resource resides -- together with the resource (owner/resource) this becomes a unique resourceID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource)$ and also used for the web URL to view the resource.\nA count of total number of datasets and projects as well as proportion that are set to open or discoverable.\nColumn\nDescription\nn_datasets_projects\nNumber of currently existing datasets or projects.\nn_open\nNumber of currently existing open datasets or projects.\nn_discoverable\nNumber of currently existing discoverable datasets or projects.\npct_open\nPercent of datasets or projects which are open.\npct_discoverable\nPercent of datasets or projects which are discoverable.\npct_open_or_disc\nPercent of datasets or projects which are open or discoverable.\nA detailed list of all datasets set to sync from source on a schedule.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource ID (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nconnection_agentid\nAgentid associated with the connection.\nconnection_created\nCalendar date when connection was created.\nconnection_updated\nCalendar date when connection was last updated.\nsyncstatus\nStatus of connection.\nlastsyncstart\nTimestamp of start of most recent sync.\nlastsyncfinish\nTimestamp of finish of most recent sync.\nnextscheduledsync\nTimestamp of next scheduled sync.\nnextscheduledsynctype\nType of next scheduled sync.",
    "url": "https://docs.data.world/en/114575-resources.html"
  },
  {
    "title": "Tops",
    "content": "A list of the top ten users ranked by number of bookmarks added (by the user).\nColumn\nDescription\nagentid\nID of the user that executed the action.\ndisplayname\nThe name of the user that executed the action.\nemail\nEmail address associated with the data.world account of the user.\nn\nNumber of bookmarks added.\nA list of users ranked by number of datasets created.\nColumn\nDescription\nagentid\nID of the user that executed the action.\ndisplayname\nThe name of the user that executed the action.\nemail\nEmail address associated with the data.world account of the user.\ndatasets_created\nNumber of datasets created.\nA list of users ranked by key engagement metrics.\nColumn\nDescription\nagentid\nID of the user that executed the action.\nemail\nEmail address associated with the data.world account of the user.\ndisplayname\nThe name of the user that executed the action.\nonboard_date\nDate when the data.world account was created for the user.\nrads\nCount of total all-time Real Active Days (RADs).\nui_events\nCount total all-time UI events (clicks in data.world app).\napi_events\nCount of total all_time events executed using an API client.\nfirst_seen\nDate of user's first activity in data.world.\nlast_seen\nDate of user's most recent activity in data.world.\nA list of datasets/projects, ranked by number of (currently existing) bookmarks.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource\nresourcetype\nType of resource that was bookmarked.\nbookmarks\nCount of currently existing bookmarks.\nA list of resources, ranked by number of times viewed.\nColumn\nDescription\nresourcetype\nType of resource where comments were posted.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resourceid) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\ncomments\nNumber of comments posted for the resource.\nA ranked list of the most commonly-searched-for terms along with a count of number of occurrences.\nColumn\nDescription\nsearch_value\nThe term or phrase that was searched.\nNote: a blank search_value indicates that the user pressed the space bar and then enter -- in other words, submitted a string comprising one or more spaces to the search bar.\nsearch_type\nThe type of search event. Possible values are:\nsearch bar full search: is counted when a search value is typed into the main search bar and enter is pressed.\nsearch results view: is counted when the search results from full search loads and is viewable to the user.\nsearch bar suggestion click: is counted when the user types in the main search bar and clicks directly on a suggestion surfaced in the drop down list\nlibrary view: is counted when a search value is provided and enter pressed in the \"Your datasets\" view.\nglossary view: is counted when a search value is provided and enter pressed in the Glossary view.\ndata catalog view: is counted when a search value is provided and enter pressed in the Resources view.\nanalysis hub view: is counted when a search value is provided and enter pressed in the \"Your projects\" view.\nbookmarks view: is counted when a search value is provided and enter pressed in the Bookmarks view.\nn\nThe number of times the search value and type is entered in the search bar.\nA list of resources, ranked by number of times viewed.\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier -- concatenation of (owner/resource) and also used for the web URL to view the resource.\nresource_type\nType of resource that was viewed: dataset_or_project, source, analysis or catalog\nviews\nCount of number of times the resource has been viewed.\nCounts of pageviews grouped by viewer and resource viewed.\nColumn\nDescription\nagentid\nUnique ID of the user who viewed the resource.\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nresource_type\nType of resource that was viewed:\ndataset_or_project, source, analysis or catalog\npageviews\nNumber of times the user viewed the resource.\nList of top resources, ranked by number of requests made (against the resource).\nColumn\nDescription\nowner\nThe resource owner and the namespace where the resource resides. Together with the resource (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresource\nThe resource identifier -- together with the resource owner (owner/resource) this becomes a unique ID, and also used for the web URL to view the resource.\nresourceid\nThe unique resource identifier. It is the concatenation of (owner/resource) and also used for the web URL to view the resource.\nresource_type\nThe type of resource.\nrequests\nNumber of requests submitted against the resource.",
    "url": "https://docs.data.world/en/114576-tops.html"
  },
  {
    "title": "Visits",
    "content": "",
    "url": "https://docs.data.world/en/114577-visits.html"
  },
  {
    "title": "Visits - Daily Adoption",
    "content": "A daily summary of number of users who used the platform with proportion which were new users.\nColumn\nDescription\ndate\nCalendar date being summarized.\nuser_days\nMembers of the account that are unique visitors that day (at least 1 pageview event).\nreturn_days\nMembers of the account that are unique return visitors that day (at least 1 pageview event) and this is not their first day (e.g. creation day).\nnew_days\ncount of users visiting within their first calendar day.\npercent_new\nnew_days / user_days * 100\nrads\ncount of Real Active Days.\npercent_rad\nrads / user_days * 100\nUseful queries\nFind the number of active user days by week, for the past year. An active user day is counted when a user has had 11 or more interactions with the application in one calendar day. active user day.",
    "url": "https://docs.data.world/en/114578-visits---daily-adoption.html"
  },
  {
    "title": "Visits - Average Visits Weekly",
    "content": "Measures the mean number of days (per week) that active users visited the platform. In other words, for those users who visited the platform at least once during the week, how many different days, on average, did the users visit?\nColumn\nDescription\nweek\nCalendar week being summarized.\nweekly_user_days\nNumber of user days per week.\nunique_visitors\nNumber of distinct visitors per week.\navg_visits_per_week\nAverage number of distinct days that users visited the platform (during the summarized week).",
    "url": "https://docs.data.world/en/114579-visits---average-visits-weekly.html"
  },
  {
    "title": "Visits - CRADs",
    "content": "Listing of Customer Real Active Days (CRAD) by date and agentid.\nColumn\nDescription\ndate\nCalendar date.\nagentid\nUnique ID of the user who performed CRAD.\nis_rad\nPossible values are True or False.\napi_events\nTotal number of API events recorded for that agent for that date. An API event is triggered when an action is executed by an application or integration that uses data.world's API. For example, data.world integrations or data.world API\nui_events\nTotal number of UI events recorded for that agent for that date. A UI event is triggered when a person clicks buttons in data.world\nCounts of Customer Real Active Days (CRAD) grouped by calendar date.\nColumn\nDescription\ndate\nCalendar date.\nrads\ncount of Real Active Days per date.\nSummary of CRAD metrics by various time groupings.\nThis table is available only in organizations in multi-tenant installations.\nColumn\nDescription\ncustomer\nUnique customer ID.\ntoday_total\nCRAD count today.\nyesterday_total\nCRAD count yesterday.\nday_before_yest_total\nCRAD count day before yesterday.\nlast_7day_total\nCRAD total past 7 days (not counting today).\ndays_8_to_14_total\nCRAD total week before last.\ndays_15_to_21_total:\nCRAD total two to three weeks ago.\nl90_total:\nCRAD total past 90 days.\nlast_7day_daily_avg\nCRAD average past 7 days (not counting today).\ndays_8_to_14_daily_avg\nCRAD average week before last.\ndays_15_to_21_daily_avg\nCRAD average two to three weeks ago.\nl90_daily_average\nCRAD average past 90 days.",
    "url": "https://docs.data.world/en/114580-visits---crads.html"
  },
  {
    "title": "Visits - New Users By Month",
    "content": "Count of new users, summarized by month.\nColumn\nDescription\nmonth\nCalendar month being summarized.\nnew_users\nNumber of new users during summarized month.",
    "url": "https://docs.data.world/en/114581-visits---new-users-by-month.html"
  },
  {
    "title": "Visits - Request Rates By Month",
    "content": "A by-month summary of ratio of distinct requesters and total distinct users.\nColumn\nDescription\nmonth\nThe month being summarized.\nunique_visitors\nNumber of distinct users during the month.\nunique_requesters\nNumber of distinct users that made a request for authorization during the month.\npct_made_request\nPercent of users that made a request during the month.\nNotes:\nThe unique_visitors column counts only visitors who are members of the customer's data.world account.\nThe unique_requesters column may count requests submitted by users outside of the customer's account if it's the case that a resource is set to be discoverable at the PUBLIC level. For this reason, it's possible for unique_requesters to be greater than unique_visitors.",
    "url": "https://docs.data.world/en/114582-visits---request-rates-by-month.html"
  },
  {
    "title": "Visits - Return visitors",
    "content": "A log of of all past user days, excluding new days (first day on platform).\nColumn\nDescription\ndate\nCalendar date of return day.\nagentid\nID of visiting user.\nemail\nEmail address of visiting user.\ndisplayname\nName of visiting user.\nA list of all users that have visited the platform on at least two separate calendar days.\nIncludes a count of total number of return days per user.\nColumn\nDescription\nagentid\nID of visiting user.\nemail\nEmail address of visiting user.\ndisplayname\nName of visiting user.\nreturn_days\nCount of active days (excluding user's first day on platform).\nA daily summary showing number of users that visited the platform (excludes first visit calendar day).\nColumn\nDescription\ndate\nThe calendar date being summarized.\nreturn_visitors\nNumber of users that visited platform (excludes first visit calendar day).\nA monthly summary showing number of users that visited the platform (excludes first visit calendar day).\nColumn\nDescription\nmonth\nThe calendar month being summarized.\nreturn_visitors\nNumber of users that visited platform (excludes first visit calendar day).\nA quarterly summary showing number of users that visited the platform (excludes first visit calendar day).\nColumn\nDesription\nquarter\nThe quarter being summarized.\nreturn_visitors\nNumber of users that visited platform (excludes first visit calendar day).\nTotal number of users that have visited the platform on at least two separate calendar days.\nColumn\nDescription\nreturn_visitors\nTotal number of users that have visited the platform on at least two separate calendar days.\nA weekly summary showing number of users that visited the platform (excludes first visit calendar day).\nColumn\nDescription\nweek\nThe calendar week being summarized.\nreturn_visitors\nNumber of users that visited platform (excludes first visit calendar day).\nShows count of all users that have visited the platform along with the proportion that visited again on a subsequent day.\nColumn\nDescription\nall_visitors\nTotal number of users that visited the platform.\nreturn_visitors\nTotal number of users that visited the platform on at least two separate calendar days.\npct_returned\nreturn_visitors / all_visitors * 100",
    "url": "https://docs.data.world/en/114583-visits---return-visitors.html"
  },
  {
    "title": "Visits - Unique visitors",
    "content": "A log of of all past user days.\nColumn\nDescription\ndate\nThe calendar date when the user visited the platform.\nagentid\nThe ID of the visiting user.\nemail\nThe email address of the visiting user.\ndisplayname\nThe name of the visiting user.\nA list of all users that have visited the platform. It includes a count of total number of return days per user.\nColumn\nDescription\nagentid\nID of the visiting user.\nemail\nThe email address of the visiting user.\ndisplayname\nThe name of the visiting user.\nactive_days\nThe count of days user was active on the platform.\nA daily summary showing number of users that visited the platform.\nColumn\nDescription\ndate\nThe calendar date being summarized.\nunique_visitors\nNumber of users that visited the platform.\nA monthly summary showing number of users that visited the platform.\nColumn\nDescription\nmonth\nThe calendar month being summarized.\nunique_visitors\nNumber of users that visited the platform.\nA quarterly summary showing number of users that visited the platform.\nColumn\nDescription\nquarter\nThe quarter being summarized.\nunique_visitors\nNumber of users that visited the platform.\nTotal number of users that have visited the platform.\nColumn\nDescription\ncount\nTotal number of users that have visited the platform.\nA weekly summary showing number of users that visited the platform.\nColumn\nDescription\nweek\nThe calendar week being summarized.\nunique_visitors\nNumber of distinct users that visited the platform.\nThe number of unique visitors that accessed the application. The results are grouped by week, for the past year.\nSample query\n",
    "url": "https://docs.data.world/en/114584-visits---unique-visitors.html"
  },
  {
    "title": "Audit events for the catalog",
    "content": "",
    "url": "https://docs.data.world/en/142581-audit-events-for-the-catalog.html"
  },
  {
    "title": "About audit events",
    "content": "This feature is only available for Private instance and Single Tenant instance installations.\nAudit events allow\u00a0administrators to monitor the actions performed by all the users in the data.world application through the UI or while using API. The audit log reporting functionality enhances the accountability of actions in the application. Administrators can track the actions taken by users in the application and find the root cause of issues by identifying the resources on which the action was performed and who performed the action.\nEvent types that are tracked include:\nResource creation\nResource deletion\nMetadata edits\nSuggest changeevents, such as change suggested, change rejected, change accepted, suggestion cancelled.\nAudit events are captured for the following resources:\nAll metadata catalog resources including but not limited to:\nCollections\nDatabase resources such as tables and columns\nAnalysis resources such as dashboards\nBusiness\u00a0glossary items\nAll\u00a0custom\u00a0resources defined in your metadata profile\nDatasets (Note that events are not captured for adding or removing files from datasets)\nProjects (Note that events are not captured for adding or removing Project Files, Project queries, Project Insights)\nThe audit events information is stored in two tables.\naudit_events\naudit_events_with_changes\nReconstruct the chronology of changes to catalog resources within your catalog. This can be useful for determining how often resources are added, removed, or updated in the catalog. Additionally, the audit event data can be used to rollback changes to a previous value in cases an undesirable change was applied.\nYou can also use the events data to understand how often, when, and by whom changes to catalog resources are suggested, what changes are proposed, and whether those suggestions are approved, rejected, or cancelled.\ndata.world can deliver the Audit Events information to you in two ways:\nA special dataset (baseplatformdata dataset) with the two audit events tables (audit_events, audit_events_with_changes) is automatically added to all single tenant and private instance installations.\nAdditionally, customers on single tenant and private instance installations can request the base platform data to be delivered to your Snowflake account using Private Marketplace Listing.\nTo be able to use this feature, your Snowflake account must be setup as Consumer of listings.\nThis table captures all parent events, for example, when the resource was created, updated, or deleted.\nColumn\nDescription\nAGENTID\nThe user responsible for triggering the event.\nENVIRONMENT\nThe data.world environment where the event occurred.\nEVENT_DATE_UTC\nThe date when the event was generated.\nEVENT_TIMESTAMP_UTC\nThe timestamp when the event was generated.\nEVENT_TYPE\nThe type of event, for example, resource.create, resource.delete, suggestion.create.\nID\nUnique identifier for the event.\nORG\nThe data.world organization to which this event applies.\nPREDICATES\nAn array of predicates to which this event applies.\nSITEID\nUnique data.world site ID associated with this event.\nSOURCE\nWhether the event was generated by interacting with the UI or via the API. Note that changes made through the base layer are not captured.\nSUGGESTION_APPROVER\nThe organization who can approve the suggested changes.\nSUGGESTION_COMPLETEDBY\nThe principal who approved or rejected the suggested change.\nSUGGESTION_ID\nUnique identifier for the suggestion object.\nSUGGESTION_REQUESTOR\nThe principal requesting that a change should be considered.\nTARGET_RESOURCE\nThe resource IRI of the resource impacted by this event.\nTYPE_IRIS\nAn array of associated semantic types for the target resource, that is, https://dwec.data.world/v0/Catalog\nCount of all resources created on a specified date.\nSample query\nFind all activities for a particular target resource ordered from newest to oldest (UTC timestamp).\nSample query\nFind suggestion requester, approvers, and target resources for all suggestions that occurred on a particular date and ordered by resource and requester.\nSample query\nThis table captures how often, when, and by whom changes to catalog resources are suggested, what changes are proposed, and whether those suggestions are approved, rejected, or cancelled.\nColumn\nDescription\nACTION\nThe action performed in an event. For example, add, remove, etc.\nAGENTID\nThe user responsible for triggering the event.\nCHANGESET_DATE_UTC\nDate of the event in the format yyyy-mm-dd\nCHANGESET_EVENT_ID\nThe event ID for the detailed child event.\nCHANGESET_PREDICATE\nThe target predicate or context for the change or event.\nCHANGESET_TIMESTAMP_UTC\nTimestamp of the event in the format yyyy-mm-dd HH:MM:SS\nCHANGE_SEQUENCE_NO\nSequence order in which the child events were generated for a given parent event.\nCHANGE_TYPE\nThe type of the child event. For example, changeset.applied, suggestion.propose, suggestion.reject, etc.\nCURRENT_VALUE\nThe value of the Updated field prior to applying the change event.\nDATA_TYPE\nThe data type of the field that was modified.\nENVIRONMENT\nThe data.world environment where the event occurred.\nEVENT_DATE_UTC\nThe date when the parent event was generated.\nEVENT_ID\nThe unique identifier for the parent event.\nEVENT_TIMESTAMP_UTC\nThe timestamp when the parent event was generated.\nEVENT_TYPE\nThe type of the parent event. For example, resource.create, resource.delete, suggestion.create, etc.\nFIELD_TYPE\nThe type of the field that was modified, that is, literal or uri\nLANGUAGE\nWhen provided, describes the language tag associated with the field being updated. For example, en=english, sp=spanish, etc.\nORG\nThe data.world organization to which this event applies.\nPREDICATES\nFor a given parent ID, an array of predicates to which this event applies.\nSITEID\nUnique data.world site ID associated with this event.\nSOURCE\nWhether the event was generated by interacting with the UI or via the API.\nTARGET_RESOURCE\nThe resource IRI of the resource impacted by this event.\nTYPE_IRIS\nAn array of associated semantic types for the target resource, i.e. https://dwec.data.world/v0/Catalog\nVALUE\nThe value of the Updated field after the event was generated.\nFind all change details for a particular target resource ordered from newest to oldest and by change sequence. Note that the change sequence captures the order in which an action occurs (for example, add or remove some value)\nSample query\nFind the proposed suggestions details for a particular date and ordered by target resource.\nSample query\nFind approved suggestions details for a particular target resource and predicate, ordered by timestamp descending and change sequence ascending.\nSample query",
    "url": "https://docs.data.world/en/142582-about-audit-events.html"
  },
  {
    "title": "Getting access to audit events tables through a dataset",
    "content": "Private instance and single tenant instance installations get the audit_events and audit_events_with_changes tables through a special baseplatformdata dataset.\nYou will see the events data captured for the last 90 days. If you have purchased additional add-ons, you may be entitled to the full audit history from the time users had started using the data.world application. Contact your Customer Success Director for more details.\nThe information in the tables in the dataset is automatically updated every 6 hours. Do not make any manual edits to this dataset as the changes will be overwritten with each automatic update of the dataset.\nIf you don't have access to the dataset or the tables in it, do the following to get access:\nContact data.world support team to get the special baseplatformdata dataset added to your organization.\nIn your request to the data.world support team, include the name and ID of the data.world organization where you want them to add the\u00a0baseplatformdata dataset.\nOnce the Support team informs you that the dataset is added to your organization, you can browse to the dataset to view the audit_events and audit_events_with_changes tables.",
    "url": "https://docs.data.world/en/142824-getting-access-to-audit-events-tables-through-a-dataset.html"
  },
  {
    "title": "Getting access to base platform data through a Snowflake Private Marketplace Listing",
    "content": "If you have a Snowflake account, data.world can deliver the base platform data (that includes the audit events tables information) directly to your Snowflake instance using Private Marketplace Listing.\nThe base platform data delivered to your Snowflake account includes a collection of tables and secure views containing data.world platform usage and governance data. It comprises of:\nPlatform records\nPlatform event logs\nPlatform audits logs\nDerived tables such as fact and dimension tables suitable for building dashboards\nFor this to work, the destination Snowflake account must be setup as Consumer of listings.\nThe audit events delivered through Private Marketplace Listing are automatically updated every 1 hour.\nYou will see the events data captured from the time users had started using the data.world application.\nTo get the base platform data delivered to your snowflake account:\nContact data.world support to get this delivery method setup for your data.world installation.\nFrom the Snowflake console, get the account ID, URL, and Region and provide in your request to the support team. Also include the name and ID of the data.world organization for which you want the audit events.\nOnce the data.world Support team informs you that the Private Marketplace Listing is set up, a Snowflake user with ACCOUNTADMIN privileges can view and accept the shared listing.\u00a0 The listing will appear in the Snowsight Data > Private Sharing menu.\nClick the Private Listing entry and then click Get.\u00a0 Accepting the listing creates a virtualized database representing the data.world usage data in this Snowflake account. You can now view and query the audit data just as you would view any other data in Snowflake.\nGrant privileges to other users and roles for the data to be visible to other users in this Snowflake account.",
    "url": "https://docs.data.world/en/142825-getting-access-to-base-platform-data-through-a-snowflake-private-marketplace-listing.html"
  },
  {
    "title": "Security",
    "content": "Security is a paramount concern for enterprise customers who are evaluating new systems. The data.world service is not just cloud-first, but it's also security-first. We\u2019ve designed it from the ground up to ensure that we can support a unique combination of internal and external compliance needs. In the following articles we have documented how we keep both your data and your connections safe, and how we enable you to do the same.",
    "url": "https://docs.data.world/en/98530-security.html"
  },
  {
    "title": "How our data connections interact with your data",
    "content": "There are three main ways that data.world can interact with data from your environment:\nMetadata collection, including optional Technical Lineage Server\nVirtual query capability\nData extract/import capability\nTypically for the vast majority of security and compliance needs, (1) Metadata collection and (2) virtual query are sufficient, and (3) data extract/import can be optionally disabled. If only (1) and (2) are leveraged, data.world does NOT store data in our platform--only metadata.",
    "url": "https://docs.data.world/en/98532-how-our-data-connections-interact-with-your-data.html"
  },
  {
    "title": "Metadata collection, including optional Technical Lineage Server",
    "content": "Docker microservices (or Java application in the case of the Technical Lineage Server) are deployed in the customer environment by the customer. These services only transmit metadata to data.world:\nSource system descriptive information\nSchema information such as tables and columns\nObject-oriented descriptive information such as the titles/names of dashboards or reports and when they were created\nTransmission of this information is over HTTPS with optional custom SSL certificate support. This information is non-sensitive in nature, and can be managed by data stewards, data governance, and data security professionals in your organization for further curation and access control.",
    "url": "https://docs.data.world/en/98533-metadata-collection,-including-optional-technical-lineage-server.html"
  },
  {
    "title": "Data extract/import",
    "content": "Virtual connections can be leveraged to extract/import data into data.world. Any data imported into data.world will often benefit from query performance improvements if it is in a small or medium size dataset (i.e., under 3GB). We recommend larger amounts of data be separated into multiple datasets, or accessed via virtualization instead. Data in data.world datasets is kept securely in encrypted data stores.\nSome customers choose to disable the ability to extract and import data to prevent any data from being persisted in data.world for compliance or other reasons. The decision to allow or disallow extract/import is fully in the hands of the customer.",
    "url": "https://docs.data.world/en/98534-data-extract-import.html"
  },
  {
    "title": "Virtual query",
    "content": "Datasets in data.world are built off of the connections, but no data is stored in them. When a user performs a data query, a short-lived connection fetches only the query results. These results are NOT persisted--neither in storage nor in memory. Additionally, by default, queries have a 5 minute timeout for performance and security reasons. Finally, all queries are comprehensively tracked and audited via query audit logs that customers can access and monitor.\nA virtual appliance (or optionally a hardware appliance or reverse SSH tunnel service) is implemented in the customer environment by the customer. The virtual and hardware appliances are powered by our partner Trustgrid, a secure connection bridge technology vendor trusted by many of the largest banks and financial institutions. Using the Trustgrid technology, a network bridge is configured on the data.world side in conjunction with the customer. At that point, read-only system user credentials can be securely stored in data.world as connections, and encrypted connections are initiated as outbound only. Connections in the system are encrypted and initiated as outbound only.",
    "url": "https://docs.data.world/en/98535-virtual-query.html"
  },
  {
    "title": "Connection security",
    "content": "There are a variety of connection solutions available for configuring data.world access to remote data sources, and there are several factors to consider when determining the best way to connect. Some connection types are simpler to set up, while others offer enhanced security characteristics but require the involvement of internal IT to deploy software and/or hardware. The connection types available are:\nDirect connection (inbound)\nSSH tunnel (inbound, preferred)\nBridged connection appliance (outbound)",
    "url": "https://docs.data.world/en/98541-connection-security.html"
  },
  {
    "title": "Direct connection (inbound)",
    "content": "When a database is accessible on the open internet from a known hostname or IP, you can simply connect to it directly from data.world. You can configure the connection either with the Connection manager, or through the Integration gallery. This solution is perfect for online or SAAS products like Snowflake or when there is a desire to test virtualization capabilities.\nWhen a database is located inside of an organization's network, this is not the preferred solution. Opening database servers to the open web can pose a security risk and should be avoided. If you are considering this solution, you should block traffic from all hosts except for those from a specified allowlist.Allowlist for data.world",
    "url": "https://docs.data.world/en/98542-direct-connection--inbound-.html"
  },
  {
    "title": "SSH tunnel (inbound, preferred)",
    "content": "When connecting to a database server, data.world allows you to optionally configure an SSH tunnel to connect through. This solution requires some setup on the part of the network administrator, but is more secure than a direct connection.\nThis is the preferred method for connecting database servers to data.world. It is easy, flexible and secure. No additional hardware is required beyond a bastion server. Many organizations have these deployed as a normal part of their infrastructure.\nFor additional security, data.world provides user specific SSH public keys which should be configured on the bastion server (in .authorized_keys) to ensure traffic is from data.world.",
    "url": "https://docs.data.world/en/98543-ssh-tunnel--inbound,-preferred-.html"
  },
  {
    "title": "Bridged connection appliance (outbound)",
    "content": "With some organizations, any incoming connection at all is considered insecure. This is especially true in industries which deal with health or financial data. In these cases, consider an alternative architecture which does not require the organization to accept connections from the open web.\nA bridged connection involves deploying an appliance inside of the organization's network\u2014where the target database servers reside. The appliance makes outbound connections at startup, and maintains these connections over time.\ndata.world has partnered with Trustgrid (http://trustgrid.io) to provide this capability. While Trustgrid helps configure and maintain the connection to the data.world network, data is visible only to data.world. No data enters Trustgrid's network.\nThis solution requires a commitment of time and resources on the part of the organization's IT department. They must work with data.world to deploy and configure the appliance. Ongoing maintenance of the appliance should be minimal\u2014 requiring time only in exceptional cases.\nThe appliance runs inside the organization's network and makes outbound connections to:\ndata.world \"Data Plane\" \u2013 data transfer only\nTrustgrid \"Control Plane\" \u2013 configuration only\nThe target database instance(s)\nThere are no inbound connections to the organization from the open internet with this solution.",
    "url": "https://docs.data.world/en/98544-bridged-connection-appliance--outbound-.html"
  },
  {
    "title": "Appliance requirements for bridged connections",
    "content": "A hardware or virtual appliance may be deployed. If you meet minimum requirements outlined below, virtual appliances are preferred, as hardware deployments require additional time and physical (rack) space.\nHardware: 2vCPU, 4GBRAM, 32GB Storage (supports 250Mbps of throughput)\nHypervisor/Virtualization: VMware vSphere5.5 (or greater), Amazon Web Services, Microsoft Azure Cloud, and Google Cloud Platform\nTrustgrid appliances are stateless network devices. They require specific vSphere configuration settings to maximize uptime:\nSet the DRS level for a VM to either PartiallyAutomated or Disabled\nCreate an anti\u2010affinity rule\nDo not backup Trustgrid virtual appliances\nDeploy Trustgrid secondary high-availability appliances on separate physical hosts\nHigh availability deployments require two instances to be deployed and running.",
    "url": "https://docs.data.world/en/98545-appliance-requirements-for-bridged-connections.html"
  },
  {
    "title": "Integrations and security",
    "content": "Integrations in data.world are primarily used either to bring data in for querying or for metadata analysis, or to take data out and work with it in a third-party application. Security for both types is comprehensive. In structure, integrations are stored as datasets in data.world.",
    "url": "https://docs.data.world/en/98546-integrations-and-security.html"
  },
  {
    "title": "Integration access",
    "content": "At the core, every integration uses some form of a user token to ensure that users only have access to the data that they should have access to. In the case of integrations used to download data into third-party applications, this token can be created via an OAuth flow or may involve the user copy/pasting a token that they copied from their advanced settings page.\nIn the case of database integrations, where we connect out to the system, we use the credentials entered by the Data Engineer. For Athena, the data engineer must also configure their AWS instance to allow us to connect.",
    "url": "https://docs.data.world/en/98547-integration-access.html"
  },
  {
    "title": "Integration visibility and permissions",
    "content": "The integrations available to users are presented on an integrations web page:\nOrganizations in the multi-tenant environment have all the publicly available integrations as well as any private integrations that they have specifically created for their organizations. Access to private integrations can be set by an organization admin.\nOrganizations currently using a VPC have implementations that come default with NO integrations on their integrations page. This configuration enables them to set visibility and access very flexibly. All permission levels and access can be set within the platform by the data administrators in the organization.",
    "url": "https://docs.data.world/en/98548-integration-visibility-and-permissions.html"
  },
  {
    "title": "Integration architecture",
    "content": "data.world is a cloud-first, SaaS solution. However some use cases and capabilities either can be or must be implemented in your own compute environment, dependent on security or infrastructure operations requirements.\nThis document outlines the architecture design of those customer hosted components, and how to implement them.",
    "url": "https://docs.data.world/en/98549-integration-architecture.html"
  },
  {
    "title": "Servers and permissions",
    "content": "Metadata collectors can be run on the same server. If you do so, ensure you have enough combined compute, storage, and networking resources to meet peak usage based on your scheduling. For example, if you plan to schedule 10 complex collection tasks to run simultaneously from the same instance, it is recommended that you increase available resources 3-5X, and monitor the resource usage on that instance for further optimization.\nWe recommend the data.world Bridge and technical lineage server be implemented on separate instances from metadata collectors to minimize resource interference and maximize network security control. Specifically, MANTA recommends, for the technical lineage server, that a \u201cdedicated machine is recommended for MANTA to avoid collision for resources and limit access of MANTA to other data and applications for security reasons.\u201d\nMake sure the services have the proper permissions to allow network connectivity:\nMetadata collectors must be able to connect to the source systems like databases and BI tools that you intend to collect metadata from.\nThe metadata collector you are using for lineage collection must be able to connect to the technical lineage server (MANTA) in order to pass some lineage information to data.world.\nIf you intend to automate the upload of metadata to data.world, metadata collectors must have permission to do an outbound connection to api.data.world via HTTPS on port 443.\nThe technical lineage server also must be able to connect to the source systems like databases and BI tools that you intend to collect metadata from.\nThe technical lineage server does not need to have outbound internet access, unless you intend to make the technical lineage server UI available via the public internet. However the technical lineage server does have it\u2019s own UI, which your technical users will benefit from being able to use directly. The UI port should be made accessible for others in your enterprise to reach, and optionally it supports LDAP/AD integration for simplified, consolidated user administration.\nThe data.world Bridge can be a hardware appliance, but most often is leveraged as a virtual appliance. These appliances are provided by data.world. The data.world Bridge does not need to be able to connect with the on-premise metadata collectors nor the technical lineage server. It is only focused on secure brokering of data.world hosted metadata collectors (noted in the data.world Collector hosted by data.world below) and data.world data virtualization / federated query capabilities.\nMore details are available in the supplemental technical lineage server (Manta) and data.world Bridge documentation articles.",
    "url": "https://docs.data.world/en/98550-servers-and-permissions.html"
  },
  {
    "title": "Metadata collectors",
    "content": "Metadata collectors are used to connect to source systems such as databases and BI tools, and collect and generate useful information that should be in the data.world catalog. The most commonly used one that we provide is called the data.world Collector.\nIf the technical lineage server (Manta) is in scope of the customer solution, the data.world Collector is used to collect metadata from Manta and pass it to data.world. This is the method in which data.world receives information from Manta (Manta does not connect to data.world directly).",
    "url": "https://docs.data.world/en/98551-metadata-collectors.html"
  },
  {
    "title": "The data.world Collector hosted by data.world",
    "content": "By default, the data.world Collector is run in the cloud, fully managed by data.world as part of the Connection Manager using tasks. If you have the data.world Bridge set up, it will leverage your bridge connection just like the data virtualization / federated query engine can. See our article on tasks for more details.",
    "url": "https://docs.data.world/en/98552-the-data-world-collector-hosted-by-data-world.html"
  },
  {
    "title": "The data.world Collector self-hosted by the customer",
    "content": "However, for security or infrastructure operations reasons, you may opt to use the data.world Collector in your on-premise compute environment behind your own firewall. By doing so keep in mind that you cannot use the Connection Manager UI -- rather instead you will leverage the command line developer user experience for the Collector, which is documented in our data.world Collector documentation.\nThe data.world Collector ships as a Docker image which can be loaded and run with a series of command line (CLI) options. It outputs a file with the extension *.dwec.ttl that you can upload to data.world manually, or you can have the catalog collector upload it automatically using an API token.",
    "url": "https://docs.data.world/en/98553-the-data-world-collector-self-hosted-by-the-customer.html"
  },
  {
    "title": "Technical lineage server MANTA",
    "content": "The technical lineage server, powered by MANTA, connects to source systems like databases, ETL tools, and BI tools, analyzing SQL and other code and configuration assets where applicable, and generates holistic, cross-system technical data lineage. The technical lineage server is always self-hosted by the customer, similar to self-hosted data.world Collector.\nThis article provides a high-level summary of prerequisite information. For official information and additional details, see the supplemental documentation \u201cMANTA Flow Installation and Usage Manual\u201d.\nTechnical lineage server (MANTA), also known as MANTA Flow, is made up of three major components:\nMANTA Flow CLI\u2014Java command line application that extracts all scripts from source databases and repositories, analyzes them, sends all gathered metadata to the MANTA Flow Server, and optionally, processes and uploads the generated export to a target metadata database\nMANTA Flow Server\u2014Java server application that stores all gathered metadata inside its metadata repository, transforms it to a form suitable for import to a target metadata database, and provides it to its visualization or third-party applications via API\nMANTA Admin UI (runs on the MANTA Flow Service Utility application server)\u2014Java server application providing a graphical and programming interface for installation, configuration, updating, and overall maintenance of MANTA Flow (detailed documentation can be found on the page MANTA Admin UI)\nA dedicated machine is recommended for MANTA to avoid collision for resources and limit access of MANTA to other data and applications for security reasons.",
    "url": "https://docs.data.world/en/98554-technical-lineage-server-manta.html"
  },
  {
    "title": "Minimum Configuration",
    "content": "CPU - 4 cores at 2.5 GHz\nRAM - 12 GB\nHDD - 500 MB for MANTA installation + 50 GB space for metadata; minimum 150 IOPS",
    "url": "https://docs.data.world/en/98556-minimum-configuration.html"
  },
  {
    "title": "Recommended Configuration",
    "content": "CPU - 8 cores at 3 GHz\nRAM - 24\u2013256 GB\nHDD - 500 MB for MANTA installation + 100\u2013300 GB space for metadata; SSD, minimum 2000 IOPS",
    "url": "https://docs.data.world/en/98558-recommended-configuration.html"
  },
  {
    "title": "Software Requirements",
    "content": "OS- Windows 7/Server 2008 or newer, Linux or Solaris, Mac (without installer)\nJava - JRE (Java Runtime Environment) version 8 (update 151) or higher (for a 64-bit architecture) is a prerequisite for MANTA. JRE is NOT part of the MANTA package. It is the sole responsibility of the customer to obtain JRE. The versions tested by MANTA are OpenJDK and Oracle. Each distributor has its own licence conditions, and it is up to the customer to fulfill those conditions; MANTA is not in any way responsible for the customer \u0301s compliance with the required conditions.\nUser Access Requirements - Dedicated OS user with limited privileges under which MANTA will run. This is not required but highly recommended to limit access of MANTA to other data and application for security reasons.\nJava Installation Instructions - Every MANTA product requires Java installation, specifically Java Runtime Environment (JRE). JRE version 8 update 151 or higher (for a 64-bit architecture) is a prerequisite for MANTA.",
    "url": "https://docs.data.world/en/98559-software-requirements.html"
  },
  {
    "title": "Installing the technical lineage server and configuring and running your first lineage analysis",
    "content": "For detailed instructions on how to install the technical lineage server (MANTA) and run and schedule your lineage analysis, see the supplemental documentation on the MANTA portal. You will need to create a login in order to access these documents.",
    "url": "https://docs.data.world/en/98560-installing-the-technical-lineage-server-and-configuring-and-running-your-first-lineage-analysis.html"
  },
  {
    "title": "data.world bridge",
    "content": "The data.world Bridge provides a customer-hosted connectivity hub where connections are outbound-only, short-lived, and highly secure. It is powered by Trustgrid, and trusted by banks, healthcare companies, and many other organizations with very strict security and compliance needs.\nCustomer-hosted metadata collectors and the technical lineage server do not require inbound connectivity, however data virtualization / federated query and data.world-hosted metadata collection does. The data.world Bridge enables a secure, outbound-only method for brokering those connections.\nThe data.world Bridge is provided by data.world, and can be implemented as a hardware appliance or a software virtual appliance (image support for AWS, vSphere, Microsoft Azure, and Google Cloud Platform).",
    "url": "https://docs.data.world/en/98561-data-world-bridge.html"
  },
  {
    "title": "High-level architecture",
    "content": "data.world handles the Application, Gateway Node, and Cloud Management aspects. Customer responsibility is only the Edge Node(s) and connection to the Database (which represents the databases, BI tools, etc. that is desired to connect to data.world).\nWe recommend a high availability (HA) implementation, and will provide up to 2 nodes for customers if they are using the data.world Bridge.",
    "url": "https://docs.data.world/en/98562-high-level-architecture.html"
  },
  {
    "title": "Network",
    "content": "If you firewall outbound traffic to the internet, or if there will be a firewall between the nodes and your servers, please be sure that the necessary firewall rules are in place to allow traffic from the data.world Bridge. Please contact support with any questions regarding the necessary firewall rules.\nIf your environment involves multiple VLANs, prepare the desired switch ports to allow access to the correct VLAN.\nStandard (one node)\n1 Public IP Address\n1 Private IP Address\n2 DNS Servers\nHA (two nodes)\n2 Public IP Addresses\n2 Private IP Addresses\n1 Private IP Address for Cluster IP\n2 DNS Servers",
    "url": "https://docs.data.world/en/98563-network.html"
  },
  {
    "title": "Recommended Software Requirements",
    "content": "4 vCPU (or equivalent)\n4 GB RAM\n30 GB disk space",
    "url": "https://docs.data.world/en/98564-recommended-software-requirements.html"
  },
  {
    "title": "Installing the data.world bridge",
    "content": "For instructions on how to install the data.world Bridge, see this documentation. The complete trustgrid documentation, is available here.",
    "url": "https://docs.data.world/en/98565-installing-the-data-world-bridge.html"
  },
  {
    "title": "Enhanced security and privacy features",
    "content": "In addition to requiring a single sign-on (SSO) to access your organization\u2019s data, organizations on data.world can have additional enhanced security and privacy features enabled to prevent the accidental sharing of confidential information. In this article we\u2019ll cover:\nEnhanced security and privacy features\nHow to get the features enabled or disabled\nSupport implications",
    "url": "https://docs.data.world/en/98566-enhanced-security-and-privacy-features.html"
  },
  {
    "title": "Features",
    "content": "The following features are available to be enabled per organization on data.world:\nDisable Open Datasets and Projects - This feature disables the option to make a resource on data.world \u201cOpen\u201d for our wider community. The option will not be presented as a setting for any resource, and the default privacy will be \u201cPrivate\u201d for all resources.\nDisable Guest Authorizations - Requires any contributor to a dataset or project to be an existing member of your organization. This prevents a dataset admin from adding the wrong \u201cJohn Smith\u201d to a resource, for example. Combined with SSO, this feature makes it so only proven organization members can receive invites to resources.\nDisable Share URLs - Removes the ability to create \u201cShare URLs\u201d of resources or data on data.world. Typically these URLs are used for the easy sharing of data without requiring the recipient of a share URL to be logged into data.world.\nDisable Organization Member Invites - Requires that all organization members be provisioned through SSO. This feature disables the ability to invite community members directly to your organization. Any and all organization members would have to join the organization through your SSO provider, such as clicking on the application link or tile in most SSO provider\u2019s home pages, like Okta or Azure ADFS.",
    "url": "https://docs.data.world/en/98567-features.html"
  },
  {
    "title": "How to get features enabled or disabled",
    "content": "Email support@data.world to enable any of the above listed features for your organization.",
    "url": "https://docs.data.world/en/98568-how-to-get-features-enabled-or-disabled.html"
  },
  {
    "title": "Support implications",
    "content": "Enabling any or all of these features can change the typical experience of using data.world. For example, Disable Guest Authorizations will prevent our support and solutions staff from having direct access to a resource, and Disable Open Datasets and Projects will prevent you from publishing an Open dataset on data.world under your organization\u2019s namespace, even if the data was meant to be Open.",
    "url": "https://docs.data.world/en/98569-support-implications.html"
  },
  {
    "title": "Security best practices",
    "content": "There are several best practices you can follow to improve the security of your data and manage access to it on data.world.",
    "url": "https://docs.data.world/en/98570-security-best-practices.html"
  },
  {
    "title": "Use organization-owned connections",
    "content": "The Connection Manager on your organization page allows for connections to be managed by only organization administrators. All database and dataset connections are audited and reportable.",
    "url": "https://docs.data.world/en/98571-use-organization-owned-connections.html"
  },
  {
    "title": "Leverage identity integration",
    "content": "Integrate with your identity management system, such as Okta or Ping. data.world supports pre-provisioned accounts with SAML authentication, or just-in-time (JIT) SAML provisioning. Your identity management system will provide you the ability to manage token expiration, password policy, multi-factor authentication, conditional access restrictions, and more in conjunction with your data.world solution.",
    "url": "https://docs.data.world/en/98572-leverage-identity-integration.html"
  },
  {
    "title": "SAML",
    "content": "Use an SSO application with your provider to authorize access to your organization's data.",
    "url": "https://docs.data.world/en/98573-saml.html"
  },
  {
    "title": "Turn off organization visibility",
    "content": "By default, organizations in a VPC environment do not show up in a list of data.world organizations. However this feature is also availbe for multi-tenant clients. It is possible to configure any organization so that it does not show up in a publicly visible list of data.world organizations.",
    "url": "https://docs.data.world/en/98574-turn-off-organization-visibility.html"
  },
  {
    "title": "Never share keys or tokens",
    "content": "Some third party applications may require an API token or key to work with data.world. If you have such a key or token, or one for data.world's metadata catalog collector, you should never share them with anyone else. These tokens run as your user with your permission levels. Every user who needs an API token should have their own for security and accountability.",
    "url": "https://docs.data.world/en/98575-never-share-keys-or-tokens.html"
  },
  {
    "title": "Upload restrictions",
    "content": "Uploads can be restricted, including to 0GB (uploads disabled) to prevent data being manually added to the platform by users.",
    "url": "https://docs.data.world/en/98576-upload-restrictions.html"
  },
  {
    "title": "Provide masked/limited file previews on discoverable datasets",
    "content": "Often for evaluating data you want users to understand not only the column names and other descriptive metadata, but also some example rows. Masking/limitations applied to samples allow for them to be provided in a way that effectively works within sensitive data or compliance needs.",
    "url": "https://docs.data.world/en/98577-provide-masked-limited-file-previews-on-discoverable-datasets.html"
  },
  {
    "title": "Reference",
    "content": "",
    "url": "https://docs.data.world/en/99113-reference.html"
  },
  {
    "title": "Account removal due to terms violations - FAQ",
    "content": "If an account or materials created by an account is in violation of our terms of use and our community guidelines, we may remove the account and/or materials from data.world. Below are some common questions in respect to this:\nWhy was my account deleted?\nCommon reasons for account deletion may include:\nProgrammatic Access:We may delete an account if it appears that the owner is accessing our site using robots, spiders, scrapers or other automated means in violation of our terms of use, for the purpose of bulk extracting or \"scraping\" content and data from data.world. We provide APIs for the purpose of interacting programmatically with our resources in approved ways.\nSpam: We often delete accounts because they are spammy or fake, which can lead to security risks for us and our users. These types of accounts are against our terms of use and our community guideline\nAccount security at risk: If we think an account has been hacked or compromised, we may delete it so it does not put other accounts, users or us at risk. We will work with the owner of the account to secure his or her information on our site.\nAbusive behavior: We may delete an account if we receive reports from others or notice that a user is being abusive, threatening or engaging in other egregious behavior in violation of our terms of use and community guidelines.\nCan I access my account after it has been deleted?\nOnce we have deleted your account, you will not be able to access it, any datasets or comments on them. In addition, you will not be able to contribute to any other datasets or projects, and contributors to your datasets and projects will not be able to access them or your user page.\nCan I or a family member open a new account if my account is deleted?\nYou will not be allowed to register a new account after your account has been deleted. We may also delete existing accounts that anyone in your household opens.\nHave any other questions?\nContact help@data.world",
    "url": "https://docs.data.world/en/99210-account-removal-due-to-terms-violations---faq.html"
  },
  {
    "title": "Aggregation and function support",
    "content": "",
    "url": "https://docs.data.world/en/99114-aggregation-and-function-support.html"
  },
  {
    "title": "Athena aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nNatively Supported\napprox_median\nNatively Supported\napprox_percentile\nNatively Supported\narbitrary\nNatively Supported\narray_agg\nNatively Supported\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nNatively Supported\ncorrelation\nNatively Supported\ncount\nNatively Supported\ncount(*)\nNatively Supported\ncount_if\nNatively Supported\ncovar_pop\nNatively Supported\ncovar_samp\nNatively Supported\ngroup_concat\nNatively Supported\nkurtosis\nNatively Supported\nmax\nNatively Supported\nmax_by\nNatively Supported\nmin\nNatively Supported\nmin_by\nNatively Supported\nregr_avgx\nNatively Supported (DISTINCT emulated)\nregr_avgy\nNatively Supported (DISTINCT emulated)\nregr_count\nNatively Supported (DISTINCT emulated)\nregr_intercept\nNatively Supported\nregr_r2\nNatively Supported (DISTINCT emulated)\nregr_slope\nNatively Supported\nregr_sxx\nNatively Supported (DISTINCT emulated)\nregr_sxy\nNatively Supported (DISTINCT emulated)\nregr_syy\nNatively Supported (DISTINCT emulated)\nskewness\nNatively Supported\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99257-athena-aggregation-support.html"
  },
  {
    "title": "Athena function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nNatively Supported\narray_append\nNatively Supported\narray_concat\nNatively Supported\narray_contains\nNatively Supported\narray_join\nNatively Supported\narray_length\nNatively Supported\narray_prepend\nNatively Supported\nasin\nNatively Supported\nat_time_zone\nEmulated\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nNatively Supported\ncurrent_user\nEmulated\ndate_add\nEmulated\ndate_diff\nNatively Supported\ndate_format\nEmulated\ndate_parse\nEmulated\ndate_part\nEmulated\ndate_sub\nEmulated\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nNatively Supported\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nNatively Supported\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nNatively Supported\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nEmulated\nregexp_extract\nNatively Supported\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nNatively Supported\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nNatively Supported\nsha256\nNatively Supported\nsha384\nEmulated\nsha512\nNatively Supported\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nNatively Supported\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nNatively Supported\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nNatively Supported\nurl_extract_host\nNatively Supported\nurl_extract_parameter\nNatively Supported\nurl_extract_path\nNatively Supported\nurl_extract_port\nNatively Supported\nurl_extract_protocol\nNatively Supported\nurl_extract_query\nNatively Supported\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99258-athena-function-support.html"
  },
  {
    "title": "Azure Synapse aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nEmulated\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nNatively Supported\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nNatively Supported\ncorrelation\nEmulated\ncount\nNatively Supported\ncount(*)\nEmulated\ncount_if\nNatively Supported\ncovar_pop\nEmulated\ncovar_samp\nEmulated\ngroup_concat\nEmulated\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported (DISTINCT emulated)\nregr_avgy\nNatively Supported (DISTINCT emulated)\nregr_count\nNatively Supported (DISTINCT emulated)\nregr_intercept\nNatively Supported (DISTINCT emulated)\nregr_r2\nNatively Supported (DISTINCT emulated)\nregr_slope\nNatively Supported (DISTINCT emulated)\nregr_sxx\nNatively Supported (DISTINCT emulated)\nregr_sxy\nNatively Supported (DISTINCT emulated)\nregr_syy\nNatively Supported (DISTINCT emulated)\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nEmulated\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nEmulated\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99259-azure-synapse-aggregation-support.html"
  },
  {
    "title": "Azure Synapse function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nEmulated\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nNatively Supported\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nNatively Supported\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nEmulated\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported (3-argument version emulated)\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nEmulated\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nEmulated\nregexp_extract\nEmulated\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nEmulated\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nNatively Supported\nsha256\nNatively Supported\nsha384\nEmulated\nsha512\nNatively Supported\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nNatively Supported\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nNatively Supported\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99260-azure-synapse-function-support.html"
  },
  {
    "title": "BigQuery aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nNatively Supported\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nNatively Supported\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nEmulated\ncorrelation\nNatively Supported\ncount\nNatively Supported\ncount(*)\nNatively Supported\ncount_if\nNatively Supported\ncovar_pop\nNatively Supported\ncovar_samp\nNatively Supported\ngroup_concat\nNatively Supported\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported (DISTINCT emulated)\nregr_avgy\nNatively Supported (DISTINCT emulated)\nregr_count\nNatively Supported (DISTINCT emulated)\nregr_intercept\nNatively Supported (DISTINCT emulated)\nregr_r2\nNatively Supported (DISTINCT emulated)\nregr_slope\nNatively Supported (DISTINCT emulated)\nregr_sxx\nNatively Supported (DISTINCT emulated)\nregr_sxy\nNatively Supported (DISTINCT emulated)\nregr_syy\nNatively Supported (DISTINCT emulated)\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99261-bigquery-aggregation-support.html"
  },
  {
    "title": "BigQuery function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nNatively Supported\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nNatively Supported\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nEmulated\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nNatively Supported\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nNatively Supported (3-argument version emulated)\nregexp_extract\nNatively Supported (3-argument version emulated)\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nNatively Supported\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nNatively Supported\nsha256\nNatively Supported\nsha384\nEmulated\nsha512\nNatively Supported\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nNatively Supported\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nNatively Supported\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99262-bigquery-function-support.html"
  },
  {
    "title": "Denodo aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nEmulated\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nEmulated\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nEmulated\ncorrelation\nEmulated\ncount\nNatively Supported\ncount(*)\nEmulated\ncount_if\nNatively Supported\ncovar_pop\nEmulated\ncovar_samp\nEmulated\ngroup_concat\nEmulated\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported (DISTINCT emulated)\nregr_avgy\nNatively Supported (DISTINCT emulated)\nregr_count\nNatively Supported (DISTINCT emulated)\nregr_intercept\nNatively Supported (DISTINCT emulated)\nregr_r2\nNatively Supported (DISTINCT emulated)\nregr_slope\nNatively Supported (DISTINCT emulated)\nregr_sxx\nNatively Supported (DISTINCT emulated)\nregr_sxy\nNatively Supported (DISTINCT emulated)\nregr_syy\nNatively Supported (DISTINCT emulated)\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99263-denodo-aggregation-support.html"
  },
  {
    "title": "Denodo function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nEmulated\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nEmulated\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nEmulated\ndate_format\nNatively Supported\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nEmulated\nltrim\nNatively Supported\nmd5\nEmulated\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nNatively Supported\nregexp_extract\nEmulated\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nEmulated\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nEmulated\nsha256\nEmulated\nsha384\nEmulated\nsha512\nEmulated\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nEmulated\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nEmulated\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99264-denodo-function-support.html"
  },
  {
    "title": "PostgreSQL aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nEmulated\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nEmulated\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nEmulated\ncorrelation\nNatively Supported\ncount\nNatively Supported\ncount(*)\nNatively Supported\ncount_if\nNatively Supported\ncovar_pop\nNatively Supported\ncovar_samp\nNatively Supported\ngroup_concat\nEmulated\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported\nregr_avgy\nNatively Supported\nregr_count\nNatively Supported\nregr_intercept\nNatively Supported\nregr_r2\nNatively Supported\nregr_slope\nNatively Supported\nregr_sxx\nNatively Supported\nregr_sxy\nNatively Supported\nregr_syy\nNatively Supported\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99265-postgresql-aggregation-support.html"
  },
  {
    "title": "PostgreSQL function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nNatively Supported\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nEmulated\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nNatively Supported\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nNatively Supported\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nNatively Supported\nregexp_extract\nEmulated\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nNatively Supported\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nEmulated\nsha256\nEmulated\nsha384\nEmulated\nsha512\nEmulated\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nEmulated\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nEmulated\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99266-postgresql-function-support.html"
  },
  {
    "title": "Redshift aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nEmulated\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nEmulated\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nEmulated\ncorrelation\nEmulated\ncount\nNatively Supported\ncount(*)\nNatively Supported\ncount_if\nNatively Supported\ncovar_pop\nEmulated\ncovar_samp\nEmulated\ngroup_concat\nEmulated\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nEmulated\nregr_avgy\nEmulated\nregr_count\nEmulated\nregr_intercept\nEmulated\nregr_r2\nEmulated\nregr_slope\nEmulated\nregr_sxx\nEmulated\nregr_sxy\nEmulated\nregr_syy\nEmulated\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99267-redshift-aggregation-support.html"
  },
  {
    "title": "Redshift function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nNatively Supported\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nEmulated\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nNatively Supported\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nNatively Supported\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nNatively Supported\nregexp_extract\nEmulated\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nNatively Supported\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nEmulated\nsha256\nEmulated\nsha384\nEmulated\nsha512\nEmulated\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nEmulated\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nEmulated\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99268-redshift-function-support.html"
  },
  {
    "title": "Snowflake aggregation support",
    "content": "Aggregation\nSupport\napprox_distinct\nNatively Supported\napprox_median\nNatively Supported (2-argument version emulated)\napprox_percentile\nNatively Supported (3-argument version emulated)\narbitrary\nNatively Supported\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nEmulated\ncorrelation\nNatively Supported\ncount\nNatively Supported\ncount(*)\nNatively Supported (DISTINCT emulated)\ncount_if\nNatively Supported\ncovar_pop\nNatively Supported\ncovar_samp\nNatively Supported\ngroup_concat\nNatively Supported\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported\nregr_avgy\nNatively Supported\nregr_count\nNatively Supported\nregr_intercept\nNatively Supported\nregr_r2\nNatively Supported\nregr_slope\nNatively Supported\nregr_sxx\nNatively Supported\nregr_sxy\nNatively Supported\nregr_syy\nNatively Supported\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nNatively Supported\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nNatively Supported\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99269-snowflake-aggregation-support.html"
  },
  {
    "title": "Snowflake function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nEmulated\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nEmulated\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nNatively Supported\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nEmulated\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nNatively Supported\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nNatively Supported\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nNatively Supported\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nNatively Supported\nregexp_extract\nNatively Supported\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nNatively Supported\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nNatively Supported\nsha256\nEmulated\nsha384\nEmulated\nsha512\nEmulated\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nNatively Supported\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nNatively Supported\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99270-snowflake-function-support.html"
  },
  {
    "title": "SQL server aggregation support",
    "content": "function\nSupport\napprox_distinct\nEmulated\napprox_median\nEmulated\napprox_percentile\nEmulated\narbitrary\nNatively Supported\narray_agg\nUnavailable\navg\nNatively Supported\nbool_and\nNatively Supported\nbool_or\nNatively Supported\nchecksum\nNatively Supported\ncorrelation\nEmulated\ncount\nNatively Supported\ncount(*)\nNatively Supported\ncount_if\nNatively Supported\ncovar_pop\nEmulated\ncovar_samp\nEmulated\ngroup_concat\nEmulated\nkurtosis\nEmulated\nmax\nNatively Supported\nmax_by\nEmulated\nmin\nNatively Supported\nmin_by\nEmulated\nregr_avgx\nNatively Supported (DISTINCT emulated)\nregr_avgy\nNatively Supported (DISTINCT emulated)\nregr_count\nNatively Supported (DISTINCT emulated)\nregr_intercept\nNatively Supported (DISTINCT emulated)\nregr_r2\nNatively Supported (DISTINCT emulated)\nregr_slope\nNatively Supported (DISTINCT emulated)\nregr_sxx\nNatively Supported (DISTINCT emulated)\nregr_sxy\nNatively Supported (DISTINCT emulated)\nregr_syy\nNatively Supported (DISTINCT emulated)\nskewness\nEmulated\nstd_pop\nNatively Supported\nstd_samp\nEmulated\nstdev\nNatively Supported\nsum\nNatively Supported\nvar_pop\nNatively Supported\nvar_samp\nEmulated\nvariance\nNatively Supported",
    "url": "https://docs.data.world/en/99271-sql-server-aggregation-support.html"
  },
  {
    "title": "SQL Server function support",
    "content": "Function\nSupport\nabs\nNatively Supported\nacos\nNatively Supported\narray\nUnavailable\narray_append\nUnavailable\narray_concat\nUnavailable\narray_contains\nUnavailable\narray_join\nUnavailable\narray_length\nUnavailable\narray_prepend\nUnavailable\nasin\nNatively Supported\nat_time_zone\nEmulated\natan\nNatively Supported\natan2\nNatively Supported\nattr_of\nEmulated\nceiling\nNatively Supported\nchar\nNatively Supported\ncoalesce\nNatively Supported\nconcat\nNatively Supported\ncos\nNatively Supported\ncosh\nNatively Supported\ncurrent_user\nEmulated\ndate_add\nNatively Supported\ndate_diff\nNatively Supported\ndate_format\nEmulated\ndate_parse\nEmulated\ndate_part\nNatively Supported\ndate_sub\nNatively Supported\ndate_trunc\nNatively Supported\nday\nNatively Supported\ndegrees\nNatively Supported\nelement_at\nUnavailable\nexp\nNatively Supported\nexp10\nNatively Supported\nfloor\nNatively Supported\nget_path\nEmulated\ngreatest\nNatively Supported\nhours\nNatively Supported\niri_of\nEmulated\njson_extract_scalar\nEmulated\nlabel_of\nEmulated\nleast\nNatively Supported\nleft\nNatively Supported\nlength\nNatively Supported\nlike\nNatively Supported\nlog\nNatively Supported\nlog10\nNatively Supported\nlower\nNatively Supported\nlpad\nEmulated\nltrim\nNatively Supported\nmd5\nNatively Supported\nmid\nNatively Supported\nminutes\nNatively Supported\nmod\nNatively Supported\nmonth\nNatively Supported\nnow\nNatively Supported\npi\nNatively Supported\nposition\nNatively Supported\npow\nNatively Supported\nradians\nNatively Supported\nrand\nNatively Supported\nrandom\nNatively Supported\nregex\nEmulated\nregexp_extract\nEmulated\nreplace\nNatively Supported\nright\nNatively Supported\nround\nNatively Supported\nrpad\nEmulated\nrtrim\nNatively Supported\nseconds\nNatively Supported\nsha1\nNatively Supported\nsha256\nNatively Supported\nsha384\nEmulated\nsha512\nNatively Supported\nsign\nNatively Supported\nsin\nNatively Supported\nsinh\nNatively Supported\nsqrt\nNatively Supported\nstring_split\nEmulated\nsubstring\nNatively Supported\ntan\nNatively Supported\ntanh\nNatively Supported\ntrim\nNatively Supported\nupper\nNatively Supported\nurl_extract_fragment\nEmulated\nurl_extract_host\nEmulated\nurl_extract_parameter\nEmulated\nurl_extract_path\nEmulated\nurl_extract_port\nEmulated\nurl_extract_protocol\nEmulated\nurl_extract_query\nEmulated\nyear\nNatively Supported",
    "url": "https://docs.data.world/en/99272-sql-server-function-support.html"
  },
  {
    "title": "Common licenses in order of most open to most restrictive:",
    "content": "Dedicate your dataset to the public domain: This isn\u2019t technically a license since you are relinquishing all your rights in your dataset by choosing to dedicate your dataset to the public domain. To donate your work to the public domain, you can select \u201cpublic domain\u201d from the license menu when creating your dataset.\nThis license is one of the Open Data Commons licenses and is like a public domain dedication. It allows you, as a dataset owner, to use a license mechanism to surrender your rights in a dataset when you might not otherwise be able to dedicate your dataset to the public domain under applicable law.\nThis license is one of the open Creative Commons licenses and allows users to share and adapt your dataset so long as they give credit to you.\nThis Community Data License Agreement is similar to permissive open source licenses such as the MIT license. It allows users to use, modify and adapt your dataset and the data within it, and to share it. The CDLA-Permissive-2.0 terms explicitly do not impose any obligations or restrictions on results obtained from users\u2019 computational use of the data. The 2.0 version is significantly shorter, uses plain language to express the grant of permissions and requirements. The only obligation is to \"make available the text of this agreement with the shared Data,\" including the disclaimer of warranties and liability.\nThis license is one of the Open Data Commons licenses and allows users to share and adapt your dataset so long as they give credit to you.\nThis license is one of the open Creative Commons licenses and allows users to share and adapt your dataset so long as they give credit to you and distribute any additions, transformations or changes to your dataset under this license. We consider this license (a.k.a a viral license) problematic since others may decide not to work with your CC-BY-SA licensed dataset if there is risk that by doing so their work on your dataset will need to be shared under this license when they would rather use another license.\nThis license is one of the Community Data License Agreement licenses and was designed to embody the principles of \"copyleft\" in a data license. It allows users to use, modify and adapt your dataset and the data within it, and to share the dataset and data with their changes so long as they do so under the CDLA-Sharing and give credit to you. The CDLA-Sharing terms explicitly do not impose any obligations or restrictions on results obtained from users\u2019 computational use of the data.\nThis license is one of the Open Data Commons licenses and allows users to share and adapt your dataset so long as they give credit to you and distribute any additions, transformation or changes to your dataset under this license. We consider this license (a.k.a a viral license) problematic since others may decide not to work with your ODC-ODbL licensed dataset if there is risk that by doing so their work on your dataset will need to be shared under this license when they would rather use another license.\nThis license is one of the more restrictive Creative Commons licenses. Users can share and adapt your dataset if they give credit to you and do not use your dataset for any commercial purposes.\nThis license is one of the more restrictive Creative Commons licenses. Users can share your dataset if they give credit to you, but they cannot make any additions, transformations or changes to your dataset under this license.\nThis license is one of the most restrictive Creative Commons licenses. Users can share your dataset only if they (1) give credit to you, (2) do not use your dataset for any commercial purposes, and (3) distribute any additions, transformations or changes to your dataset under this license. We consider this license a viral license since users will need to share their work on your dataset under this same license and any users of the adapted dataset would likewise need to share their work on the adapted dataset under this license and so on for any other changes to those modified datasets.\nThis license is one of the most restrictive Creative Commons licenses. Users can share only your unmodified dataset if they give credit to you and do not share it for commercial purposes. Users cannot make any additions, transformations or changes to your dataset under this license.\nIf a license is not listed in the data.world menu options, you may select Other and specify the details in the summary of your dataset.\nNo one can use, share, distribute, re-post, add to, transform or change your dataset if you have not specified a license.\nThese descriptions are only summaries of these licenses. For the actual text of the licenses, which we strongly encourage you to read, click on the links provided.",
    "url": "https://docs.data.world/en/99118-common-licenses-in-order-of-most-open-to-most-restrictive-.html"
  },
  {
    "title": "Data Inspections",
    "content": "When loading your file into data.world, the following warnings may be generated. These warnings will only be visible to the dataset owner and any contributors with write access to the dataset.\nWarnings are informational only and may be ignored.",
    "url": "https://docs.data.world/en/99119-data-inspections.html"
  },
  {
    "title": "Geospatial",
    "content": "Country name/abbreviation actually exists\nThis cell doesn't contain a valid country name\nState name actually exists\nThis cell doesn't contain a valid state name\nState abbreviation actually exists\nThis cell doesn't contain a valid state abbreviation",
    "url": "https://docs.data.world/en/99120-geospatial.html"
  },
  {
    "title": "Noise",
    "content": "Mostly numeric columns with rare (non-N/A) text\nThis value doesn't look like a number\nOccasional percentage or currency numbers\nThis value doesn't look like a number",
    "url": "https://docs.data.world/en/99121-noise.html"
  },
  {
    "title": "Numeric",
    "content": "Numeric outlier\nThis number looks way too [big or small]\nLikely noise numbers\nThis number looks like a placeholder\nNumeric truncation\nPossible Numeric Truncation",
    "url": "https://docs.data.world/en/99122-numeric.html"
  },
  {
    "title": "Security / PII",
    "content": "Social security numbers\nThis column may contain social security numbers\nCredit card numbers\nThis column may contain credit card numbers\nPhone numbers\nThis column may contain phone numbers\nEmail addresses\nThis column may contain email addresses",
    "url": "https://docs.data.world/en/99123-security---pii.html"
  },
  {
    "title": "Structural Warnings",
    "content": "Empty Columns\nThis column is blank\nDuplicate Rows\nThis row is a duplicate of the one above it.\nLikely row truncation\nPossible row truncation. The number of characters in this row could indicate that some amount of data was clipped.\nLikely column truncation\nPossible column truncation. The number of characters in a given field could indicate that some amount of data was clipped.\nSuspiciously round numbers of rows\nSuspiciously round number of rows. I.e. exactly 1000 rows. Perhaps this isn\u2019t the full data, but rather a subset.\nRare blank cells in columns\nA column which contains mostly filled values has some small number left blank.",
    "url": "https://docs.data.world/en/99124-structural-warnings.html"
  },
  {
    "title": "Text",
    "content": "String truncation\nPossible string truncation.\nLikely noise text\nThis text looks like a placeholder (i.e. qwerty, asdf)\nString length outlier\nThis text looks [longer or shorter] than the rest",
    "url": "https://docs.data.world/en/99125-text.html"
  },
  {
    "title": "Data limits",
    "content": "The size of data files you can store on data.world is set by your account plan. To see your file limits, go to your profile >settings> > billing. More information on free and paid accounts can be found here. Here's what we currently support:\nDataset Limits:\nA dataset ingested by data.world may have a maximum size of 1GB and up to 250 individual files. Datasets from live connections have no size limit, nor do metadata management datasets created by metadata crawling.\nIndividual File Upload Limits:\nThe maximum size for an individual file is 1GB. If you have a file that is larger than that, try compressing the file to get it under the limit, but note that it would then only be available for download due to size constraints.\nInference & Preview Limits:\nNon-tabular files that can be previewed only display a file preview if less than 40k. Images will be displayed beyond that limit if possible.\nFor xls / xlsx, the file must be less than 100MB uncompressed for us to support query and data preview functionality.\nFor other supported data files, we will provide data preview and query capabilities up to 1GB.\nFor deeper details we have tables with specific size limit and timeout information. Please contact us if your application requires a greater number of files or a larger maximum file size.",
    "url": "https://docs.data.world/en/99126-data-limits.html"
  },
  {
    "title": "Definitions of common data.world terms",
    "content": "Name\nDescription\nSummary\nAdministrator\nThe person in an organization who can manage organization members and access levels, and access all data sets and projects owned by the organization (even private ones).\nAPI\nApplication Program Interface\nA set of routines, protocols, and tools for building software applications. Basically, an API specifies how software components should interact. Additionally, APIs are used when programming graphical user interface (GUI) components.\nArticle\nDocumentation on data.world is broken up into four different types. One of those types is articles which are instructional for a specific task or feature, and are not hands-on.\nBest practices\nBest practices is a type of documentation which is instructional, not hands on, and recommends a specific way of doing something.\nBookmarks\nYou can add a bookmark to any dataset or project that interests you, whether or not it is owned by you or your organization. Search is enabled in your bookmarks section to help you quickly find datasets or projects. If your data project is bookmarked, you can think of it as similar to a \"like\" on Facebook.\nBusiness glossary\nA list of terms defined as they are used in your specific business environment.\nCatalog\nA catalog is an organized list of information.\nCC BY-NC\nCreative Commons Attribution-NonCommercial 4.0 International\nThis license is one of the more restrictive Creative Commons licenses. Users can share and adapt your dataset if they give credit to you and do not use your dataset for any commercial purposes.\nCC BY-NC-ND\nCreative Commons Attribution-NonCommercial-NoDerivatives 4.0 International\nThis license is one of the most restrictive Creative Commons licenses. Users can share only your unmodified dataset if they give credit to you and do not share it for commercial purposes. Users cannot make any additions, transformations or changes to your dataset under this license.\nCC BY-NC-SA\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International\nThis license is one of the most restrictive Creative Commons licenses. Users can share your dataset only if they (1) give credit to you, (2) do not use your dataset for any commercial purposes, and (3) distribute any additions, transformations or changes to your dataset under this license. We consider this license a viral license since users will need to share their work on your dataset under this same license and any users of the adapted dataset would likewise need to share their work on the adapted dataset under this license and so on for any other changes to those modified datasets.\nCC BY-ND\nCreative Commons Attribution-NoDerivatives 4.0 International\nThis license is one of the more restrictive Creative Commons licenses. Users can share your dataset if they give credit to you, but they cannot make any additions, transformations or changes to your dataset under this license.\nCC-0\nCreative Commons Public Domain Dedication\nThis license is one of the open Creative Commons licenses and is like a public domain dedication. It allows you, as a dataset owner, to use a license mechanism to surrender your rights in a dataset when you might not otherwise be able to dedicate your dataset to the public domain under applicable law.\nCC-BY\nCreative Commons Attribution 4.0 International\nThis license is one of the open Creative Commons licenses and allows users to share and adapt your dataset so long as they give credit to you.\nCC-BY-SA\nCreative Commons Attribution-ShareAlike 4.0 International\nThis license is one of the open Creative Commons licenses and allows users to share and adapt your dataset so long as they give credit to you and distribute any additions, transformations or changes to your dataset under this license. We consider this license (a.k.a a viral license) problematic since others may decide not to work with your CC-BY-SA licensed dataset if there is risk that by doing so their work on your dataset will need to be shared under this license when they would rather use another license.\nCDLA-Permissive-2.0\nCommunity Data License Agreement \u2013 Permissive, Version 2.0\nCommunity Data License Agreement \u2013 Permissive, Version 2.0This Community Data License Agreement is similar to permissive open source licenses such as the MIT license. It allows users to use, modify and adapt your dataset and the data within it, and to share it. The\u00a0CDLA-Permissive-2.0 terms explicitly do not impose any obligations or restrictions on results obtained from users\u2019 computational use of the data. The 2.0 version is significantly shorter, uses plain language to express the grant of permissions and requirements. The only obligation is to \"make available the text of this agreement with the shared Data,\" including the disclaimer of warranties and liability.\nCDLA-Sharing-1.0\nCommunity Data License Agreement \u2013 Sharing, Version 1.0\nThis license is one of the Community Data License Agreement licenses and was designed to embody the principles of \"copyleft\" in a data license. It allows users to use, modify and adapt your dataset and the data within it, and to share the dataset and data with their changes so long as they do so under the CDLA-Sharing and give credit to you. The CDLA-Sharing terms explicitly do not impose any obligations or restrictions on results obtained from users\u2019 computational use of the data.\nClassroom\nA classroom is a type of organization you can set-up in data.world so you and your students can upload datasets, create projects, discuss, and share insights. A classroom includes unlimited private projects & datasets, 1GB per project/dataset, & up to 100 members, so it's a perfect way to collaborate with any group that needs to learn together.\nColumns\nData in tabular format is arranged into rows and columns. Columns represent data of the same type across all the records.\nCommunity\nThe data.world community includes every person who uses the platform whether enterprise, educational, or individual.\nContent contributor\nA Content Contributor is a person in an organization who can create and interact with the organization's projects and datasets.\nContributor\nA Contributor is a person who is invited to access a dataset or project. Contributor permissions can be set to Discover only, View only, Edit (view and edit), or Manage (view, edit, and manage).\nCreated and Updated Date\nCreated and updated are two operators which can be used to find datasets, projects, insights, users and organizations based on the date they were added or last updated. Timestamps are set in UTC, not your local time, so you might get results that are a day off of your local time depending on where you are:\nCreator\nThe creator of a dataset or project is the individual who creates it. The creator can be different from the owner (see owner for more details). The distinction between owner and creator is important for organizations as the owner manages a resource with the same privileges as the creator, but owners can be changed (as personnel changes) while creator is a static entry.\nCrowdsourced data\nAn organization can be configured so that an individual outside the organization can propose that the organization own a dataset created by the individual. Datasets created in this way are called crowdsourced data.\nCSV\nComma-Separated-Value is a file format used to transform text into tables. Commas are used to separate the data into columns of the same data type, and paragraph breaks are used to separate it into records or rows.\nData\nData is just information, and it can take many forms from images to spreadsheets. Data in data.world can be in any file format.\nDatabase\nA structured set of data held in a computer, especially one that is accessible in various ways.\nData dictionary\nThe data dictionary contains all the metadata (data about the data) for the files, tables and columns in a dataset. For all files it contains:\nThe names of all the files in the dataset, a place to add descriptions for each file, and the labels for each file. For tabular files it has: The column names, the format of the data in each column, and a place to add a description for each column.\nData inspector\nWhen data is ingested into data.world the Data Inspector evaluates it to rapidly diagnose issues with it. The inspector does not examine data brought in through a live connection, only data uploaded to data.world\nData sources\nA data source is any place you can get data from including databases, local files, cloud-based files, real-time sources like log files, SaaS data, URL's, a corporate network.\nDataset\nDatasets are where all data is stored and documented for later sharing and use in projects. A dataset is the basic repository for data files and associated metadata, documentation, scripts, and any other supporting resources that should be stored alongside the data.\nDescription fields\nDatasets, projects, all the files in each, and all the columns in any structured data files have description fields associated with them. Descriptions are very short and serve as a quick reference for the item they describe.\nFAQ\nFrequently Asked Question\nA document format consisting of questions and answers.\nGlossary\nA glossary is an alphabetical list of terms or words found in or relating to a specific subject with explanations; a brief dictionary.\nGraph database\nA graph database is a database that uses graph structures for semantic queries with nodes, edges and properties to represent and store data.\nInsights\nFindings, conclusions, and interesting points for discussion about a project are stored as insights in the project.\nIntegration\nAn application or program that connects to data.world in order to transport, manipulate, sync, or share data and analyses of the data.\nJSON\nJavaScript Object Notation\nJSON (pronounced jay-saun) is a language-independent, open standard file format, and data interchange format, that uses human-readable text to store and transmit data objects consisting of attribute\u2013value pairs and array data types (or any other serializable value).\nResources\nYour resources are the datasets and projects owned by you or your organization(s).\nlicense\ndata.world allows you to specify how you allow data you own to be used by others.\nlicense type\nBy providing a license, you are setting expectations about how you want your data to be used. You can think of a license as the Terms of Use for your data.\nMarkup language\nA markup language is a computer language that uses tags to define elements within a document. It is human-readable, meaning markup files contain standard words, rather than typical programming syntax. The two most common mark-up languages are HTML and XML.\nMetadata\nNational Information Standards Organization (NISO), Metadata is structured information that describes, explains, locates, or otherwise makes it easier to retrieve, use, or manage an information resource. Metadata is often called data about data or information about information.\nMetadata catalog\nAn organized list containing all the information about your data resources. For example, the source, the type, the location, the owner, the update and creation dates, descriptions of the resource, etc.\nMetamap\nA graph-based data repository containing the metadata about all public datasets stored in data.world.\nODC-BY\nOpen Data Commons Attribution License\nThis license is one of the Open Data Commons licenses and allows users to share and adapt your dataset so long as they give credit to you.\nODC-ODbL\nOpen Data Commons Open Database License\nThis license is one of the Open Data Commons licenses and allows users to share and adapt your dataset so long as they give credit to you and distribute any additions, transformation or changes to your dataset under this license. We consider this license (a.k.a a viral license) problematic since others may decide not to work with your ODC-ODbL licensed dataset if there is risk that by doing so their work on your dataset will need to be shared under this license when they would rather use another license.\nOKTA\nCloud software that helps companies manage and secure user authentication into modern applications, and for developers to build identity controls into applications, website web services and devices. Provides secure identity management with Single Sign-On, Multi-factor Authentication and Lifecycle Management (Provisioning).\nOrganization\nA group on data.world that you belong to which determines what data resources you can see and edit.\nOwner\nWhen a dataset or project is created the person creating it is the creator, but the owner can be designated as either the person who created it, one of the organizations in which the creator is a member, or an organization that accepts ownership proposals. The owner has all the same permissions for management and editing of the dataset or project that the creator has.\nPDDL\nOpen Data Commons Public Domain Dedication and License\nThis license is one of the Open Data Commons licenses and is like a public domain dedication. It allows you, as a dataset owner, to use a license mechanism to surrender your rights in a dataset when you might not otherwise be able to dedicate your dataset to the public domain under applicable law.\nPlatform\nThe data.world application is also referred to as the platform.\nProject\nProjects are where all querying, analysis and discussion of data takes place in data.world. Data in different datasets can be used for many different projects, but each project contains all and only the data that is relevant for that project. The information in a project can come from datasets, files attached directly to the project, insights written by the project's team members about the data and the project, and discussions about the project.\nPublic API\nThe public API is used to create an integration or application with data.world. The API can also be used to get data out of data.world.\nPublic Domain\nPublic Domain License\nThe work has been dedicated to the public domain by waiving all rights to the work worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law.\nQuery\nA statement written to retrieve information from a dataset on data.world. Queries can be written in SQL or SPARQL.\nQuick start guide\nA quick-start guides is a short hands-on type of documentation derived from tutorials and designed to quickly get users comfortable with basic use of the data.world platform.\nRDF\nResource Description Framework\nRDF represents information using semantic triples, which comprise a subject, predicate, and object. Turtle provides a way to group three URIs to make a triple, and provides ways to abbreviate such information, for example by factoring out common portions of URIs.\nRDF triple store\nAn RDF triple store is similar to a graph database and stores information in semantic triples. It is accessed and manipulated using the SPARQL query language.\nReference\nA type of documentation that includes tables, lists, glossaries, appendices, etc. It is informational, not instructional, in format and is not hands-on.\nSAML\nSecurity Assertion Markup Language\nAn open standard for exchanging authentication and authorization data between parties, in particular, between an identity provider and a service provider. SAML enables Single-Sign On (SSO)\nShare-alike license\nIf you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.\nSPARQL\nSPARQL Protocol and RDF Query Language\nPronounced \"sparkle\", SPARQL is an RDF query language\u2014that is, a semantic query language for databases\u2014able to retrieve and manipulate data stored in RDF format.\nSQL\nStructured Query Language\nSQL is a language used to access and manipulate relational database management systems.\nSSO\nSingle Sign-on\na property of access control of multiple related, yet independent, software systems. With this property, a user logs in with a single ID and password to gain access to any of several related systems.\nStreams\nStreams are a type of input (jsonl) that allows you to update and append records to a data file on data.world instead of having to re-upload the entire file when changes need to be made.\nSummary\nThe summary is one of two documents created with a dataset or project. The summary is where all of the information about the origin of the data, why you created the dataset, further documentation of your work, etc. is found. Use the Summary section to tell your data's story.\nTag\nTags can be used to organize and group your dataset or project by topic, category, source, department, or team. They can be searched for explicitly with the tag search operator, and can also help to filter down more generic search results.\nTeam\nA team is a group of people working on a project. A team could be an organization or a subset of an organization.\nTitle\nThe name of the dataset or project. Titles are accessible via search.\nTriple\nAKA Semantic triples\nA triple is a set of three entities that arranges a statement about semantic data in the form of subject\u2013predicate\u2013object expressions. Each item in the triple is expressed as a Web URI.\nTTL or Turtle\nTerse RDF Triple Language\nTerse RDF Triple Language (Turtle) is a syntax and file format for expressing data in the RDF data model. Turtle syntax is similar to that of SPARQL. Turtle provides a way to group three URIs to make a triple, and provides ways to abbreviate such information, for example by factoring out common portions of URIs.\nTutorial\nOne of our four types of documentation is a tutorial. Tutorials are instructional, in depth, and hands-on. A variation on the tutorial is a quick start which is a shorter, derivative version of a tutorial.\nURI\nUniform Resource Identifier\nA string of characters that unambiguously identifies a particular resource. To guarantee uniformity, all URIs follow a predefined set of syntax rules but also maintain extensibility through a separately defined hierarchical naming scheme (e.g. http://).The most common form of URI is the Uniform Resource Locator (URL), frequently referred to informally as a web address.\nWhite paper\nA high-level, but very technical document. It is informational, not instructional, in format and is not hands on.",
    "url": "https://docs.data.world/en/99127-definitions-of-common-data-world-terms.html"
  },
  {
    "title": "FAQ",
    "content": "",
    "url": "https://docs.data.world/en/99130-faq.html"
  },
  {
    "title": "Can I change my user name?",
    "content": "We don't currently offer the ability to change your username within data.world, however, there are a couple of workarounds:\ncreate a new account using a different email address. If you'd like the initial account removed once you've created your new account, just submit a request for us to do so. Once removed, you could then go into your account settings to update your email if desired.\nsubmit a request for us to delete your account which will free up your email address so you'll be able to create a new account with the preferred username.\nNote that both of these options will remove all content and social activity (likes, follows, etc.) associated with the account being deleted. Please be sure to back up your work and be ready to recreate it under the new account.",
    "url": "https://docs.data.world/en/99131-can-i-change-my-user-name-.html"
  },
  {
    "title": "Can I update a file in my dataset?",
    "content": "Yes! To update a file on data.world simply upload the updated version with the same name and we will overwrite the existing one with the new version.\nNote that we also store previous versions of you files so you can always revert to them if you need to.",
    "url": "https://docs.data.world/en/99132-can-i-update-a-file-in-my-dataset-.html"
  },
  {
    "title": "How do I delete my account?",
    "content": "To remove or cancel a data.world account, you'll currently need to submit a request for data.world to manually remove it.\nWe hope to allow members to manage this in the future, but until then are happy to help with your direct request and also appreciate any final feedback you might share with us as part of it.\nNote that upon deletion, all content stored under your account will be removed and your username will be back up for grabs by new members.",
    "url": "https://docs.data.world/en/99133-how-do-i-delete-my-account-.html"
  },
  {
    "title": "How much will data.world cost me?",
    "content": "data.world is free for individuals and small teams to discover and use open data, as well as create and collaborate on their own Datasets and Data Projects up to a specific size and number.\nIn line with data.world's mission to build the most meaningful, collaborative, and abundant data resource in the world, there is no limit to the number of public Datasets or Projects created and we encourage all of our members to help in this mission by adding open datasets they're building or working with!\nFor members and teams who need additional limits and features beyond what our free tier provides, please see our pricing page for details on the available options.",
    "url": "https://docs.data.world/en/99134-how-much-will-data-world-cost-me-.html"
  },
  {
    "title": "What's the difference between Open and Private?",
    "content": "When creating a dataset or Data Project, you're given the option between open and private. Open datasets and projects will be visible, in their entirety, to anyone signed into data.world. They could be returned in search results, will be visible under your profile and will be available for querying and downloads. No other members will be able to change the dataset or project without explicit permission from you by adding them as a contributor with edit rights. If you are in an organization you have additional options for determining who can access and use your data. For more information see the article on setting dataset permissions .",
    "url": "https://docs.data.world/en/99135-what-s-the-difference-between-open-and-private-.html"
  },
  {
    "title": "What are the size limits for data.world?",
    "content": "The data.world team is hard at work in extending the boundaries of the platform. Depending on your account plan (free or paid). A list of of what we currently support can be found in the article on data size limits.",
    "url": "https://docs.data.world/en/99136-what-are-the-size-limits-for-data-world-.html"
  },
  {
    "title": "What file types can I upload?",
    "content": "There is no restriction on file types that can be uploaded or downloaded on data.world, and a dataset can consist of any combination of files added to it. There are some, size limitations, and files are handled differently based on their extension.",
    "url": "https://docs.data.world/en/99137-what-file-types-can-i-upload-.html"
  },
  {
    "title": "Why can't I upload or download data?",
    "content": "If you can neither upload nor download data from data.world you might be behind a firewall that's blocking your access. If you think that might be the problem, try performing the same tasks on a different network, such as your home internet connection. You can find information about configuring your network firewall to accept data.world connections in Allowlist for data.world.Allowlist for data.world",
    "url": "https://docs.data.world/en/99138-why-can-t-i-upload-or-download-data-.html"
  },
  {
    "title": "File upload status messages",
    "content": "Below is a list of status messages you might encounter when uploading data files to data.world. Please open a support ticket for additional assistance.\nError message\nMore details\nNo data could be extracted from this file **\nThis status will display if a file type is supported by data.world, yet cannot be previewed.\nCheck for syntax or formatting errors within your file.\nWant to see data previews? Reupload this file with an extension.\nCurrently, data.world depends on file extensions to determine how best to prepare your data. If a file is uploaded with no extension, then you will see this status message.\nIf you believe this file\u2019s data is actually a known format (say, .csv), then re-upload this file with the new extension added.\nExcel files >100MB may only be downloaded.**\nDue to how Excel files are structured, in some cases we are not able to fully preview or query the data inside the file. It is, however, still available for sharing and download. NOTE: CSV files over 100 mg can be previewed and queried.\nThis file type >100MB can only be downloaded.\nThe file is too large to properly ingest into data.world and is unavailable for queries or previews.\nThis file is shareable, though some advanced features may be unavailable due to its size.**\nThis status indicates that a file contains more cells of data than data.world was expecting. In some cases, you might be able to remove any unnecessary blank columns, rows or tabs.\nOnly the first 50 of 111 files were extracted.\nWhen uploading archived or compressed files (zip, tar, etc), ensure each contains 50 files or less. Any files over this limit will not be extracted.\n2 files were too large to be extracted from this archive. **\nIf a file within an archive exceeds data.world's data limits we will show this status.\nTry splitting the file into multiple smaller files within our size limits, then reupload.\nSorry, we can't extract the contents of this archive. It may be corrupted.\nThe archive cannot be extracted for another reason - it may be an invalid archive or an unsupported file type.\nNo data could be extracted from this file.\nThe file is of a supported type, but has a structural problem that prevents its from being extracted.\nThis file is shareable, though some advanced features may be unavailable due to the size of this dataset. **\nIf a data file is uploaded to a dataset that results in the total dataset exceeding what data.world can process, this status will be displayed.\nCheck for and remove any unnecessary blank columns, rows or tabs from all tabular files within the dataset, or contact support for further assistance.\n** Note that these errors are related to enhancing tabular and graph data to provide advanced functionality (data previews and queries). The file will still be uploaded to data.world and be available for download.",
    "url": "https://docs.data.world/en/99139-file-upload-status-messages.html"
  },
  {
    "title": "Finding your API tokens for data.world",
    "content": "When you need an API token for an integration, you can get it from your profile settings. Click on your avatar and choose Settings:\nThen select Advanced from the sidebar:\nBoth Read/Write and Admin tokens are provided. For the metadata catalog collector you can use the Read/Write token for the metadata catalog collector if you have write permissions to your organization's ddw-catalogs dataset.",
    "url": "https://docs.data.world/en/99142-finding-your-api-tokens-for-data-world.html"
  },
  {
    "title": "Licensing and data you found",
    "content": "You'll need to check the licensing terms on that dataset to see if you are authorized by the owner to distribute, re-post, re-publish or share it. If those terms allow you to do these things, you'll also need to review and comply with the conditions under which you can do so. We've put together a list of common licenses for datasets with links to the license terms here.Common license types for datasets\nEven if datasets are publicly available, their owners can continue to have rights in those datasets. Those rights extend to how the data is organized, displayed, described, visualized, etc. and can include the effort in compiling the data. These intellectual property rights need to be respected. To do so, make sure that you read and comply with the license terms on the dataset.\nIf you don't comply with the license and terms of use on a dataset, you could be found to be in breach of contract and/or violation of copyright law. For example, if you are found by a court to have violated US copyright law, you would have to pay damages set by law without the owner of the copyright having to prove he or she suffered financially from your actions.\nYou could also be in violation of our terms of use by not having the right to post a dataset to data.world, including if you don't specify the appropriate license on a dataset, and you and/or the dataset could be removed from our platform.\nSometimes finding the license terms on a dataset can be difficult. You can look for them:\nOn the main webpage\nOn the page where the summary or description of the dataset is located\nOn the download page of the dataset\nIn the terms of use or terms of service located in the footer of the webpage\nUnder \"legal\" in the footer of the webpage\nAfter searching the site where you found the dataset, you can't locate any terms or licenses that cover the dataset, you can reach out to the owner to see if he or she will give you permission to use the dataset or put a license on the dataset on the site. A dataset that does not have any license terms means the owner retains all rights in the dataset and does not authorize anyone else to use, copy, distribute, share, combine it with other data, or make any changes to it or derivative works from it.\nFair use is a tricky area. If you use copyrighted materials in a certain way that complies with the fair use doctrine, you might not be infringing on the copyright. However, courts look at the specific circumstances of the usage, so even if your usage is similar to how others have used copyrighted materials, there is no guaranty that a court will find that you have not violated someone's copyright since your circumstances may be different.\nThe US Copyright office has summarized Section 107 of the US Copyright Act.\nSection 107 provides the framework for determining whether something is a fair use and identifies certain types of uses\u2014such as criticism, comment, news reporting, teaching, scholarship, and research\u2014as examples of activities that may qualify as fair use. Section 107 calls for consideration of the following four factors in evaluating a question of fair use:\nPurpose and character of the use, including whether the use is of a commercial nature or is for nonprofit educational purposes: Courts look at how the party claiming fair use is using the copyrighted work, and are more likely to find that nonprofit educational and noncommercial uses are fair. This does not mean, however, that all nonprofit education and noncommercial uses are fair and all commercial uses are not fair; instead, courts will balance the purpose and character of the use against the other factors below. Additionally, \"transformative\" uses are more likely to be considered fair. Transformative uses are those that add something new, with a further purpose or different character, and do not substitute for the original use of the work.\nNature of the copyrighted work: This factor analyzes the degree to which the work that was used relates to copyright's purpose of encouraging creative expression. Thus, using a more creative or imaginative work (such as a novel, movie, or song) is less likely to support a claim of a fair use than using a factual work (such as a technical article or news item). In addition, use of an unpublished work is less likely to be considered fair.\nAmount and substantiality of the portion used in relation to the copyrighted work as a whole: Under this factor, courts look at both the quantity and quality of the copyrighted material that was used. If the use includes a large portion of the copyrighted work, fair use is less likely to be found; if the use employs only a small amount of copyrighted material, fair use is more likely. That said, some courts have found use of an entire work to be fair under certain circumstances. And in other contexts, using even a small amount of a copyrighted work was determined not to be fair because the selection was an important part\u2014or the \"heart\"\u2014of the work.\nEffect of the use upon the potential market for or value of the copyrighted work: Here, courts review whether, and to what extent, the unlicensed use harms the existing or future market for the copyright owner's original work. In assessing this factor, courts consider whether the use is hurting the current market for the original work (for example, by displacing sales of the original) and/or whether the use could cause substantial harm if it were to become widespread.\nIn addition to the above, other factors may also be considered by a court in weighing a fair use question, depending upon the circumstances. Courts evaluate fair use claims on a case-by-case basis, and the outcome of any given case depends on a fact-specific inquiry. This means that there is no formula to ensure that a predetermined percentage or amount of a work\u2014or specific number of words, lines, pages, copies\u2014may be used without permission.",
    "url": "https://docs.data.world/en/99143-licensing-and-data-you-found.html"
  },
  {
    "title": "Licensing and data you own",
    "content": "If your dataset does not have any license terms, it means you do not authorize anyone else to use, copy, distribute, share, combine it with other data, or make any changes to it or make derivative works from it. This absence of a license greatly reduces the reuse potential and usefulness of your dataset.\nWe encourage pick as open a license as you feel comfortable to maximize the benefits of your dataset. We believe the more open a license is, the more others will use your dataset. For more information on the details of licenses, see our list of common license types for datasets.Common license types for datasets\nBy choosing an established license like one from our list of common license types, you are choosing a license that is widely adopted. Such licenses were drafted by organizations dedicated to making those licenses functional in many situations as well as making them interoperable, clear and understandable. You'll need to read the actual licenses by clicking on the links we've provided to make sure you've picked the appropriate one for your dataset and how you would like others to interact with your dataset.\nThe more open a license you choose, the more others can use, share and distribute your dataset to get to insights faster. Your dataset could be important to solving a pressing issue. We encourage you to maximize your dataset's potential by choosing an open license.\nWhen a project involves a number of datasets, each with different licenses, the licenses may conflict and greatly restrict or even prohibit the resulting work. By choosing the most open license, you amplify your dataset's usefulness. Another tip is to review the licenses of the other datasets that may be involved in a project or used in your industry to determine what type of license would allow your dataset to be used alongside those datasets. Usually, two datasets, both with CC-BY licenses, can be combined under those license terms. However, you will still need to pay attention to the different versions of those licenses to make sure they work with one another. In addition, just because datasets have licenses which are similar like a CC-BY and ODC-ODbL, does not mean those datasets can be combined because of conflicts between those licenses.\nWe like the current versions of the open Creative Commons licenses, since these licenses are widely adopted, are applicable to databases and facilitate collaboration. We believe these licenses are becoming the more widely accepted for datasets and databases. In addition, Creative Commons has created a tool to help you choose the appropriate license for your dataset.\nFor instructions on how to set the license type for a dataset, see Setting a license type\nTo help determine the license to select, see Common license types for datasetsCommon license types for datasets\nFind a dataset you'd like to share on data.world? Check out Licensing and data you found.",
    "url": "https://docs.data.world/en/99144-licensing-and-data-you-own.html"
  },
  {
    "title": "Query editor shortcuts",
    "content": "Query editor shortcuts for both SQL and SPARQL are available on data.world. Below is a list of the supported commands for Mac and Windows:\ncommand + option + L\n(ctrl + alt + L on Windows)\nAutomatically reformat your query to make it more readable.\ncommand + shift + enter\n(ctrl + shift + enter on Windows)\nAutomatically reformat AND run your query.\ncommand + enter\n(ctrl + enter on Windows)\nRun your query.\ncommand + S\n(ctrl + S on Windows)\nSave your query",
    "url": "https://docs.data.world/en/99146-query-editor-shortcuts.html"
  },
  {
    "title": "Size limit and timeout specifications",
    "content": "Account type\nIndividual/Team Free\nIndividual/Team Professional\nEnterprise\nDataset ingested to data.world\n100 MB\n1 GB\n1 GB\nMetadata management datasets\nn/a\nn/a\nno limit\nProject\n100 MB of project-specific files, no limit on linked datasets.\n1GB of project-specific files, no limit on linked datasets.\n1 GB\nDerived dataset\n100 MB\n1 GB\n1 GB\nVirtual dataset (hosted on a remote server)\nSize not limited by data.world\nSize not limited by data.world\nSize not limited by data.world\nSize of a file in a dataset\n100 MB\n1 GB\n1 GB\nNumber of files in a dataset\n250\n250\n250\nNumber of columns in a table\nlimited by file size\nlimited by file size\nlimited by file size\nNumber of columns previewed in a table\n50\n50\n50\nNumber of columns previewed in query results\n500\n500\n500\nNumber of rows in a table\nlimited by file size\nlimited by file size\nlimited by file size\nNumber of rows previewed in query results\n10,000\n10,000\n10,000\nRate limiting: Number of burst streams API calls\n5 in the first second or after a 5 second idle period, then 1 per second\n5 in the first second or after a 5 second idle period, then 1 per second\n5 in the first second or after a 5 second idle period, then 1 per second\nSize of a record streamed\n1 MiB\n1 MiB\n1 MiB\nSize of a request streamed\n100 MB\n1 GB\n1 GB\nNumber of JSON objects in a stream\n100 MB divided by the avg record size\n1 GB divided by the avg record size\n1 GB divided by the avg record size\nAccount type\nIndividual/Team Free\nIndividual/Team Professional\nEnterprise\nQuery timeout before first byte is transmitted\n1 minute\n1 minute\n1 minute\n(upgrade to 5 minutes available upon request)\nQuery timeout before last byte is transmitted\n60 minutes\n60 minutes\n60 minutes\nData upload timeout\nNone: As long as packets continue to be passed the connection will stay open\nNone: As long as packets continue to be passed the connection will stay open\nNone: As long as packets continue to be passed the connection will stay open",
    "url": "https://docs.data.world/en/99147-size-limit-and-timeout-specifications.html"
  },
  {
    "title": "Supported file types",
    "content": "There is no restriction on file types that can be uploaded or downloaded on data.world, and a dataset can consist of any combination of files added to it. There are some size limitations, and files are handled differently based on the extension as follows:\nFormats: sqlite\nDatabase dumps will consist of multiple tables, and a schema that models the type information and the relationships between those tables. Each table will be represented as a data.world table, which can be previewed and queried naturally via our SQL engine.\nFormats: csv, tsv, xls, xlsx\nTabular files are presented in a spreadsheet-style preview and we perform basic analyses on each of the columns:\nThe data is then queryable using SQL and SPARQL; take a look at Query basics for more info on getting started with querying.\nTo provide these querying capabilities and in line with our mission to connect the world\u2019s data (by making it linkable), we\u2019re converting it to RDF Triples, or graph data, under-the-hood.\nExcel files will include all of the underlying sheets in a tabbed interface. Only the tabular data will be included; other elements like pivot tables and charts will not be shown in the preview but they will still be available in the original file.\nThe character # at the beginning of a line in CSV and TSV files is ignored during upload. If you have lines beginning with a #, they will not render correctly on data.world.\nIn addition to viewing a preview of the data in the table, you can also see the metadata for the table by clicking on Switch to column overview. The ability to switch between the data preview and the column overview persists in the summary even after it's been saved.\nFormats: rdf, rdfs, owl, jsonl, nt, ttl, n3\nThese formats are serializations of RDF data - since RDF is the native data format for data.world\u2019s platform, the statements in this file are simply loaded into the graph for the dataset or project that the file is added to. By uploading raw RDF data into a dataset or project, that data is searchable via the attached SPARQL endpoint. We show a preview of the contents of the file, including summaries of the classes, properties, and namespaces used in the file.\nFormats: json, ND-JSON, other 'sufficiently tabular' json files\nWhen a JSON file has a \"sufficiently tabular\" structure, we will attempt to produce a table of data that represents the contents of the file. Common logging formats that include JSON arrays of simple objects or newline-separated JSON objects will generally work great with this interpretation. If the structure of the file is too hierarchical or inconsistent in nature, the file will instead be treated in its raw form - you can view or download the file, but it\u2019s not queryable through our query engine.\nFormats: zip, tar, tbz2, tbz, bz2, tgz, gz, -gz, z, -z\nArchives that contain multiple files can be extracted and the first 50 files are stored in the dataset. Each extracted file is then handled using the criteria established for its extension. Please note that archives are not extracted by default. To do so, a Contributor must click on the \u2018Extract\u2019 button on the right-hand side of the archive.\nIndividual files that are compressed (i.e. foo.csv.gz) are decompressed and then treated as though the uncompressed file had been added directly.\nFormats: jpg, jpeg, png, gif, svg\nImages are displayed in-line.\nFormats: ipynb (version 4 and higher), js, r, py, as, apl, bash, bas, bat, c, cpp, cs, css, d, dart, diff, go, ini, java, julia, kt, lua, matlab, nasm, ml, perl, php, ps1, rb, scala, sql, tcl, ts, vim, yaml, xml, asp, jade, tex, less, sass, scss, Dockerfile\nSource files are presented with full syntax highlighting where appropriate.\nFormats: txt, html, md, pdf\nThe above document formats are rendered during preview. Other document types can be uploaded but are not available for preview.\nNote that iframe embeds are not rendered in html files. Instead, try adding it as an insight in a Data project.\nAll other file types can be uploaded and downloaded as long as they are within the supported size limits.",
    "url": "https://docs.data.world/en/99201-supported-file-types.html"
  }
]